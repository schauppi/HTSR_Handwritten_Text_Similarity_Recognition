{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Paper: Siamese Neural Networks for One-shot Image Recognition\n",
    "http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Images sizes: 40x40 / 75x75 / 105x105 / 120x120 / 150x150\n",
    "Images in RGB / Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import math\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from helper_functions import plot_training\n",
    "from helper_functions import create_tf_data_datasets_contrastive\n",
    "from helper_functions import create_tf_data_testset_contrastive\n",
    "from helper_functions import get_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Original Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## First Run - 30k Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_30k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_30k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_original_model(height, width, channels):\n",
    "\n",
    "    input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "    x = keras.layers.Conv2D(64, (10,10), activation=\"relu\",\n",
    "                            kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv1')(input)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(128, (7,7), activation=\"relu\",\n",
    "                            kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv2')(x)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(128, (4,4), activation=\"relu\",\n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv3')(x)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(256, (4,4), activation=\"relu\",\n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv4')(x)\n",
    "\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    output = keras.layers.Dense(4096, activation=\"sigmoid\", kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.2),\n",
    "                           bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                           kernel_regularizer=keras.regularizers.l2(1e-3), name='Dense1')(x)\n",
    "\n",
    "    model = keras.models.Model(input, output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_model = get_original_model(height,width,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 105, 105, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 96, 96, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 30k\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_siamese_model.compile(loss=config.loss_function,\n",
    "                               optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_30k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "learning rate scheduled to 0.0009900000470224768\n",
      "  6/166 [>.............................] - ETA: 9s - loss: 1510.7972 - accuracy: 0.4883WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0265s vs `on_train_batch_end` time: 0.0313s). Check your callbacks.\n",
      "166/166 [==============================] - 14s 73ms/step - loss: 1509.8461 - accuracy: 0.5195 - val_loss: 1508.8473 - val_accuracy: 0.5477\n",
      "Epoch 2/200\n",
      "learning rate scheduled to 0.000980100086890161\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1507.8730 - accuracy: 0.5517 - val_loss: 1506.8850 - val_accuracy: 0.5713\n",
      "Epoch 3/200\n",
      "learning rate scheduled to 0.0009702991275116801\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1505.9220 - accuracy: 0.5641 - val_loss: 1504.9448 - val_accuracy: 0.5760\n",
      "Epoch 4/200\n",
      "learning rate scheduled to 0.0009605961316265165\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1503.9926 - accuracy: 0.5570 - val_loss: 1503.0267 - val_accuracy: 0.5584\n",
      "Epoch 5/200\n",
      "learning rate scheduled to 0.0009509901772253215\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1502.0851 - accuracy: 0.5658 - val_loss: 1501.1294 - val_accuracy: 0.5626\n",
      "Epoch 6/200\n",
      "learning rate scheduled to 0.0009414802846731617\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1500.1985 - accuracy: 0.5495 - val_loss: 1499.2531 - val_accuracy: 0.5622\n",
      "Epoch 7/200\n",
      "learning rate scheduled to 0.0009320654743351042\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1498.3329 - accuracy: 0.5498 - val_loss: 1497.3988 - val_accuracy: 0.5624\n",
      "Epoch 8/200\n",
      "learning rate scheduled to 0.0009227448242017999\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1496.4883 - accuracy: 0.5533 - val_loss: 1495.5640 - val_accuracy: 0.5464\n",
      "Epoch 9/200\n",
      "learning rate scheduled to 0.0009135173546383158\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1494.6643 - accuracy: 0.5528 - val_loss: 1493.7501 - val_accuracy: 0.5502\n",
      "Epoch 10/200\n",
      "learning rate scheduled to 0.0009043822012608871\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1492.8597 - accuracy: 0.5554 - val_loss: 1491.9556 - val_accuracy: 0.5545\n",
      "Epoch 11/200\n",
      "learning rate scheduled to 0.0008953383844345808\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1491.0754 - accuracy: 0.5547 - val_loss: 1490.1818 - val_accuracy: 0.5584\n",
      "Epoch 12/200\n",
      "learning rate scheduled to 0.000886384982150048\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1489.3113 - accuracy: 0.5561 - val_loss: 1488.4277 - val_accuracy: 0.5515\n",
      "Epoch 13/200\n",
      "learning rate scheduled to 0.0008775211300235242\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1487.5659 - accuracy: 0.5630 - val_loss: 1486.6920 - val_accuracy: 0.5588\n",
      "Epoch 14/200\n",
      "learning rate scheduled to 0.0008687459060456604\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1485.8402 - accuracy: 0.5646 - val_loss: 1484.9774 - val_accuracy: 0.5607\n",
      "Epoch 15/200\n",
      "learning rate scheduled to 0.0008600584458326921\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1484.1348 - accuracy: 0.5679 - val_loss: 1483.2804 - val_accuracy: 0.5683\n",
      "Epoch 16/200\n",
      "learning rate scheduled to 0.0008514578850008547\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1482.4471 - accuracy: 0.5776 - val_loss: 1481.6041 - val_accuracy: 0.5833\n",
      "Epoch 17/200\n",
      "learning rate scheduled to 0.0008429433015407995\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1480.7802 - accuracy: 0.5809 - val_loss: 1479.9431 - val_accuracy: 0.5924\n",
      "Epoch 18/200\n",
      "learning rate scheduled to 0.0008345138886943459\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1479.1293 - accuracy: 0.5868 - val_loss: 1478.3011 - val_accuracy: 0.5958\n",
      "Epoch 19/200\n",
      "learning rate scheduled to 0.0008261687244521454\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1477.4977 - accuracy: 0.5922 - val_loss: 1476.6799 - val_accuracy: 0.5952\n",
      "Epoch 20/200\n",
      "learning rate scheduled to 0.0008179070596816018\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1475.8861 - accuracy: 0.5899 - val_loss: 1475.0784 - val_accuracy: 0.5873\n",
      "Epoch 21/200\n",
      "learning rate scheduled to 0.0008097279723733664\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1474.2894 - accuracy: 0.5968 - val_loss: 1473.4921 - val_accuracy: 0.6029\n",
      "Epoch 22/200\n",
      "learning rate scheduled to 0.000801630713394843\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1472.7108 - accuracy: 0.6010 - val_loss: 1471.9152 - val_accuracy: 0.6082\n",
      "Epoch 23/200\n",
      "learning rate scheduled to 0.0007936144183622674\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1471.1504 - accuracy: 0.6053 - val_loss: 1470.3665 - val_accuracy: 0.6078\n",
      "Epoch 24/200\n",
      "learning rate scheduled to 0.0007856782805174589\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1469.6035 - accuracy: 0.6153 - val_loss: 1468.8282 - val_accuracy: 0.6186\n",
      "Epoch 25/200\n",
      "learning rate scheduled to 0.0007778214931022376\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1468.0759 - accuracy: 0.6203 - val_loss: 1467.3094 - val_accuracy: 0.6288\n",
      "Epoch 26/200\n",
      "learning rate scheduled to 0.0007700433069840073\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1466.5646 - accuracy: 0.6254 - val_loss: 1465.8110 - val_accuracy: 0.6192\n",
      "Epoch 27/200\n",
      "learning rate scheduled to 0.0007623428577790037\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1465.0709 - accuracy: 0.6268 - val_loss: 1464.3197 - val_accuracy: 0.6376\n",
      "Epoch 28/200\n",
      "learning rate scheduled to 0.0007547194539802149\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1463.5918 - accuracy: 0.6337 - val_loss: 1462.8486 - val_accuracy: 0.6456\n",
      "Epoch 29/200\n",
      "learning rate scheduled to 0.0007471722312038764\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1462.1288 - accuracy: 0.6404 - val_loss: 1461.3939 - val_accuracy: 0.6467\n",
      "Epoch 30/200\n",
      "learning rate scheduled to 0.0007397004979429766\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1460.6816 - accuracy: 0.6467 - val_loss: 1459.9513 - val_accuracy: 0.6544\n",
      "Epoch 31/200\n",
      "learning rate scheduled to 0.0007323035050649196\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1459.2504 - accuracy: 0.6517 - val_loss: 1458.5309 - val_accuracy: 0.6552\n",
      "Epoch 32/200\n",
      "learning rate scheduled to 0.000724980445811525\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1457.8333 - accuracy: 0.6589 - val_loss: 1457.1237 - val_accuracy: 0.6621\n",
      "Epoch 33/200\n",
      "learning rate scheduled to 0.0007177306286757812\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1456.4297 - accuracy: 0.6648 - val_loss: 1455.7256 - val_accuracy: 0.6699\n",
      "Epoch 34/200\n",
      "learning rate scheduled to 0.0007105533045250923\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1455.0441 - accuracy: 0.6740 - val_loss: 1454.3439 - val_accuracy: 0.6876\n",
      "Epoch 35/200\n",
      "learning rate scheduled to 0.0007034477818524464\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1453.6680 - accuracy: 0.6859 - val_loss: 1452.9767 - val_accuracy: 0.6844\n",
      "Epoch 36/200\n",
      "learning rate scheduled to 0.000696413311525248\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1452.3135 - accuracy: 0.6890 - val_loss: 1451.6378 - val_accuracy: 0.6863\n",
      "Epoch 37/200\n",
      "learning rate scheduled to 0.0006894492020364851\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1450.9730 - accuracy: 0.6921 - val_loss: 1450.2922 - val_accuracy: 0.7014\n",
      "Epoch 38/200\n",
      "learning rate scheduled to 0.0006825547042535618\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1449.6477 - accuracy: 0.7002 - val_loss: 1448.9771 - val_accuracy: 0.6989\n",
      "Epoch 39/200\n",
      "learning rate scheduled to 0.0006757291842950508\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1448.3364 - accuracy: 0.7056 - val_loss: 1447.6836 - val_accuracy: 0.7108\n",
      "Epoch 40/200\n",
      "learning rate scheduled to 0.0006689718930283561\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1447.0380 - accuracy: 0.7104 - val_loss: 1446.3938 - val_accuracy: 0.7053\n",
      "Epoch 41/200\n",
      "learning rate scheduled to 0.0006622821965720505\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1445.7533 - accuracy: 0.7161 - val_loss: 1445.1145 - val_accuracy: 0.7155\n",
      "Epoch 42/200\n",
      "learning rate scheduled to 0.0006556594034191221\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1444.4882 - accuracy: 0.7174 - val_loss: 1443.8448 - val_accuracy: 0.7257\n",
      "Epoch 43/200\n",
      "learning rate scheduled to 0.0006491028220625594\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1443.2343 - accuracy: 0.7187 - val_loss: 1442.6505 - val_accuracy: 0.6897\n",
      "Epoch 44/200\n",
      "learning rate scheduled to 0.0006426118186209351\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1441.9928 - accuracy: 0.7215 - val_loss: 1441.3646 - val_accuracy: 0.7362\n",
      "Epoch 45/200\n",
      "learning rate scheduled to 0.0006361857015872375\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1440.7684 - accuracy: 0.7241 - val_loss: 1440.1431 - val_accuracy: 0.7342\n",
      "Epoch 46/200\n",
      "learning rate scheduled to 0.0006298238370800391\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1439.5547 - accuracy: 0.7274 - val_loss: 1438.9360 - val_accuracy: 0.7400\n",
      "Epoch 47/200\n",
      "learning rate scheduled to 0.0006235255912179127\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1438.3555 - accuracy: 0.7256 - val_loss: 1437.7771 - val_accuracy: 0.7172\n",
      "Epoch 48/200\n",
      "learning rate scheduled to 0.000617290330119431\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1437.1694 - accuracy: 0.7277 - val_loss: 1436.5664 - val_accuracy: 0.7327\n",
      "Epoch 49/200\n",
      "learning rate scheduled to 0.0006111174199031666\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1435.9958 - accuracy: 0.7281 - val_loss: 1435.4087 - val_accuracy: 0.7308\n",
      "Epoch 50/200\n",
      "learning rate scheduled to 0.0006050062266876921\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1434.8298 - accuracy: 0.7327 - val_loss: 1434.2590 - val_accuracy: 0.7342\n",
      "Epoch 51/200\n",
      "learning rate scheduled to 0.0005989561742171646\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1433.6853 - accuracy: 0.7318 - val_loss: 1433.0975 - val_accuracy: 0.7396\n",
      "Epoch 52/200\n",
      "learning rate scheduled to 0.0005929666286101564\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1432.5488 - accuracy: 0.7328 - val_loss: 1431.9653 - val_accuracy: 0.7447\n",
      "Epoch 53/200\n",
      "learning rate scheduled to 0.0005870369559852406\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1431.4260 - accuracy: 0.7350 - val_loss: 1430.8665 - val_accuracy: 0.7344\n",
      "Epoch 54/200\n",
      "learning rate scheduled to 0.000581166580086574\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1430.3159 - accuracy: 0.7318 - val_loss: 1429.7407 - val_accuracy: 0.7538\n",
      "Epoch 55/200\n",
      "learning rate scheduled to 0.0005753549246583134\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1429.2163 - accuracy: 0.7354 - val_loss: 1428.6726 - val_accuracy: 0.7347\n",
      "Epoch 56/200\n",
      "learning rate scheduled to 0.0005696013558190316\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1428.1266 - accuracy: 0.7423 - val_loss: 1427.5834 - val_accuracy: 0.7464\n",
      "Epoch 57/200\n",
      "learning rate scheduled to 0.0005639053549384699\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1427.0515 - accuracy: 0.7421 - val_loss: 1426.5179 - val_accuracy: 0.7425\n",
      "Epoch 58/200\n",
      "learning rate scheduled to 0.0005582662881352008\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1425.9874 - accuracy: 0.7412 - val_loss: 1425.4668 - val_accuracy: 0.7234\n",
      "Epoch 59/200\n",
      "learning rate scheduled to 0.0005526836367789656\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1424.9360 - accuracy: 0.7407 - val_loss: 1424.4032 - val_accuracy: 0.7410\n",
      "Epoch 60/200\n",
      "learning rate scheduled to 0.0005471568246139213\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1423.8971 - accuracy: 0.7408 - val_loss: 1423.3633 - val_accuracy: 0.7459\n",
      "Epoch 61/200\n",
      "learning rate scheduled to 0.000541685275384225\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1422.8647 - accuracy: 0.7434 - val_loss: 1422.3595 - val_accuracy: 0.7291\n",
      "Epoch 62/200\n",
      "learning rate scheduled to 0.0005362684128340334\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1421.8483 - accuracy: 0.7408 - val_loss: 1421.3300 - val_accuracy: 0.7508\n",
      "Epoch 63/200\n",
      "learning rate scheduled to 0.0005309057183330878\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1420.8390 - accuracy: 0.7424 - val_loss: 1420.3521 - val_accuracy: 0.7377\n",
      "Epoch 64/200\n",
      "learning rate scheduled to 0.0005255966732511297\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1419.8370 - accuracy: 0.7483 - val_loss: 1419.3491 - val_accuracy: 0.7438\n",
      "Epoch 65/200\n",
      "learning rate scheduled to 0.0005203407013323158\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1418.8553 - accuracy: 0.7457 - val_loss: 1418.3788 - val_accuracy: 0.7191\n",
      "Epoch 66/200\n",
      "learning rate scheduled to 0.0005151372839463875\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1417.8779 - accuracy: 0.7455 - val_loss: 1417.3843 - val_accuracy: 0.7532\n",
      "Epoch 67/200\n",
      "learning rate scheduled to 0.0005099859024630859\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1416.9117 - accuracy: 0.7475 - val_loss: 1416.4457 - val_accuracy: 0.7438\n",
      "Epoch 68/200\n",
      "learning rate scheduled to 0.0005048860382521525\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1415.9556 - accuracy: 0.7497 - val_loss: 1415.4779 - val_accuracy: 0.7517\n",
      "Epoch 69/200\n",
      "learning rate scheduled to 0.0004998371726833284\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1415.0114 - accuracy: 0.7495 - val_loss: 1414.5400 - val_accuracy: 0.7400\n",
      "Epoch 70/200\n",
      "learning rate scheduled to 0.0004948387871263549\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1414.0732 - accuracy: 0.7536 - val_loss: 1413.6356 - val_accuracy: 0.7251\n",
      "Epoch 71/200\n",
      "learning rate scheduled to 0.0004898904205765575\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1413.1494 - accuracy: 0.7513 - val_loss: 1412.6749 - val_accuracy: 0.7611\n",
      "Epoch 72/200\n",
      "learning rate scheduled to 0.00048499149677809326\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1412.2343 - accuracy: 0.7544 - val_loss: 1411.7728 - val_accuracy: 0.7607\n",
      "Epoch 73/200\n",
      "learning rate scheduled to 0.00048014158353907986\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1411.3280 - accuracy: 0.7558 - val_loss: 1410.8782 - val_accuracy: 0.7492\n",
      "Epoch 74/200\n",
      "learning rate scheduled to 0.00047534016222925855\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1410.4314 - accuracy: 0.7547 - val_loss: 1409.9823 - val_accuracy: 0.7577\n",
      "Epoch 75/200\n",
      "learning rate scheduled to 0.0004705867718439549\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1409.5490 - accuracy: 0.7557 - val_loss: 1409.1305 - val_accuracy: 0.7315\n",
      "Epoch 76/200\n",
      "learning rate scheduled to 0.0004658808937529102\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1408.6715 - accuracy: 0.7545 - val_loss: 1408.2053 - val_accuracy: 0.7790\n",
      "Epoch 77/200\n",
      "learning rate scheduled to 0.0004612220957642421\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1407.8057 - accuracy: 0.7561 - val_loss: 1407.3727 - val_accuracy: 0.7583\n",
      "Epoch 78/200\n",
      "learning rate scheduled to 0.00045660988806048406\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1406.9438 - accuracy: 0.7565 - val_loss: 1406.4998 - val_accuracy: 0.7683\n",
      "Epoch 79/200\n",
      "learning rate scheduled to 0.0004520437808241695\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1406.0945 - accuracy: 0.7591 - val_loss: 1405.6794 - val_accuracy: 0.7623\n",
      "Epoch 80/200\n",
      "learning rate scheduled to 0.0004475233418634161\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1405.2565 - accuracy: 0.7582 - val_loss: 1404.8359 - val_accuracy: 0.7643\n",
      "Epoch 81/200\n",
      "learning rate scheduled to 0.0004430481101735495\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1404.4224 - accuracy: 0.7592 - val_loss: 1403.9932 - val_accuracy: 0.7730\n",
      "Epoch 82/200\n",
      "learning rate scheduled to 0.0004386176247498952\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1403.6021 - accuracy: 0.7604 - val_loss: 1403.1704 - val_accuracy: 0.7753\n",
      "Epoch 83/200\n",
      "learning rate scheduled to 0.00043423145340057087\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1402.7836 - accuracy: 0.7619 - val_loss: 1402.3737 - val_accuracy: 0.7700\n",
      "Epoch 84/200\n",
      "learning rate scheduled to 0.0004298891351209022\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1401.9827 - accuracy: 0.7611 - val_loss: 1401.5635 - val_accuracy: 0.7696\n",
      "Epoch 85/200\n",
      "learning rate scheduled to 0.00042559023771900686\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1401.1790 - accuracy: 0.7664 - val_loss: 1400.7910 - val_accuracy: 0.7609\n",
      "Epoch 86/200\n",
      "learning rate scheduled to 0.0004213343290030025\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1400.3965 - accuracy: 0.7624 - val_loss: 1399.9956 - val_accuracy: 0.7675\n",
      "Epoch 87/200\n",
      "learning rate scheduled to 0.00041712097678100687\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1399.6107 - accuracy: 0.7648 - val_loss: 1399.2184 - val_accuracy: 0.7747\n",
      "Epoch 88/200\n",
      "learning rate scheduled to 0.00041294977767392994\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1398.8433 - accuracy: 0.7670 - val_loss: 1398.4529 - val_accuracy: 0.7698\n",
      "Epoch 89/200\n",
      "learning rate scheduled to 0.0004088202706770971\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1398.0789 - accuracy: 0.7658 - val_loss: 1397.7241 - val_accuracy: 0.7494\n",
      "Epoch 90/200\n",
      "learning rate scheduled to 0.00040473208122421056\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1397.3246 - accuracy: 0.7650 - val_loss: 1396.9454 - val_accuracy: 0.7734\n",
      "Epoch 91/200\n",
      "learning rate scheduled to 0.0004006847483105957\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1396.5729 - accuracy: 0.7716 - val_loss: 1396.1936 - val_accuracy: 0.7666\n",
      "Epoch 92/200\n",
      "learning rate scheduled to 0.0003966778973699547\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1395.8354 - accuracy: 0.7674 - val_loss: 1395.4524 - val_accuracy: 0.7773\n",
      "Epoch 93/200\n",
      "learning rate scheduled to 0.0003927111250231974\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1395.1027 - accuracy: 0.7700 - val_loss: 1394.7374 - val_accuracy: 0.7790\n",
      "Epoch 94/200\n",
      "learning rate scheduled to 0.0003887840278912336\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1394.3784 - accuracy: 0.7726 - val_loss: 1394.0200 - val_accuracy: 0.7598\n",
      "Epoch 95/200\n",
      "learning rate scheduled to 0.000384896173782181\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1393.6625 - accuracy: 0.7720 - val_loss: 1393.2960 - val_accuracy: 0.7711\n",
      "Epoch 96/200\n",
      "learning rate scheduled to 0.00038104721694253384\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1392.9540 - accuracy: 0.7690 - val_loss: 1392.5890 - val_accuracy: 0.7787\n",
      "Epoch 97/200\n",
      "learning rate scheduled to 0.000377236753993202\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1392.2524 - accuracy: 0.7715 - val_loss: 1391.8923 - val_accuracy: 0.7755\n",
      "Epoch 98/200\n",
      "learning rate scheduled to 0.0003734643815550953\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1391.5518 - accuracy: 0.7760 - val_loss: 1391.2009 - val_accuracy: 0.7766\n",
      "Epoch 99/200\n",
      "learning rate scheduled to 0.0003697297250619158\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1390.8663 - accuracy: 0.7759 - val_loss: 1390.5237 - val_accuracy: 0.7787\n",
      "Epoch 100/200\n",
      "learning rate scheduled to 0.0003660324387601577\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1390.1864 - accuracy: 0.7729 - val_loss: 1389.8613 - val_accuracy: 0.7704\n",
      "Epoch 101/200\n",
      "learning rate scheduled to 0.00036237211927073074\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1389.5144 - accuracy: 0.7772 - val_loss: 1389.1753 - val_accuracy: 0.7813\n",
      "Epoch 102/200\n",
      "learning rate scheduled to 0.0003587483920273371\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1388.8490 - accuracy: 0.7781 - val_loss: 1388.5099 - val_accuracy: 0.7868\n",
      "Epoch 103/200\n",
      "learning rate scheduled to 0.0003551609112764709\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1388.1910 - accuracy: 0.7759 - val_loss: 1387.8534 - val_accuracy: 0.7824\n",
      "Epoch 104/200\n",
      "learning rate scheduled to 0.0003516093024518341\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1387.5383 - accuracy: 0.7773 - val_loss: 1387.2227 - val_accuracy: 0.7724\n",
      "Epoch 105/200\n",
      "learning rate scheduled to 0.0003480932197999209\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1386.8956 - accuracy: 0.7777 - val_loss: 1386.5726 - val_accuracy: 0.7700\n",
      "Epoch 106/200\n",
      "learning rate scheduled to 0.0003446122887544334\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1386.2585 - accuracy: 0.7760 - val_loss: 1385.9261 - val_accuracy: 0.7930\n",
      "Epoch 107/200\n",
      "learning rate scheduled to 0.00034116616356186566\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1385.6237 - accuracy: 0.7788 - val_loss: 1385.3113 - val_accuracy: 0.7807\n",
      "Epoch 108/200\n",
      "learning rate scheduled to 0.00033775449846871195\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1384.9978 - accuracy: 0.7786 - val_loss: 1384.6826 - val_accuracy: 0.7885\n",
      "Epoch 109/200\n",
      "learning rate scheduled to 0.0003343769477214664\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1384.3831 - accuracy: 0.7789 - val_loss: 1384.0945 - val_accuracy: 0.7551\n",
      "Epoch 110/200\n",
      "learning rate scheduled to 0.0003310331655666232\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1383.7725 - accuracy: 0.7778 - val_loss: 1383.4542 - val_accuracy: 0.7949\n",
      "Epoch 111/200\n",
      "learning rate scheduled to 0.00032772283506346864\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1383.1628 - accuracy: 0.7804 - val_loss: 1382.8774 - val_accuracy: 0.7717\n",
      "Epoch 112/200\n",
      "learning rate scheduled to 0.00032444561045849693\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1382.5648 - accuracy: 0.7824 - val_loss: 1382.2670 - val_accuracy: 0.7798\n",
      "Epoch 113/200\n",
      "learning rate scheduled to 0.00032120114599820226\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1381.9724 - accuracy: 0.7813 - val_loss: 1381.6614 - val_accuracy: 0.7924\n",
      "Epoch 114/200\n",
      "learning rate scheduled to 0.00031798912474187093\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1381.3816 - accuracy: 0.7845 - val_loss: 1381.0846 - val_accuracy: 0.7881\n",
      "Epoch 115/200\n",
      "learning rate scheduled to 0.0003148092297487892\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1380.8009 - accuracy: 0.7824 - val_loss: 1380.5101 - val_accuracy: 0.7773\n",
      "Epoch 116/200\n",
      "learning rate scheduled to 0.0003116611440782435\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1380.2246 - accuracy: 0.7855 - val_loss: 1379.9594 - val_accuracy: 0.7643\n",
      "Epoch 117/200\n",
      "learning rate scheduled to 0.000308544521976728\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1379.6591 - accuracy: 0.7822 - val_loss: 1379.3721 - val_accuracy: 0.7787\n",
      "Epoch 118/200\n",
      "learning rate scheduled to 0.0003054590753163211\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1379.0948 - accuracy: 0.7832 - val_loss: 1378.8143 - val_accuracy: 0.7749\n",
      "Epoch 119/200\n",
      "learning rate scheduled to 0.0003024044871563092\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1378.5391 - accuracy: 0.7840 - val_loss: 1378.2576 - val_accuracy: 0.7870\n",
      "Epoch 120/200\n",
      "learning rate scheduled to 0.00029938044055597855\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1377.9873 - accuracy: 0.7844 - val_loss: 1377.7101 - val_accuracy: 0.7847\n",
      "Epoch 121/200\n",
      "learning rate scheduled to 0.0002963866473874077\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1377.4423 - accuracy: 0.7863 - val_loss: 1377.1692 - val_accuracy: 0.7941\n",
      "Epoch 122/200\n",
      "learning rate scheduled to 0.000293422790709883\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1376.9028 - accuracy: 0.7850 - val_loss: 1376.6221 - val_accuracy: 0.7981\n",
      "Epoch 123/200\n",
      "learning rate scheduled to 0.00029048855358269063\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1376.3711 - accuracy: 0.7831 - val_loss: 1376.0962 - val_accuracy: 0.7881\n",
      "Epoch 124/200\n",
      "learning rate scheduled to 0.0002875836766907014\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1375.8386 - accuracy: 0.7868 - val_loss: 1375.5647 - val_accuracy: 0.7909\n",
      "Epoch 125/200\n",
      "learning rate scheduled to 0.0002847078430932015\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1375.3165 - accuracy: 0.7852 - val_loss: 1375.0527 - val_accuracy: 0.7881\n",
      "Epoch 126/200\n",
      "learning rate scheduled to 0.0002818607646622695\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1374.7979 - accuracy: 0.7867 - val_loss: 1374.5496 - val_accuracy: 0.7706\n",
      "Epoch 127/200\n",
      "learning rate scheduled to 0.00027904215326998385\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1374.2859 - accuracy: 0.7880 - val_loss: 1374.0280 - val_accuracy: 0.7854\n",
      "Epoch 128/200\n",
      "learning rate scheduled to 0.00027625172078842296\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1373.7782 - accuracy: 0.7884 - val_loss: 1373.5383 - val_accuracy: 0.7868\n",
      "Epoch 129/200\n",
      "learning rate scheduled to 0.0002734892079024576\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1373.2811 - accuracy: 0.7864 - val_loss: 1373.0177 - val_accuracy: 0.7920\n",
      "Epoch 130/200\n",
      "learning rate scheduled to 0.0002707543264841661\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1372.7800 - accuracy: 0.7866 - val_loss: 1372.5345 - val_accuracy: 0.7892\n",
      "Epoch 131/200\n",
      "learning rate scheduled to 0.000268046788405627\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1372.2899 - accuracy: 0.7878 - val_loss: 1372.0372 - val_accuracy: 0.7956\n",
      "Epoch 132/200\n",
      "learning rate scheduled to 0.000265366334351711\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1371.8047 - accuracy: 0.7885 - val_loss: 1371.5647 - val_accuracy: 0.7864\n",
      "Epoch 133/200\n",
      "learning rate scheduled to 0.00026271267619449646\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1371.3218 - accuracy: 0.7905 - val_loss: 1371.0784 - val_accuracy: 0.7968\n",
      "Epoch 134/200\n",
      "learning rate scheduled to 0.00026008555461885406\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1370.8469 - accuracy: 0.7900 - val_loss: 1370.6042 - val_accuracy: 0.7958\n",
      "Epoch 135/200\n",
      "learning rate scheduled to 0.00025748471030965445\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1370.3776 - accuracy: 0.7878 - val_loss: 1370.1443 - val_accuracy: 0.7903\n",
      "Epoch 136/200\n",
      "learning rate scheduled to 0.0002549098551389761\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1369.9099 - accuracy: 0.7882 - val_loss: 1369.6733 - val_accuracy: 0.7830\n",
      "Epoch 137/200\n",
      "learning rate scheduled to 0.00025236075860448183\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1369.4493 - accuracy: 0.7901 - val_loss: 1369.2080 - val_accuracy: 0.8002\n",
      "Epoch 138/200\n",
      "learning rate scheduled to 0.0002498371613910422\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1368.9890 - accuracy: 0.7908 - val_loss: 1368.7582 - val_accuracy: 0.7924\n",
      "Epoch 139/200\n",
      "learning rate scheduled to 0.0002473388041835278\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1368.5350 - accuracy: 0.7908 - val_loss: 1368.3187 - val_accuracy: 0.7875\n",
      "Epoch 140/200\n",
      "learning rate scheduled to 0.0002448654276668094\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1368.0896 - accuracy: 0.7902 - val_loss: 1367.8645 - val_accuracy: 0.7917\n",
      "Epoch 141/200\n",
      "learning rate scheduled to 0.00024241677252575755\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1367.6486 - accuracy: 0.7897 - val_loss: 1367.4257 - val_accuracy: 0.7866\n",
      "Epoch 142/200\n",
      "learning rate scheduled to 0.00023999260825803502\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1367.2098 - accuracy: 0.7911 - val_loss: 1366.9937 - val_accuracy: 0.7917\n",
      "Epoch 143/200\n",
      "learning rate scheduled to 0.00023759267554851249\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1366.7748 - accuracy: 0.7892 - val_loss: 1366.5511 - val_accuracy: 0.7975\n",
      "Epoch 144/200\n",
      "learning rate scheduled to 0.0002352167438948527\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1366.3483 - accuracy: 0.7881 - val_loss: 1366.1333 - val_accuracy: 0.7883\n",
      "Epoch 145/200\n",
      "learning rate scheduled to 0.00023286458279471843\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1365.9229 - accuracy: 0.7907 - val_loss: 1365.7125 - val_accuracy: 0.7864\n",
      "Epoch 146/200\n",
      "learning rate scheduled to 0.00023053593293298035\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1365.5027 - accuracy: 0.7911 - val_loss: 1365.3065 - val_accuracy: 0.7796\n",
      "Epoch 147/200\n",
      "learning rate scheduled to 0.0002282305782136973\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1365.0847 - accuracy: 0.7921 - val_loss: 1364.8807 - val_accuracy: 0.7883\n",
      "Epoch 148/200\n",
      "learning rate scheduled to 0.00022594827372813598\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1364.6733 - accuracy: 0.7952 - val_loss: 1364.4805 - val_accuracy: 0.7902\n",
      "Epoch 149/200\n",
      "learning rate scheduled to 0.00022368878897395915\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1364.2687 - accuracy: 0.7893 - val_loss: 1364.0598 - val_accuracy: 0.7930\n",
      "Epoch 150/200\n",
      "learning rate scheduled to 0.00022145190785522573\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1363.8630 - accuracy: 0.7902 - val_loss: 1363.6476 - val_accuracy: 0.8015\n",
      "Epoch 151/200\n",
      "learning rate scheduled to 0.00021923738546320237\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1363.4647 - accuracy: 0.7909 - val_loss: 1363.2593 - val_accuracy: 0.7915\n",
      "Epoch 152/200\n",
      "learning rate scheduled to 0.00021704500570194797\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1363.0684 - accuracy: 0.7928 - val_loss: 1362.8740 - val_accuracy: 0.7968\n",
      "Epoch 153/200\n",
      "learning rate scheduled to 0.00021487455247552135\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1362.6759 - accuracy: 0.7921 - val_loss: 1362.4761 - val_accuracy: 0.7962\n",
      "Epoch 154/200\n",
      "learning rate scheduled to 0.00021272580968798138\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1362.2875 - accuracy: 0.7936 - val_loss: 1362.0979 - val_accuracy: 0.7892\n",
      "Epoch 155/200\n",
      "learning rate scheduled to 0.00021059854683699086\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1361.9049 - accuracy: 0.7942 - val_loss: 1361.7045 - val_accuracy: 0.7996\n",
      "Epoch 156/200\n",
      "learning rate scheduled to 0.0002084925622330047\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1361.5251 - accuracy: 0.7929 - val_loss: 1361.3285 - val_accuracy: 0.7985\n",
      "Epoch 157/200\n",
      "learning rate scheduled to 0.0002064076397800818\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1361.1531 - accuracy: 0.7937 - val_loss: 1360.9878 - val_accuracy: 0.7696\n",
      "Epoch 158/200\n",
      "learning rate scheduled to 0.000204343563382281\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1360.7814 - accuracy: 0.7926 - val_loss: 1360.5905 - val_accuracy: 0.7960\n",
      "Epoch 159/200\n",
      "learning rate scheduled to 0.0002023001313500572\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1360.4125 - accuracy: 0.7934 - val_loss: 1360.2452 - val_accuracy: 0.7847\n",
      "Epoch 160/200\n",
      "learning rate scheduled to 0.0002002771275874693\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1360.0487 - accuracy: 0.7962 - val_loss: 1359.9019 - val_accuracy: 0.7687\n",
      "Epoch 161/200\n",
      "learning rate scheduled to 0.0001982743504049722\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1359.6895 - accuracy: 0.7955 - val_loss: 1359.4985 - val_accuracy: 0.8056\n",
      "Epoch 162/200\n",
      "learning rate scheduled to 0.00019629161251941696\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1359.3357 - accuracy: 0.7936 - val_loss: 1359.1512 - val_accuracy: 0.8002\n",
      "Epoch 163/200\n",
      "learning rate scheduled to 0.0001943286978348624\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1358.9774 - accuracy: 0.7960 - val_loss: 1358.7944 - val_accuracy: 0.7945\n",
      "Epoch 164/200\n",
      "learning rate scheduled to 0.00019238540466176346\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1358.6318 - accuracy: 0.7918 - val_loss: 1358.4515 - val_accuracy: 0.7986\n",
      "Epoch 165/200\n",
      "learning rate scheduled to 0.00019046154571697116\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1358.2833 - accuracy: 0.7938 - val_loss: 1358.1013 - val_accuracy: 0.8028\n",
      "Epoch 166/200\n",
      "learning rate scheduled to 0.0001885569337173365\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1357.9418 - accuracy: 0.7959 - val_loss: 1357.7667 - val_accuracy: 0.7898\n",
      "Epoch 167/200\n",
      "learning rate scheduled to 0.00018667136697331444\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1357.6040 - accuracy: 0.7933 - val_loss: 1357.4373 - val_accuracy: 0.7934\n",
      "Epoch 168/200\n",
      "learning rate scheduled to 0.00018480465820175596\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1357.2712 - accuracy: 0.7936 - val_loss: 1357.0946 - val_accuracy: 0.7992\n",
      "Epoch 169/200\n",
      "learning rate scheduled to 0.000182956605713116\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1356.9393 - accuracy: 0.7936 - val_loss: 1356.7743 - val_accuracy: 0.7919\n",
      "Epoch 170/200\n",
      "learning rate scheduled to 0.00018112703663064166\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1356.6100 - accuracy: 0.7963 - val_loss: 1356.4377 - val_accuracy: 0.8028\n",
      "Epoch 171/200\n",
      "learning rate scheduled to 0.00017931576367118395\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1356.2865 - accuracy: 0.7922 - val_loss: 1356.1178 - val_accuracy: 0.7975\n",
      "Epoch 172/200\n",
      "learning rate scheduled to 0.0001775225995515939\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1355.9623 - accuracy: 0.7959 - val_loss: 1355.8020 - val_accuracy: 0.7960\n",
      "Epoch 173/200\n",
      "learning rate scheduled to 0.00017574737139511854\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1355.6454 - accuracy: 0.7962 - val_loss: 1355.4958 - val_accuracy: 0.7896\n",
      "Epoch 174/200\n",
      "learning rate scheduled to 0.00017398989191860892\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1355.3329 - accuracy: 0.7943 - val_loss: 1355.1744 - val_accuracy: 0.7977\n",
      "Epoch 175/200\n",
      "learning rate scheduled to 0.00017224998824531213\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1355.0194 - accuracy: 0.7967 - val_loss: 1354.8586 - val_accuracy: 0.8047\n",
      "Epoch 176/200\n",
      "learning rate scheduled to 0.00017052748749847522\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1354.7129 - accuracy: 0.7950 - val_loss: 1354.5573 - val_accuracy: 0.7977\n",
      "Epoch 177/200\n",
      "learning rate scheduled to 0.00016882221680134535\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1354.4061 - accuracy: 0.7947 - val_loss: 1354.2593 - val_accuracy: 0.7924\n",
      "Epoch 178/200\n",
      "learning rate scheduled to 0.00016713398887077346\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1354.1034 - accuracy: 0.7946 - val_loss: 1353.9506 - val_accuracy: 0.7939\n",
      "Epoch 179/200\n",
      "learning rate scheduled to 0.00016546264523640276\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1353.8018 - accuracy: 0.7989 - val_loss: 1353.6405 - val_accuracy: 0.7998\n",
      "Epoch 180/200\n",
      "learning rate scheduled to 0.00016380801302148028\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1353.5048 - accuracy: 0.7971 - val_loss: 1353.3633 - val_accuracy: 0.7888\n",
      "Epoch 181/200\n",
      "learning rate scheduled to 0.00016216993375564926\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1353.2123 - accuracy: 0.7964 - val_loss: 1353.0725 - val_accuracy: 0.7817\n",
      "Epoch 182/200\n",
      "learning rate scheduled to 0.00016054823456215672\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1352.9241 - accuracy: 0.7940 - val_loss: 1352.7775 - val_accuracy: 0.7977\n",
      "Epoch 183/200\n",
      "learning rate scheduled to 0.00015894275697064586\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1352.6340 - accuracy: 0.7958 - val_loss: 1352.4875 - val_accuracy: 0.7941\n",
      "Epoch 184/200\n",
      "learning rate scheduled to 0.00015735332810436374\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1352.3513 - accuracy: 0.7947 - val_loss: 1352.2054 - val_accuracy: 0.7905\n",
      "Epoch 185/200\n",
      "learning rate scheduled to 0.00015577978949295356\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1352.0699 - accuracy: 0.7968 - val_loss: 1351.9360 - val_accuracy: 0.7947\n",
      "Epoch 186/200\n",
      "learning rate scheduled to 0.00015422199707245453\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1351.7932 - accuracy: 0.7956 - val_loss: 1351.6587 - val_accuracy: 0.7920\n",
      "Epoch 187/200\n",
      "learning rate scheduled to 0.00015267977796611375\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1351.5176 - accuracy: 0.7953 - val_loss: 1351.3815 - val_accuracy: 0.7879\n",
      "Epoch 188/200\n",
      "learning rate scheduled to 0.00015115297370357439\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1351.2461 - accuracy: 0.7964 - val_loss: 1351.1014 - val_accuracy: 0.8034\n",
      "Epoch 189/200\n",
      "learning rate scheduled to 0.00014964144022087568\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1350.9766 - accuracy: 0.7956 - val_loss: 1350.8362 - val_accuracy: 0.8002\n",
      "Epoch 190/200\n",
      "learning rate scheduled to 0.00014814501904766075\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1350.7079 - accuracy: 0.7954 - val_loss: 1350.5720 - val_accuracy: 0.7937\n",
      "Epoch 191/200\n",
      "learning rate scheduled to 0.00014666356611996888\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1350.4470 - accuracy: 0.7931 - val_loss: 1350.3146 - val_accuracy: 0.7975\n",
      "Epoch 192/200\n",
      "learning rate scheduled to 0.00014519693737383933\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1350.1842 - accuracy: 0.7954 - val_loss: 1350.0581 - val_accuracy: 0.7960\n",
      "Epoch 193/200\n",
      "learning rate scheduled to 0.0001437449743389152\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1349.9260 - accuracy: 0.7943 - val_loss: 1349.8063 - val_accuracy: 0.7909\n",
      "Epoch 194/200\n",
      "learning rate scheduled to 0.00014230751854483968\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1349.6687 - accuracy: 0.7982 - val_loss: 1349.5514 - val_accuracy: 0.7870\n",
      "Epoch 195/200\n",
      "learning rate scheduled to 0.00014088444033404812\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1349.4161 - accuracy: 0.7967 - val_loss: 1349.2786 - val_accuracy: 0.8011\n",
      "Epoch 196/200\n",
      "learning rate scheduled to 0.0001394755956425797\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1349.1630 - accuracy: 0.7984 - val_loss: 1349.0345 - val_accuracy: 0.7998\n",
      "Epoch 197/200\n",
      "learning rate scheduled to 0.00013808084040647372\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1348.9170 - accuracy: 0.7957 - val_loss: 1348.7848 - val_accuracy: 0.8020\n",
      "Epoch 198/200\n",
      "learning rate scheduled to 0.00013670003056176939\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1348.6669 - accuracy: 0.7979 - val_loss: 1348.5424 - val_accuracy: 0.7885\n",
      "Epoch 199/200\n",
      "learning rate scheduled to 0.000135333036450902\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1348.4230 - accuracy: 0.7962 - val_loss: 1348.2994 - val_accuracy: 0.8005\n",
      "Epoch 200/200\n",
      "learning rate scheduled to 0.00013397969960351474\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1348.1787 - accuracy: 0.7969 - val_loss: 1348.0508 - val_accuracy: 0.8047\n"
     ]
    }
   ],
   "source": [
    "history_original_siamese_model = original_siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                              model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 2s 23ms/step - loss: 1348.0594 - accuracy: 0.7954\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Second Run - 90k Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_90k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_90k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 105, 105, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 96, 96, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_2 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1te8jbgy\" target=\"_blank\">driven-snow-5</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 90k\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_siamese_model_2.compile(loss=config.loss_function,\n",
    "                               optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_90k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "learning rate scheduled to 0.0009900000470224768\n",
      "610/610 [==============================] - 45s 64ms/step - loss: 1507.0350 - accuracy: 0.5532 - val_loss: 1503.3882 - val_accuracy: 0.5779\n",
      "Epoch 2/200\n",
      "learning rate scheduled to 0.000980100086890161\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1499.8073 - accuracy: 0.5696 - val_loss: 1496.2098 - val_accuracy: 0.5619\n",
      "Epoch 3/200\n",
      "learning rate scheduled to 0.0009702991275116801\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1492.6814 - accuracy: 0.5683 - val_loss: 1489.1345 - val_accuracy: 0.5802\n",
      "Epoch 4/200\n",
      "learning rate scheduled to 0.0009605961316265165\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1485.6611 - accuracy: 0.5803 - val_loss: 1482.1699 - val_accuracy: 0.5906\n",
      "Epoch 5/200\n",
      "learning rate scheduled to 0.0009509901772253215\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1478.7444 - accuracy: 0.5917 - val_loss: 1475.3059 - val_accuracy: 0.5928\n",
      "Epoch 6/200\n",
      "learning rate scheduled to 0.0009414802846731617\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1471.9305 - accuracy: 0.6039 - val_loss: 1468.5360 - val_accuracy: 0.6190\n",
      "Epoch 7/200\n",
      "learning rate scheduled to 0.0009320654743351042\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1465.2065 - accuracy: 0.6267 - val_loss: 1461.8644 - val_accuracy: 0.6321\n",
      "Epoch 8/200\n",
      "learning rate scheduled to 0.0009227448242017999\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1458.5757 - accuracy: 0.6484 - val_loss: 1455.2772 - val_accuracy: 0.6582\n",
      "Epoch 9/200\n",
      "learning rate scheduled to 0.0009135173546383158\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1452.0315 - accuracy: 0.6749 - val_loss: 1448.7897 - val_accuracy: 0.6790\n",
      "Epoch 10/200\n",
      "learning rate scheduled to 0.0009043822012608871\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1445.5803 - accuracy: 0.7005 - val_loss: 1442.4174 - val_accuracy: 0.6789\n",
      "Epoch 11/200\n",
      "learning rate scheduled to 0.0008953383844345808\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1439.2347 - accuracy: 0.7116 - val_loss: 1436.0679 - val_accuracy: 0.7206\n",
      "Epoch 12/200\n",
      "learning rate scheduled to 0.000886384982150048\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1432.9813 - accuracy: 0.7225 - val_loss: 1429.9125 - val_accuracy: 0.7014\n",
      "Epoch 13/200\n",
      "learning rate scheduled to 0.0008775211300235242\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1426.8206 - accuracy: 0.7287 - val_loss: 1423.9325 - val_accuracy: 0.5117\n",
      "Epoch 14/200\n",
      "learning rate scheduled to 0.0008687459060456604\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1420.7604 - accuracy: 0.7171 - val_loss: 1417.7642 - val_accuracy: 0.7122\n",
      "Epoch 15/200\n",
      "learning rate scheduled to 0.0008600584458326921\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1414.7615 - accuracy: 0.7413 - val_loss: 1411.9445 - val_accuracy: 0.5789\n",
      "Epoch 16/200\n",
      "learning rate scheduled to 0.0008514578850008547\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1408.8625 - accuracy: 0.7464 - val_loss: 1405.9989 - val_accuracy: 0.7011\n",
      "Epoch 17/200\n",
      "learning rate scheduled to 0.0008429433015407995\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1403.0518 - accuracy: 0.7462 - val_loss: 1400.1442 - val_accuracy: 0.7484\n",
      "Epoch 18/200\n",
      "learning rate scheduled to 0.0008345138886943459\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1397.3131 - accuracy: 0.7526 - val_loss: 1394.4819 - val_accuracy: 0.7435\n",
      "Epoch 19/200\n",
      "learning rate scheduled to 0.0008261687244521454\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1391.6644 - accuracy: 0.7538 - val_loss: 1388.8376 - val_accuracy: 0.7611\n",
      "Epoch 20/200\n",
      "learning rate scheduled to 0.0008179070596816018\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1386.0854 - accuracy: 0.7590 - val_loss: 1383.4348 - val_accuracy: 0.6671\n",
      "Epoch 21/200\n",
      "learning rate scheduled to 0.0008097279723733664\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1380.5967 - accuracy: 0.7584 - val_loss: 1377.8844 - val_accuracy: 0.7514\n",
      "Epoch 22/200\n",
      "learning rate scheduled to 0.000801630713394843\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1375.1671 - accuracy: 0.7691 - val_loss: 1372.4764 - val_accuracy: 0.7705\n",
      "Epoch 23/200\n",
      "learning rate scheduled to 0.0007936144183622674\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1369.8274 - accuracy: 0.7668 - val_loss: 1367.1617 - val_accuracy: 0.7666\n",
      "Epoch 24/200\n",
      "learning rate scheduled to 0.0007856782805174589\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1364.5526 - accuracy: 0.7740 - val_loss: 1361.9307 - val_accuracy: 0.7708\n",
      "Epoch 25/200\n",
      "learning rate scheduled to 0.0007778214931022376\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1359.3542 - accuracy: 0.7776 - val_loss: 1356.8906 - val_accuracy: 0.6782\n",
      "Epoch 26/200\n",
      "learning rate scheduled to 0.0007700433069840073\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1354.2271 - accuracy: 0.7795 - val_loss: 1351.6852 - val_accuracy: 0.7811\n",
      "Epoch 27/200\n",
      "learning rate scheduled to 0.0007623428577790037\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1349.1678 - accuracy: 0.7838 - val_loss: 1346.6478 - val_accuracy: 0.7867\n",
      "Epoch 28/200\n",
      "learning rate scheduled to 0.0007547194539802149\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1344.1853 - accuracy: 0.7830 - val_loss: 1341.7061 - val_accuracy: 0.7760\n",
      "Epoch 29/200\n",
      "learning rate scheduled to 0.0007471722312038764\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1339.2679 - accuracy: 0.7841 - val_loss: 1336.8086 - val_accuracy: 0.7930\n",
      "Epoch 30/200\n",
      "learning rate scheduled to 0.0007397004979429766\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1334.4143 - accuracy: 0.7884 - val_loss: 1332.3622 - val_accuracy: 0.4982\n",
      "Epoch 31/200\n",
      "learning rate scheduled to 0.0007323035050649196\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1329.6329 - accuracy: 0.7857 - val_loss: 1327.2433 - val_accuracy: 0.7890\n",
      "Epoch 32/200\n",
      "learning rate scheduled to 0.000724980445811525\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1324.9077 - accuracy: 0.7906 - val_loss: 1322.5947 - val_accuracy: 0.7646\n",
      "Epoch 33/200\n",
      "learning rate scheduled to 0.0007177306286757812\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1320.2485 - accuracy: 0.7923 - val_loss: 1317.9796 - val_accuracy: 0.7568\n",
      "Epoch 34/200\n",
      "learning rate scheduled to 0.0007105533045250923\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1315.6534 - accuracy: 0.7938 - val_loss: 1313.3619 - val_accuracy: 0.7985\n",
      "Epoch 35/200\n",
      "learning rate scheduled to 0.0007034477818524464\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1311.1200 - accuracy: 0.7961 - val_loss: 1309.2632 - val_accuracy: 0.5048\n",
      "Epoch 36/200\n",
      "learning rate scheduled to 0.000696413311525248\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1306.6477 - accuracy: 0.7966 - val_loss: 1304.4231 - val_accuracy: 0.7940\n",
      "Epoch 37/200\n",
      "learning rate scheduled to 0.0006894492020364851\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1302.2341 - accuracy: 0.7983 - val_loss: 1300.0413 - val_accuracy: 0.7912\n",
      "Epoch 38/200\n",
      "learning rate scheduled to 0.0006825547042535618\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1297.8794 - accuracy: 0.7992 - val_loss: 1295.7295 - val_accuracy: 0.7862\n",
      "Epoch 39/200\n",
      "learning rate scheduled to 0.0006757291842950508\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1293.5837 - accuracy: 0.8019 - val_loss: 1291.4979 - val_accuracy: 0.7638\n",
      "Epoch 40/200\n",
      "learning rate scheduled to 0.0006689718930283561\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1289.3441 - accuracy: 0.8030 - val_loss: 1287.2780 - val_accuracy: 0.7711\n",
      "Epoch 41/200\n",
      "learning rate scheduled to 0.0006622821965720505\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1285.1630 - accuracy: 0.8022 - val_loss: 1283.1990 - val_accuracy: 0.7346\n",
      "Epoch 42/200\n",
      "learning rate scheduled to 0.0006556594034191221\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1281.0383 - accuracy: 0.8036 - val_loss: 1278.9851 - val_accuracy: 0.8039\n",
      "Epoch 43/200\n",
      "learning rate scheduled to 0.0006491028220625594\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1276.9648 - accuracy: 0.8040 - val_loss: 1275.1462 - val_accuracy: 0.6613\n",
      "Epoch 44/200\n",
      "learning rate scheduled to 0.0006426118186209351\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1272.9481 - accuracy: 0.8042 - val_loss: 1271.0267 - val_accuracy: 0.7601\n",
      "Epoch 45/200\n",
      "learning rate scheduled to 0.0006361857015872375\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1268.9812 - accuracy: 0.8060 - val_loss: 1266.9989 - val_accuracy: 0.8101\n",
      "Epoch 46/200\n",
      "learning rate scheduled to 0.0006298238370800391\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1265.0660 - accuracy: 0.8077 - val_loss: 1263.2086 - val_accuracy: 0.7453\n",
      "Epoch 47/200\n",
      "learning rate scheduled to 0.0006235255912179127\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1261.2062 - accuracy: 0.8076 - val_loss: 1259.3153 - val_accuracy: 0.7862\n",
      "Epoch 48/200\n",
      "learning rate scheduled to 0.000617290330119431\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1257.3931 - accuracy: 0.8064 - val_loss: 1255.7924 - val_accuracy: 0.6621\n",
      "Epoch 49/200\n",
      "learning rate scheduled to 0.0006111174199031666\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1253.6295 - accuracy: 0.8077 - val_loss: 1252.0464 - val_accuracy: 0.6237\n",
      "Epoch 50/200\n",
      "learning rate scheduled to 0.0006050062266876921\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1249.9150 - accuracy: 0.8088 - val_loss: 1248.0654 - val_accuracy: 0.8070\n",
      "Epoch 51/200\n",
      "learning rate scheduled to 0.0005989561742171646\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1246.2469 - accuracy: 0.8105 - val_loss: 1244.4200 - val_accuracy: 0.8149\n",
      "Epoch 52/200\n",
      "learning rate scheduled to 0.0005929666286101564\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1242.6287 - accuracy: 0.8113 - val_loss: 1240.8542 - val_accuracy: 0.7861\n",
      "Epoch 53/200\n",
      "learning rate scheduled to 0.0005870369559852406\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1239.0557 - accuracy: 0.8118 - val_loss: 1237.2771 - val_accuracy: 0.8110\n",
      "Epoch 54/200\n",
      "learning rate scheduled to 0.000581166580086574\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1235.5272 - accuracy: 0.8122 - val_loss: 1233.7715 - val_accuracy: 0.8144\n",
      "Epoch 55/200\n",
      "learning rate scheduled to 0.0005753549246583134\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1232.0459 - accuracy: 0.8137 - val_loss: 1230.3989 - val_accuracy: 0.7556\n",
      "Epoch 56/200\n",
      "learning rate scheduled to 0.0005696013558190316\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1228.6112 - accuracy: 0.8122 - val_loss: 1226.8976 - val_accuracy: 0.8139\n",
      "Epoch 57/200\n",
      "learning rate scheduled to 0.0005639053549384699\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1225.2148 - accuracy: 0.8154 - val_loss: 1223.6423 - val_accuracy: 0.7378\n",
      "Epoch 58/200\n",
      "learning rate scheduled to 0.0005582662881352008\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1221.8671 - accuracy: 0.8142 - val_loss: 1220.2166 - val_accuracy: 0.8042\n",
      "Epoch 59/200\n",
      "learning rate scheduled to 0.0005526836367789656\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1218.5553 - accuracy: 0.8171 - val_loss: 1216.9401 - val_accuracy: 0.7960\n",
      "Epoch 60/200\n",
      "learning rate scheduled to 0.0005471568246139213\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1215.2919 - accuracy: 0.8160 - val_loss: 1214.3577 - val_accuracy: 0.5004\n",
      "Epoch 61/200\n",
      "learning rate scheduled to 0.000541685275384225\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1212.0751 - accuracy: 0.8131 - val_loss: 1210.4591 - val_accuracy: 0.8192\n",
      "Epoch 62/200\n",
      "learning rate scheduled to 0.0005362684128340334\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1208.8845 - accuracy: 0.8170 - val_loss: 1207.3274 - val_accuracy: 0.8000\n",
      "Epoch 63/200\n",
      "learning rate scheduled to 0.0005309057183330878\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1205.7416 - accuracy: 0.8178 - val_loss: 1204.2291 - val_accuracy: 0.7894\n",
      "Epoch 64/200\n",
      "learning rate scheduled to 0.0005255966732511297\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1202.6344 - accuracy: 0.8182 - val_loss: 1201.0917 - val_accuracy: 0.8188\n",
      "Epoch 65/200\n",
      "learning rate scheduled to 0.0005203407013323158\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1199.5699 - accuracy: 0.8205 - val_loss: 1198.0327 - val_accuracy: 0.8232\n",
      "Epoch 66/200\n",
      "learning rate scheduled to 0.0005151372839463875\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1196.5426 - accuracy: 0.8198 - val_loss: 1195.1063 - val_accuracy: 0.7798\n",
      "Epoch 67/200\n",
      "learning rate scheduled to 0.0005099859024630859\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1193.5537 - accuracy: 0.8202 - val_loss: 1192.0641 - val_accuracy: 0.8219\n",
      "Epoch 68/200\n",
      "learning rate scheduled to 0.0005048860382521525\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1190.6021 - accuracy: 0.8213 - val_loss: 1189.1271 - val_accuracy: 0.8257\n",
      "Epoch 69/200\n",
      "learning rate scheduled to 0.0004998371726833284\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1187.6862 - accuracy: 0.8217 - val_loss: 1186.3461 - val_accuracy: 0.7636\n",
      "Epoch 70/200\n",
      "learning rate scheduled to 0.0004948387871263549\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1184.8069 - accuracy: 0.8232 - val_loss: 1183.3694 - val_accuracy: 0.8249\n",
      "Epoch 71/200\n",
      "learning rate scheduled to 0.0004898904205765575\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1181.9645 - accuracy: 0.8212 - val_loss: 1180.5573 - val_accuracy: 0.8160\n",
      "Epoch 72/200\n",
      "learning rate scheduled to 0.00048499149677809326\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1179.1549 - accuracy: 0.8236 - val_loss: 1177.7479 - val_accuracy: 0.8287\n",
      "Epoch 73/200\n",
      "learning rate scheduled to 0.00048014158353907986\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1176.3818 - accuracy: 0.8235 - val_loss: 1174.9929 - val_accuracy: 0.8285\n",
      "Epoch 74/200\n",
      "learning rate scheduled to 0.00047534016222925855\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1173.6417 - accuracy: 0.8254 - val_loss: 1172.2737 - val_accuracy: 0.8278\n",
      "Epoch 75/200\n",
      "learning rate scheduled to 0.0004705867718439549\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1170.9366 - accuracy: 0.8248 - val_loss: 1169.6934 - val_accuracy: 0.7668\n",
      "Epoch 76/200\n",
      "learning rate scheduled to 0.0004658808937529102\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1168.2642 - accuracy: 0.8251 - val_loss: 1166.9368 - val_accuracy: 0.8224\n",
      "Epoch 77/200\n",
      "learning rate scheduled to 0.0004612220957642421\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1165.6234 - accuracy: 0.8256 - val_loss: 1164.3176 - val_accuracy: 0.8191\n",
      "Epoch 78/200\n",
      "learning rate scheduled to 0.00045660988806048406\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1163.0153 - accuracy: 0.8261 - val_loss: 1161.7191 - val_accuracy: 0.8261\n",
      "Epoch 79/200\n",
      "learning rate scheduled to 0.0004520437808241695\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1160.4387 - accuracy: 0.8291 - val_loss: 1159.1459 - val_accuracy: 0.8338\n",
      "Epoch 80/200\n",
      "learning rate scheduled to 0.0004475233418634161\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1157.8934 - accuracy: 0.8282 - val_loss: 1156.6318 - val_accuracy: 0.8237\n",
      "Epoch 81/200\n",
      "learning rate scheduled to 0.0004430481101735495\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1155.3828 - accuracy: 0.8286 - val_loss: 1154.2388 - val_accuracy: 0.7659\n",
      "Epoch 82/200\n",
      "learning rate scheduled to 0.0004386176247498952\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1152.8998 - accuracy: 0.8290 - val_loss: 1151.6813 - val_accuracy: 0.8149\n",
      "Epoch 83/200\n",
      "learning rate scheduled to 0.00043423145340057087\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1150.4443 - accuracy: 0.8305 - val_loss: 1149.2217 - val_accuracy: 0.8292\n",
      "Epoch 84/200\n",
      "learning rate scheduled to 0.0004298891351209022\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1148.0221 - accuracy: 0.8306 - val_loss: 1146.8552 - val_accuracy: 0.8079\n",
      "Epoch 85/200\n",
      "learning rate scheduled to 0.00042559023771900686\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1145.6272 - accuracy: 0.8320 - val_loss: 1144.5083 - val_accuracy: 0.7868\n",
      "Epoch 86/200\n",
      "learning rate scheduled to 0.0004213343290030025\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1143.2610 - accuracy: 0.8333 - val_loss: 1142.0750 - val_accuracy: 0.8379\n",
      "Epoch 87/200\n",
      "learning rate scheduled to 0.00041712097678100687\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1140.9270 - accuracy: 0.8323 - val_loss: 1139.7670 - val_accuracy: 0.8286\n",
      "Epoch 88/200\n",
      "learning rate scheduled to 0.00041294977767392994\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1138.6189 - accuracy: 0.8322 - val_loss: 1137.4767 - val_accuracy: 0.8256\n",
      "Epoch 89/200\n",
      "learning rate scheduled to 0.0004088202706770971\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1136.3365 - accuracy: 0.8342 - val_loss: 1135.2081 - val_accuracy: 0.8252\n",
      "Epoch 90/200\n",
      "learning rate scheduled to 0.00040473208122421056\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1134.0814 - accuracy: 0.8363 - val_loss: 1132.9755 - val_accuracy: 0.8236\n",
      "Epoch 91/200\n",
      "learning rate scheduled to 0.0004006847483105957\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1131.8536 - accuracy: 0.8363 - val_loss: 1130.7465 - val_accuracy: 0.8337\n",
      "Epoch 92/200\n",
      "learning rate scheduled to 0.0003966778973699547\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1129.6555 - accuracy: 0.8354 - val_loss: 1128.5698 - val_accuracy: 0.8252\n",
      "Epoch 93/200\n",
      "learning rate scheduled to 0.0003927111250231974\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1127.4822 - accuracy: 0.8356 - val_loss: 1126.3971 - val_accuracy: 0.8343\n",
      "Epoch 94/200\n",
      "learning rate scheduled to 0.0003887840278912336\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1125.3334 - accuracy: 0.8353 - val_loss: 1124.2821 - val_accuracy: 0.8236\n",
      "Epoch 95/200\n",
      "learning rate scheduled to 0.000384896173782181\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1123.2112 - accuracy: 0.8371 - val_loss: 1122.1509 - val_accuracy: 0.8348\n",
      "Epoch 96/200\n",
      "learning rate scheduled to 0.00038104721694253384\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1121.1149 - accuracy: 0.8369 - val_loss: 1120.1112 - val_accuracy: 0.8127\n",
      "Epoch 97/200\n",
      "learning rate scheduled to 0.000377236753993202\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1119.0403 - accuracy: 0.8390 - val_loss: 1118.0361 - val_accuracy: 0.8176\n",
      "Epoch 98/200\n",
      "learning rate scheduled to 0.0003734643815550953\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1116.9944 - accuracy: 0.8385 - val_loss: 1115.9784 - val_accuracy: 0.8341\n",
      "Epoch 99/200\n",
      "learning rate scheduled to 0.0003697297250619158\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1114.9668 - accuracy: 0.8398 - val_loss: 1113.9742 - val_accuracy: 0.8292\n",
      "Epoch 100/200\n",
      "learning rate scheduled to 0.0003660324387601577\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1112.9680 - accuracy: 0.8393 - val_loss: 1111.9747 - val_accuracy: 0.8375\n",
      "Epoch 101/200\n",
      "learning rate scheduled to 0.00036237211927073074\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1110.9917 - accuracy: 0.8406 - val_loss: 1109.9973 - val_accuracy: 0.8464\n",
      "Epoch 102/200\n",
      "learning rate scheduled to 0.0003587483920273371\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1109.0380 - accuracy: 0.8408 - val_loss: 1108.0706 - val_accuracy: 0.8364\n",
      "Epoch 103/200\n",
      "learning rate scheduled to 0.0003551609112764709\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1107.1090 - accuracy: 0.8415 - val_loss: 1106.1476 - val_accuracy: 0.8423\n",
      "Epoch 104/200\n",
      "learning rate scheduled to 0.0003516093024518341\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1105.2012 - accuracy: 0.8410 - val_loss: 1104.2662 - val_accuracy: 0.8316\n",
      "Epoch 105/200\n",
      "learning rate scheduled to 0.0003480932197999209\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1103.3152 - accuracy: 0.8414 - val_loss: 1102.3723 - val_accuracy: 0.8418\n",
      "Epoch 106/200\n",
      "learning rate scheduled to 0.0003446122887544334\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1101.4526 - accuracy: 0.8426 - val_loss: 1100.5234 - val_accuracy: 0.8413\n",
      "Epoch 107/200\n",
      "learning rate scheduled to 0.00034116616356186566\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1099.6096 - accuracy: 0.8422 - val_loss: 1098.7008 - val_accuracy: 0.8373\n",
      "Epoch 108/200\n",
      "learning rate scheduled to 0.00033775449846871195\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1097.7881 - accuracy: 0.8442 - val_loss: 1096.8916 - val_accuracy: 0.8382\n",
      "Epoch 109/200\n",
      "learning rate scheduled to 0.0003343769477214664\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1095.9888 - accuracy: 0.8449 - val_loss: 1095.0864 - val_accuracy: 0.8476\n",
      "Epoch 110/200\n",
      "learning rate scheduled to 0.0003310331655666232\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1094.2123 - accuracy: 0.8446 - val_loss: 1093.3250 - val_accuracy: 0.8441\n",
      "Epoch 111/200\n",
      "learning rate scheduled to 0.00032772283506346864\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1092.4554 - accuracy: 0.8456 - val_loss: 1091.5901 - val_accuracy: 0.8375\n",
      "Epoch 112/200\n",
      "learning rate scheduled to 0.00032444561045849693\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1090.7177 - accuracy: 0.8455 - val_loss: 1089.8679 - val_accuracy: 0.8338\n",
      "Epoch 113/200\n",
      "learning rate scheduled to 0.00032120114599820226\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1089.0028 - accuracy: 0.8455 - val_loss: 1088.2227 - val_accuracy: 0.8072\n",
      "Epoch 114/200\n",
      "learning rate scheduled to 0.00031798912474187093\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1087.3046 - accuracy: 0.8460 - val_loss: 1086.4847 - val_accuracy: 0.8344\n",
      "Epoch 115/200\n",
      "learning rate scheduled to 0.0003148092297487892\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1085.6250 - accuracy: 0.8475 - val_loss: 1084.7831 - val_accuracy: 0.8529\n",
      "Epoch 116/200\n",
      "learning rate scheduled to 0.0003116611440782435\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1083.9661 - accuracy: 0.8457 - val_loss: 1083.1378 - val_accuracy: 0.8496\n",
      "Epoch 117/200\n",
      "learning rate scheduled to 0.000308544521976728\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1082.3269 - accuracy: 0.8474 - val_loss: 1081.5848 - val_accuracy: 0.8068\n",
      "Epoch 118/200\n",
      "learning rate scheduled to 0.0003054590753163211\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1080.7068 - accuracy: 0.8471 - val_loss: 1079.9425 - val_accuracy: 0.8206\n",
      "Epoch 119/200\n",
      "learning rate scheduled to 0.0003024044871563092\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1079.1039 - accuracy: 0.8488 - val_loss: 1078.3130 - val_accuracy: 0.8419\n",
      "Epoch 120/200\n",
      "learning rate scheduled to 0.00029938044055597855\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1077.5222 - accuracy: 0.8490 - val_loss: 1076.7236 - val_accuracy: 0.8556\n",
      "Epoch 121/200\n",
      "learning rate scheduled to 0.0002963866473874077\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1075.9584 - accuracy: 0.8479 - val_loss: 1075.1970 - val_accuracy: 0.8334\n",
      "Epoch 122/200\n",
      "learning rate scheduled to 0.000293422790709883\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1074.4094 - accuracy: 0.8499 - val_loss: 1073.6345 - val_accuracy: 0.8543\n",
      "Epoch 123/200\n",
      "learning rate scheduled to 0.00029048855358269063\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1072.8792 - accuracy: 0.8500 - val_loss: 1072.1644 - val_accuracy: 0.8206\n",
      "Epoch 124/200\n",
      "learning rate scheduled to 0.0002875836766907014\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1071.3691 - accuracy: 0.8499 - val_loss: 1070.6438 - val_accuracy: 0.8343\n",
      "Epoch 125/200\n",
      "learning rate scheduled to 0.0002847078430932015\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1069.8728 - accuracy: 0.8498 - val_loss: 1069.1395 - val_accuracy: 0.8418\n",
      "Epoch 126/200\n",
      "learning rate scheduled to 0.0002818607646622695\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1068.3940 - accuracy: 0.8495 - val_loss: 1067.6678 - val_accuracy: 0.8428\n",
      "Epoch 127/200\n",
      "learning rate scheduled to 0.00027904215326998385\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1066.9331 - accuracy: 0.8506 - val_loss: 1066.2031 - val_accuracy: 0.8496\n",
      "Epoch 128/200\n",
      "learning rate scheduled to 0.00027625172078842296\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1065.4872 - accuracy: 0.8502 - val_loss: 1064.8131 - val_accuracy: 0.8252\n",
      "Epoch 129/200\n",
      "learning rate scheduled to 0.0002734892079024576\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1064.0582 - accuracy: 0.8524 - val_loss: 1063.3521 - val_accuracy: 0.8464\n",
      "Epoch 130/200\n",
      "learning rate scheduled to 0.0002707543264841661\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1062.6478 - accuracy: 0.8520 - val_loss: 1061.9620 - val_accuracy: 0.8401\n",
      "Epoch 131/200\n",
      "learning rate scheduled to 0.000268046788405627\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1061.2539 - accuracy: 0.8506 - val_loss: 1060.5667 - val_accuracy: 0.8438\n",
      "Epoch 132/200\n",
      "learning rate scheduled to 0.000265366334351711\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1059.8719 - accuracy: 0.8519 - val_loss: 1059.2736 - val_accuracy: 0.8019\n",
      "Epoch 133/200\n",
      "learning rate scheduled to 0.00026271267619449646\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1058.5066 - accuracy: 0.8533 - val_loss: 1057.8271 - val_accuracy: 0.8525\n",
      "Epoch 134/200\n",
      "learning rate scheduled to 0.00026008555461885406\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1057.1593 - accuracy: 0.8527 - val_loss: 1056.4939 - val_accuracy: 0.8471\n",
      "Epoch 135/200\n",
      "learning rate scheduled to 0.00025748471030965445\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1055.8246 - accuracy: 0.8537 - val_loss: 1055.1718 - val_accuracy: 0.8444\n",
      "Epoch 136/200\n",
      "learning rate scheduled to 0.0002549098551389761\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1054.5040 - accuracy: 0.8531 - val_loss: 1053.8458 - val_accuracy: 0.8531\n",
      "Epoch 137/200\n",
      "learning rate scheduled to 0.00025236075860448183\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1053.1976 - accuracy: 0.8550 - val_loss: 1052.5500 - val_accuracy: 0.8526\n",
      "Epoch 138/200\n",
      "learning rate scheduled to 0.0002498371613910422\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1051.9077 - accuracy: 0.8542 - val_loss: 1051.2808 - val_accuracy: 0.8414\n",
      "Epoch 139/200\n",
      "learning rate scheduled to 0.0002473388041835278\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1050.6322 - accuracy: 0.8551 - val_loss: 1049.9951 - val_accuracy: 0.8534\n",
      "Epoch 140/200\n",
      "learning rate scheduled to 0.0002448654276668094\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1049.3719 - accuracy: 0.8539 - val_loss: 1048.7483 - val_accuracy: 0.8500\n",
      "Epoch 141/200\n",
      "learning rate scheduled to 0.00024241677252575755\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1048.1259 - accuracy: 0.8541 - val_loss: 1047.5226 - val_accuracy: 0.8443\n",
      "Epoch 142/200\n",
      "learning rate scheduled to 0.00023999260825803502\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1046.8923 - accuracy: 0.8553 - val_loss: 1046.2808 - val_accuracy: 0.8526\n",
      "Epoch 143/200\n",
      "learning rate scheduled to 0.00023759267554851249\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1045.6752 - accuracy: 0.8552 - val_loss: 1045.0979 - val_accuracy: 0.8377\n",
      "Epoch 144/200\n",
      "learning rate scheduled to 0.0002352167438948527\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1044.4696 - accuracy: 0.8553 - val_loss: 1043.8635 - val_accuracy: 0.8566\n",
      "Epoch 145/200\n",
      "learning rate scheduled to 0.00023286458279471843\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1043.2773 - accuracy: 0.8560 - val_loss: 1042.6926 - val_accuracy: 0.8461\n",
      "Epoch 146/200\n",
      "learning rate scheduled to 0.00023053593293298035\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1042.0984 - accuracy: 0.8562 - val_loss: 1041.5103 - val_accuracy: 0.8563\n",
      "Epoch 147/200\n",
      "learning rate scheduled to 0.0002282305782136973\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1040.9335 - accuracy: 0.8560 - val_loss: 1040.3508 - val_accuracy: 0.8587\n",
      "Epoch 148/200\n",
      "learning rate scheduled to 0.00022594827372813598\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1039.7791 - accuracy: 0.8561 - val_loss: 1039.2238 - val_accuracy: 0.8438\n",
      "Epoch 149/200\n",
      "learning rate scheduled to 0.00022368878897395915\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1038.6377 - accuracy: 0.8562 - val_loss: 1038.0745 - val_accuracy: 0.8540\n",
      "Epoch 150/200\n",
      "learning rate scheduled to 0.00022145190785522573\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1037.5084 - accuracy: 0.8564 - val_loss: 1037.0017 - val_accuracy: 0.8244\n",
      "Epoch 151/200\n",
      "learning rate scheduled to 0.00021923738546320237\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1036.3940 - accuracy: 0.8564 - val_loss: 1035.8406 - val_accuracy: 0.8538\n",
      "Epoch 152/200\n",
      "learning rate scheduled to 0.00021704500570194797\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1035.2878 - accuracy: 0.8582 - val_loss: 1034.7795 - val_accuracy: 0.8336\n",
      "Epoch 153/200\n",
      "learning rate scheduled to 0.00021487455247552135\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1034.1986 - accuracy: 0.8585 - val_loss: 1033.6752 - val_accuracy: 0.8450\n",
      "Epoch 154/200\n",
      "learning rate scheduled to 0.00021272580968798138\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1033.1208 - accuracy: 0.8589 - val_loss: 1032.6250 - val_accuracy: 0.8319\n",
      "Epoch 155/200\n",
      "learning rate scheduled to 0.00021059854683699086\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1032.0563 - accuracy: 0.8583 - val_loss: 1031.5516 - val_accuracy: 0.8415\n",
      "Epoch 156/200\n",
      "learning rate scheduled to 0.0002084925622330047\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1031.0018 - accuracy: 0.8577 - val_loss: 1030.4819 - val_accuracy: 0.8548\n",
      "Epoch 157/200\n",
      "learning rate scheduled to 0.0002064076397800818\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1029.9581 - accuracy: 0.8598 - val_loss: 1029.4443 - val_accuracy: 0.8558\n",
      "Epoch 158/200\n",
      "learning rate scheduled to 0.000204343563382281\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1028.9277 - accuracy: 0.8596 - val_loss: 1028.4395 - val_accuracy: 0.8423\n",
      "Epoch 159/200\n",
      "learning rate scheduled to 0.0002023001313500572\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1027.9088 - accuracy: 0.8572 - val_loss: 1027.4178 - val_accuracy: 0.8483\n",
      "Epoch 160/200\n",
      "learning rate scheduled to 0.0002002771275874693\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1026.8987 - accuracy: 0.8588 - val_loss: 1026.4012 - val_accuracy: 0.8568\n",
      "Epoch 161/200\n",
      "learning rate scheduled to 0.0001982743504049722\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1025.8995 - accuracy: 0.8597 - val_loss: 1025.4055 - val_accuracy: 0.8556\n",
      "Epoch 162/200\n",
      "learning rate scheduled to 0.00019629161251941696\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1024.9137 - accuracy: 0.8584 - val_loss: 1024.4316 - val_accuracy: 0.8535\n",
      "Epoch 163/200\n",
      "learning rate scheduled to 0.0001943286978348624\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1023.9340 - accuracy: 0.8603 - val_loss: 1023.4388 - val_accuracy: 0.8651\n",
      "Epoch 164/200\n",
      "learning rate scheduled to 0.00019238540466176346\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1022.9664 - accuracy: 0.8603 - val_loss: 1022.5065 - val_accuracy: 0.8476\n",
      "Epoch 165/200\n",
      "learning rate scheduled to 0.00019046154571697116\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1022.0112 - accuracy: 0.8608 - val_loss: 1021.5336 - val_accuracy: 0.8570\n",
      "Epoch 166/200\n",
      "learning rate scheduled to 0.0001885569337173365\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1021.0648 - accuracy: 0.8602 - val_loss: 1020.5953 - val_accuracy: 0.8607\n",
      "Epoch 167/200\n",
      "learning rate scheduled to 0.00018667136697331444\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1020.1299 - accuracy: 0.8612 - val_loss: 1019.6843 - val_accuracy: 0.8501\n",
      "Epoch 168/200\n",
      "learning rate scheduled to 0.00018480465820175596\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1019.2085 - accuracy: 0.8602 - val_loss: 1018.7473 - val_accuracy: 0.8601\n",
      "Epoch 169/200\n",
      "learning rate scheduled to 0.000182956605713116\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1018.2930 - accuracy: 0.8618 - val_loss: 1017.8440 - val_accuracy: 0.8583\n",
      "Epoch 170/200\n",
      "learning rate scheduled to 0.00018112703663064166\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1017.3890 - accuracy: 0.8624 - val_loss: 1017.0034 - val_accuracy: 0.8251\n",
      "Epoch 171/200\n",
      "learning rate scheduled to 0.00017931576367118395\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1016.4966 - accuracy: 0.8605 - val_loss: 1016.0577 - val_accuracy: 0.8573\n",
      "Epoch 172/200\n",
      "learning rate scheduled to 0.0001775225995515939\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1015.6141 - accuracy: 0.8621 - val_loss: 1015.1648 - val_accuracy: 0.8648\n",
      "Epoch 173/200\n",
      "learning rate scheduled to 0.00017574737139511854\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1014.7390 - accuracy: 0.8616 - val_loss: 1014.3000 - val_accuracy: 0.8646\n",
      "Epoch 174/200\n",
      "learning rate scheduled to 0.00017398989191860892\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1013.8730 - accuracy: 0.8635 - val_loss: 1013.4462 - val_accuracy: 0.8633\n",
      "Epoch 175/200\n",
      "learning rate scheduled to 0.00017224998824531213\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1013.0164 - accuracy: 0.8635 - val_loss: 1012.6014 - val_accuracy: 0.8570\n",
      "Epoch 176/200\n",
      "learning rate scheduled to 0.00017052748749847522\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1012.1703 - accuracy: 0.8633 - val_loss: 1011.7854 - val_accuracy: 0.8431\n",
      "Epoch 177/200\n",
      "learning rate scheduled to 0.00016882221680134535\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1011.3341 - accuracy: 0.8631 - val_loss: 1010.9191 - val_accuracy: 0.8597\n",
      "Epoch 178/200\n",
      "learning rate scheduled to 0.00016713398887077346\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1010.5040 - accuracy: 0.8637 - val_loss: 1010.1647 - val_accuracy: 0.8229\n",
      "Epoch 179/200\n",
      "learning rate scheduled to 0.00016546264523640276\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1009.6825 - accuracy: 0.8639 - val_loss: 1009.2731 - val_accuracy: 0.8651\n",
      "Epoch 180/200\n",
      "learning rate scheduled to 0.00016380801302148028\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1008.8704 - accuracy: 0.8623 - val_loss: 1008.4714 - val_accuracy: 0.8579\n",
      "Epoch 181/200\n",
      "learning rate scheduled to 0.00016216993375564926\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1008.0667 - accuracy: 0.8635 - val_loss: 1007.6616 - val_accuracy: 0.8666\n",
      "Epoch 182/200\n",
      "learning rate scheduled to 0.00016054823456215672\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1007.2699 - accuracy: 0.8651 - val_loss: 1006.8674 - val_accuracy: 0.8680\n",
      "Epoch 183/200\n",
      "learning rate scheduled to 0.00015894275697064586\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1006.4849 - accuracy: 0.8640 - val_loss: 1006.1127 - val_accuracy: 0.8503\n",
      "Epoch 184/200\n",
      "learning rate scheduled to 0.00015735332810436374\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1005.7095 - accuracy: 0.8639 - val_loss: 1005.3287 - val_accuracy: 0.8590\n",
      "Epoch 185/200\n",
      "learning rate scheduled to 0.00015577978949295356\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1004.9407 - accuracy: 0.8647 - val_loss: 1004.6424 - val_accuracy: 0.8169\n",
      "Epoch 186/200\n",
      "learning rate scheduled to 0.00015422199707245453\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1004.1827 - accuracy: 0.8644 - val_loss: 1003.8021 - val_accuracy: 0.8650\n",
      "Epoch 187/200\n",
      "learning rate scheduled to 0.00015267977796611375\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1003.4314 - accuracy: 0.8651 - val_loss: 1003.0555 - val_accuracy: 0.8656\n",
      "Epoch 188/200\n",
      "learning rate scheduled to 0.00015115297370357439\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1002.6904 - accuracy: 0.8642 - val_loss: 1002.3366 - val_accuracy: 0.8546\n",
      "Epoch 189/200\n",
      "learning rate scheduled to 0.00014964144022087568\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1001.9560 - accuracy: 0.8641 - val_loss: 1001.5817 - val_accuracy: 0.8683\n",
      "Epoch 190/200\n",
      "learning rate scheduled to 0.00014814501904766075\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1001.2267 - accuracy: 0.8657 - val_loss: 1000.8625 - val_accuracy: 0.8663\n",
      "Epoch 191/200\n",
      "learning rate scheduled to 0.00014666356611996888\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1000.5098 - accuracy: 0.8657 - val_loss: 1000.1672 - val_accuracy: 0.8555\n",
      "Epoch 192/200\n",
      "learning rate scheduled to 0.00014519693737383933\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 999.7970 - accuracy: 0.8659 - val_loss: 999.4362 - val_accuracy: 0.8681\n",
      "Epoch 193/200\n",
      "learning rate scheduled to 0.0001437449743389152\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 999.0930 - accuracy: 0.8666 - val_loss: 998.7509 - val_accuracy: 0.8621\n",
      "Epoch 194/200\n",
      "learning rate scheduled to 0.00014230751854483968\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 998.3973 - accuracy: 0.8667 - val_loss: 998.0515 - val_accuracy: 0.8675\n",
      "Epoch 195/200\n",
      "learning rate scheduled to 0.00014088444033404812\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 997.7075 - accuracy: 0.8669 - val_loss: 997.3656 - val_accuracy: 0.8641\n",
      "Epoch 196/200\n",
      "learning rate scheduled to 0.0001394755956425797\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 997.0245 - accuracy: 0.8666 - val_loss: 996.6989 - val_accuracy: 0.8563\n",
      "Epoch 197/200\n",
      "learning rate scheduled to 0.00013808084040647372\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 996.3481 - accuracy: 0.8672 - val_loss: 996.0480 - val_accuracy: 0.8442\n",
      "Epoch 198/200\n",
      "learning rate scheduled to 0.00013670003056176939\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 995.6803 - accuracy: 0.8672 - val_loss: 995.3484 - val_accuracy: 0.8656\n",
      "Epoch 199/200\n",
      "learning rate scheduled to 0.000135333036450902\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 995.0185 - accuracy: 0.8656 - val_loss: 994.7195 - val_accuracy: 0.8496\n",
      "Epoch 200/200\n",
      "learning rate scheduled to 0.00013397969960351474\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 994.3596 - accuracy: 0.8686 - val_loss: 994.0291 - val_accuracy: 0.8730\n"
     ]
    }
   ],
   "source": [
    "history_original_siamese_model_2 = original_siamese_model_2.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                         model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 7s 20ms/step - loss: 994.0361 - accuracy: 0.8654\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model_2.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Third Run - 150k Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_150k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_150k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 105, 105, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 96, 96, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_3 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschauppi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3flsibxx\" target=\"_blank\">toasty-plant-9</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 150k\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_siamese_model_3.compile(loss=config.loss_function,\n",
    "                                 optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_150k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "learning rate scheduled to 0.0009900000470224768\n",
      "   6/1014 [..............................] - ETA: 56s - loss: 1510.4865 - accuracy: 0.5651WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0250s vs `on_train_batch_end` time: 0.0266s). Check your callbacks.\n",
      "1014/1014 [==============================] - 78s 67ms/step - loss: 1504.4772 - accuracy: 0.5708 - val_loss: 1498.4337 - val_accuracy: 0.5637\n",
      "Epoch 2/200\n",
      "learning rate scheduled to 0.000980100086890161\n",
      "1014/1014 [==============================] - 71s 64ms/step - loss: 1492.4955 - accuracy: 0.5783 - val_loss: 1486.5610 - val_accuracy: 0.5823\n",
      "Epoch 3/200\n",
      "learning rate scheduled to 0.0009702991275116801\n",
      "1014/1014 [==============================] - 72s 65ms/step - loss: 1480.7300 - accuracy: 0.5948 - val_loss: 1474.8993 - val_accuracy: 0.6120\n",
      "Epoch 4/200\n",
      "learning rate scheduled to 0.0009605961316265165\n",
      "1014/1014 [==============================] - 73s 66ms/step - loss: 1469.1669 - accuracy: 0.6262 - val_loss: 1463.4354 - val_accuracy: 0.6393\n",
      "Epoch 5/200\n",
      "learning rate scheduled to 0.0009509901772253215\n",
      "1014/1014 [==============================] - 72s 65ms/step - loss: 1457.7999 - accuracy: 0.6574 - val_loss: 1452.1571 - val_accuracy: 0.6813\n",
      "Epoch 6/200\n",
      "learning rate scheduled to 0.0009414802846731617\n",
      "1014/1014 [==============================] - 72s 65ms/step - loss: 1446.6146 - accuracy: 0.7006 - val_loss: 1441.0907 - val_accuracy: 0.7050\n",
      "Epoch 7/200\n",
      "learning rate scheduled to 0.0009320654743351042\n",
      "1014/1014 [==============================] - 71s 65ms/step - loss: 1435.6454 - accuracy: 0.7197 - val_loss: 1430.2236 - val_accuracy: 0.7165\n",
      "Epoch 8/200\n",
      "learning rate scheduled to 0.0009227448242017999\n",
      "1014/1014 [==============================] - 71s 65ms/step - loss: 1424.8723 - accuracy: 0.7316 - val_loss: 1419.5322 - val_accuracy: 0.7320\n",
      "Epoch 9/200\n",
      "learning rate scheduled to 0.0009135173546383158\n",
      "1014/1014 [==============================] - 72s 66ms/step - loss: 1414.2921 - accuracy: 0.7409 - val_loss: 1409.0358 - val_accuracy: 0.7516\n",
      "Epoch 10/200\n",
      "learning rate scheduled to 0.0009043822012608871\n",
      "1014/1014 [==============================] - 71s 64ms/step - loss: 1403.9670 - accuracy: 0.6635 - val_loss: 1398.9020 - val_accuracy: 0.5894\n",
      "Epoch 11/200\n",
      "learning rate scheduled to 0.0008953383844345808\n",
      "1014/1014 [==============================] - 71s 64ms/step - loss: 1393.8195 - accuracy: 0.6391 - val_loss: 1388.7133 - val_accuracy: 0.6977\n",
      "Epoch 12/200\n",
      "learning rate scheduled to 0.000886384982150048\n",
      "1014/1014 [==============================] - 71s 64ms/step - loss: 1383.7073 - accuracy: 0.7172 - val_loss: 1378.7113 - val_accuracy: 0.7345\n",
      "Epoch 13/200\n",
      "learning rate scheduled to 0.0008775211300235242\n",
      "1014/1014 [==============================] - 71s 64ms/step - loss: 1373.8098 - accuracy: 0.7449 - val_loss: 1368.9026 - val_accuracy: 0.7535\n",
      "Epoch 14/200\n",
      "learning rate scheduled to 0.0008687459060456604\n",
      "1014/1014 [==============================] - 70s 64ms/step - loss: 1364.0985 - accuracy: 0.7550 - val_loss: 1359.2681 - val_accuracy: 0.7731\n",
      "Epoch 15/200\n",
      "learning rate scheduled to 0.0008600584458326921\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1354.5739 - accuracy: 0.7434 - val_loss: 1350.0337 - val_accuracy: 0.5164\n",
      "Epoch 16/200\n",
      "learning rate scheduled to 0.0008514578850008547\n",
      "1014/1014 [==============================] - 70s 63ms/step - loss: 1345.3564 - accuracy: 0.5978 - val_loss: 1340.6899 - val_accuracy: 0.6348\n",
      "Epoch 17/200\n",
      "learning rate scheduled to 0.0008429433015407995\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1336.0804 - accuracy: 0.6870 - val_loss: 1331.4780 - val_accuracy: 0.7151\n",
      "Epoch 18/200\n",
      "learning rate scheduled to 0.0008345138886943459\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1326.9739 - accuracy: 0.7282 - val_loss: 1322.4725 - val_accuracy: 0.7401\n",
      "Epoch 19/200\n",
      "learning rate scheduled to 0.0008261687244521454\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1318.0491 - accuracy: 0.7397 - val_loss: 1313.6178 - val_accuracy: 0.7480\n",
      "Epoch 20/200\n",
      "learning rate scheduled to 0.0008179070596816018\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1309.2700 - accuracy: 0.7530 - val_loss: 1304.9371 - val_accuracy: 0.7447\n",
      "Epoch 21/200\n",
      "learning rate scheduled to 0.0008097279723733664\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1300.6346 - accuracy: 0.7692 - val_loss: 1296.3755 - val_accuracy: 0.7631\n",
      "Epoch 22/200\n",
      "learning rate scheduled to 0.000801630713394843\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1292.1501 - accuracy: 0.7815 - val_loss: 1287.9397 - val_accuracy: 0.7832\n",
      "Epoch 23/200\n",
      "learning rate scheduled to 0.0007936144183622674\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1283.8434 - accuracy: 0.7588 - val_loss: 1279.7241 - val_accuracy: 0.7559\n",
      "Epoch 24/200\n",
      "learning rate scheduled to 0.0007856782805174589\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1275.6472 - accuracy: 0.7666 - val_loss: 1271.5723 - val_accuracy: 0.7795\n",
      "Epoch 25/200\n",
      "learning rate scheduled to 0.0007778214931022376\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1267.5723 - accuracy: 0.7761 - val_loss: 1263.6071 - val_accuracy: 0.7497\n",
      "Epoch 26/200\n",
      "learning rate scheduled to 0.0007700433069840073\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1259.6334 - accuracy: 0.7819 - val_loss: 1255.6895 - val_accuracy: 0.7894\n",
      "Epoch 27/200\n",
      "learning rate scheduled to 0.0007623428577790037\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1251.8252 - accuracy: 0.7836 - val_loss: 1247.9434 - val_accuracy: 0.7876\n",
      "Epoch 28/200\n",
      "learning rate scheduled to 0.0007547194539802149\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1244.1373 - accuracy: 0.7905 - val_loss: 1240.3503 - val_accuracy: 0.7794\n",
      "Epoch 29/200\n",
      "learning rate scheduled to 0.0007471722312038764\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1236.5779 - accuracy: 0.7943 - val_loss: 1232.8340 - val_accuracy: 0.7886\n",
      "Epoch 30/200\n",
      "learning rate scheduled to 0.0007397004979429766\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1229.1399 - accuracy: 0.7979 - val_loss: 1225.4417 - val_accuracy: 0.8042\n",
      "Epoch 31/200\n",
      "learning rate scheduled to 0.0007323035050649196\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1221.8214 - accuracy: 0.8007 - val_loss: 1218.1849 - val_accuracy: 0.8025\n",
      "Epoch 32/200\n",
      "learning rate scheduled to 0.000724980445811525\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1214.6183 - accuracy: 0.8042 - val_loss: 1211.0380 - val_accuracy: 0.8097\n",
      "Epoch 33/200\n",
      "learning rate scheduled to 0.0007177306286757812\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1207.5299 - accuracy: 0.8056 - val_loss: 1204.0083 - val_accuracy: 0.8110\n",
      "Epoch 34/200\n",
      "learning rate scheduled to 0.0007105533045250923\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1200.5544 - accuracy: 0.8073 - val_loss: 1197.0896 - val_accuracy: 0.8080\n",
      "Epoch 35/200\n",
      "learning rate scheduled to 0.0007034477818524464\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1193.6887 - accuracy: 0.8077 - val_loss: 1190.2687 - val_accuracy: 0.8182\n",
      "Epoch 36/200\n",
      "learning rate scheduled to 0.000696413311525248\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1186.9286 - accuracy: 0.8113 - val_loss: 1183.5688 - val_accuracy: 0.8165\n",
      "Epoch 37/200\n",
      "learning rate scheduled to 0.0006894492020364851\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1180.2737 - accuracy: 0.8134 - val_loss: 1176.9691 - val_accuracy: 0.8170\n",
      "Epoch 38/200\n",
      "learning rate scheduled to 0.0006825547042535618\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1173.7222 - accuracy: 0.8163 - val_loss: 1170.4789 - val_accuracy: 0.8085\n",
      "Epoch 39/200\n",
      "learning rate scheduled to 0.0006757291842950508\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1167.2745 - accuracy: 0.8177 - val_loss: 1164.0692 - val_accuracy: 0.8248\n",
      "Epoch 40/200\n",
      "learning rate scheduled to 0.0006689718930283561\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1160.9229 - accuracy: 0.8215 - val_loss: 1157.7673 - val_accuracy: 0.8229\n",
      "Epoch 41/200\n",
      "learning rate scheduled to 0.0006622821965720505\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1154.6674 - accuracy: 0.8240 - val_loss: 1151.5699 - val_accuracy: 0.8199\n",
      "Epoch 42/200\n",
      "learning rate scheduled to 0.0006556594034191221\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1148.5111 - accuracy: 0.8269 - val_loss: 1145.4685 - val_accuracy: 0.8201\n",
      "Epoch 43/200\n",
      "learning rate scheduled to 0.0006491028220625594\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1142.4465 - accuracy: 0.8300 - val_loss: 1139.4395 - val_accuracy: 0.8269\n",
      "Epoch 44/200\n",
      "learning rate scheduled to 0.0006426118186209351\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1136.4751 - accuracy: 0.8332 - val_loss: 1133.5186 - val_accuracy: 0.8264\n",
      "Epoch 45/200\n",
      "learning rate scheduled to 0.0006361857015872375\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1130.5952 - accuracy: 0.8348 - val_loss: 1127.6783 - val_accuracy: 0.8352\n",
      "Epoch 46/200\n",
      "learning rate scheduled to 0.0006298238370800391\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1124.8022 - accuracy: 0.8371 - val_loss: 1121.9307 - val_accuracy: 0.8340\n",
      "Epoch 47/200\n",
      "learning rate scheduled to 0.0006235255912179127\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1119.0985 - accuracy: 0.8388 - val_loss: 1116.2787 - val_accuracy: 0.8328\n",
      "Epoch 48/200\n",
      "learning rate scheduled to 0.000617290330119431\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1113.4799 - accuracy: 0.8386 - val_loss: 1110.6912 - val_accuracy: 0.8399\n",
      "Epoch 49/200\n",
      "learning rate scheduled to 0.0006111174199031666\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1107.9452 - accuracy: 0.8407 - val_loss: 1105.2023 - val_accuracy: 0.8376\n",
      "Epoch 50/200\n",
      "learning rate scheduled to 0.0006050062266876921\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1102.4932 - accuracy: 0.8425 - val_loss: 1099.7854 - val_accuracy: 0.8450\n",
      "Epoch 51/200\n",
      "learning rate scheduled to 0.0005989561742171646\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1097.1211 - accuracy: 0.8438 - val_loss: 1094.4706 - val_accuracy: 0.8323\n",
      "Epoch 52/200\n",
      "learning rate scheduled to 0.0005929666286101564\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1091.8301 - accuracy: 0.8439 - val_loss: 1089.2024 - val_accuracy: 0.8485\n",
      "Epoch 53/200\n",
      "learning rate scheduled to 0.0005870369559852406\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1086.6177 - accuracy: 0.8465 - val_loss: 1084.0308 - val_accuracy: 0.8454\n",
      "Epoch 54/200\n",
      "learning rate scheduled to 0.000581166580086574\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1081.4801 - accuracy: 0.8465 - val_loss: 1078.9292 - val_accuracy: 0.8485\n",
      "Epoch 55/200\n",
      "learning rate scheduled to 0.0005753549246583134\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1076.4193 - accuracy: 0.8477 - val_loss: 1073.9215 - val_accuracy: 0.8378\n",
      "Epoch 56/200\n",
      "learning rate scheduled to 0.0005696013558190316\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1071.4309 - accuracy: 0.8494 - val_loss: 1068.9554 - val_accuracy: 0.8512\n",
      "Epoch 57/200\n",
      "learning rate scheduled to 0.0005639053549384699\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1066.5167 - accuracy: 0.8507 - val_loss: 1064.0715 - val_accuracy: 0.8529\n",
      "Epoch 58/200\n",
      "learning rate scheduled to 0.0005582662881352008\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1061.6744 - accuracy: 0.8500 - val_loss: 1059.2739 - val_accuracy: 0.8475\n",
      "Epoch 59/200\n",
      "learning rate scheduled to 0.0005526836367789656\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1056.8994 - accuracy: 0.8518 - val_loss: 1054.5272 - val_accuracy: 0.8546\n",
      "Epoch 60/200\n",
      "learning rate scheduled to 0.0005471568246139213\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 1052.1951 - accuracy: 0.8527 - val_loss: 1049.8602 - val_accuracy: 0.8509\n",
      "Epoch 61/200\n",
      "learning rate scheduled to 0.000541685275384225\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1047.5571 - accuracy: 0.8538 - val_loss: 1045.2549 - val_accuracy: 0.8552\n",
      "Epoch 62/200\n",
      "learning rate scheduled to 0.0005362684128340334\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 1042.9888 - accuracy: 0.8536 - val_loss: 1040.7212 - val_accuracy: 0.8519\n",
      "Epoch 63/200\n",
      "learning rate scheduled to 0.0005309057183330878\n",
      "1014/1014 [==============================] - 75s 69ms/step - loss: 1038.4867 - accuracy: 0.8538 - val_loss: 1036.2417 - val_accuracy: 0.8592\n",
      "Epoch 64/200\n",
      "learning rate scheduled to 0.0005255966732511297\n",
      "1014/1014 [==============================] - 72s 65ms/step - loss: 1034.0438 - accuracy: 0.8559 - val_loss: 1031.8309 - val_accuracy: 0.8609\n",
      "Epoch 65/200\n",
      "learning rate scheduled to 0.0005203407013323158\n",
      "1014/1014 [==============================] - 71s 64ms/step - loss: 1029.6655 - accuracy: 0.8566 - val_loss: 1027.4863 - val_accuracy: 0.8603\n",
      "Epoch 66/200\n",
      "learning rate scheduled to 0.0005151372839463875\n",
      "1014/1014 [==============================] - 71s 64ms/step - loss: 1025.3506 - accuracy: 0.8573 - val_loss: 1023.2032 - val_accuracy: 0.8595\n",
      "Epoch 67/200\n",
      "learning rate scheduled to 0.0005099859024630859\n",
      "1014/1014 [==============================] - 70s 63ms/step - loss: 1021.0964 - accuracy: 0.8581 - val_loss: 1018.9791 - val_accuracy: 0.8616\n",
      "Epoch 68/200\n",
      "learning rate scheduled to 0.0005048860382521525\n",
      "1014/1014 [==============================] - 71s 64ms/step - loss: 1016.9006 - accuracy: 0.8599 - val_loss: 1014.8170 - val_accuracy: 0.8605\n",
      "Epoch 69/200\n",
      "learning rate scheduled to 0.0004998371726833284\n",
      "1014/1014 [==============================] - 70s 63ms/step - loss: 1012.7662 - accuracy: 0.8593 - val_loss: 1010.7083 - val_accuracy: 0.8643\n",
      "Epoch 70/200\n",
      "learning rate scheduled to 0.0004948387871263549\n",
      "1014/1014 [==============================] - 70s 64ms/step - loss: 1008.6887 - accuracy: 0.8605 - val_loss: 1006.6597 - val_accuracy: 0.8623\n",
      "Epoch 71/200\n",
      "learning rate scheduled to 0.0004898904205765575\n",
      "1014/1014 [==============================] - 70s 64ms/step - loss: 1004.6686 - accuracy: 0.8616 - val_loss: 1002.6678 - val_accuracy: 0.8641\n",
      "Epoch 72/200\n",
      "learning rate scheduled to 0.00048499149677809326\n",
      "1014/1014 [==============================] - 70s 64ms/step - loss: 1000.7032 - accuracy: 0.8612 - val_loss: 998.7258 - val_accuracy: 0.8664\n",
      "Epoch 73/200\n",
      "learning rate scheduled to 0.00048014158353907986\n",
      "1014/1014 [==============================] - 71s 64ms/step - loss: 996.7946 - accuracy: 0.8619 - val_loss: 994.8551 - val_accuracy: 0.8596\n",
      "Epoch 74/200\n",
      "learning rate scheduled to 0.00047534016222925855\n",
      "1014/1014 [==============================] - 70s 64ms/step - loss: 992.9402 - accuracy: 0.8626 - val_loss: 991.0275 - val_accuracy: 0.8623\n",
      "Epoch 75/200\n",
      "learning rate scheduled to 0.0004705867718439549\n",
      "1014/1014 [==============================] - 70s 64ms/step - loss: 989.1367 - accuracy: 0.8645 - val_loss: 987.2415 - val_accuracy: 0.8694\n",
      "Epoch 76/200\n",
      "learning rate scheduled to 0.0004658808937529102\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 985.3867 - accuracy: 0.8643 - val_loss: 983.5361 - val_accuracy: 0.8596\n",
      "Epoch 77/200\n",
      "learning rate scheduled to 0.0004612220957642421\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 981.6877 - accuracy: 0.8647 - val_loss: 979.8519 - val_accuracy: 0.8632\n",
      "Epoch 78/200\n",
      "learning rate scheduled to 0.00045660988806048406\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 978.0406 - accuracy: 0.8648 - val_loss: 976.2250 - val_accuracy: 0.8671\n",
      "Epoch 79/200\n",
      "learning rate scheduled to 0.0004520437808241695\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 974.4409 - accuracy: 0.8671 - val_loss: 972.6536 - val_accuracy: 0.8659\n",
      "Epoch 80/200\n",
      "learning rate scheduled to 0.0004475233418634161\n",
      "1014/1014 [==============================] - 68s 62ms/step - loss: 970.8956 - accuracy: 0.8658 - val_loss: 969.1296 - val_accuracy: 0.8673\n",
      "Epoch 81/200\n",
      "learning rate scheduled to 0.0004430481101735495\n",
      "1014/1014 [==============================] - 68s 62ms/step - loss: 967.3931 - accuracy: 0.8674 - val_loss: 965.6502 - val_accuracy: 0.8672\n",
      "Epoch 82/200\n",
      "learning rate scheduled to 0.0004386176247498952\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 963.9427 - accuracy: 0.8669 - val_loss: 962.2278 - val_accuracy: 0.8676\n",
      "Epoch 83/200\n",
      "learning rate scheduled to 0.00043423145340057087\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 960.5339 - accuracy: 0.8677 - val_loss: 958.8486 - val_accuracy: 0.8634\n",
      "Epoch 84/200\n",
      "learning rate scheduled to 0.0004298891351209022\n",
      "1014/1014 [==============================] - 68s 62ms/step - loss: 957.1729 - accuracy: 0.8684 - val_loss: 955.5078 - val_accuracy: 0.8642\n",
      "Epoch 85/200\n",
      "learning rate scheduled to 0.00042559023771900686\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 953.8573 - accuracy: 0.8693 - val_loss: 952.2130 - val_accuracy: 0.8666\n",
      "Epoch 86/200\n",
      "learning rate scheduled to 0.0004213343290030025\n",
      "1014/1014 [==============================] - 68s 62ms/step - loss: 950.5880 - accuracy: 0.8687 - val_loss: 948.9631 - val_accuracy: 0.8682\n",
      "Epoch 87/200\n",
      "learning rate scheduled to 0.00041712097678100687\n",
      "1014/1014 [==============================] - 68s 61ms/step - loss: 947.3603 - accuracy: 0.8704 - val_loss: 945.7653 - val_accuracy: 0.8643\n",
      "Epoch 88/200\n",
      "learning rate scheduled to 0.00041294977767392994\n",
      "1014/1014 [==============================] - 67s 61ms/step - loss: 944.1764 - accuracy: 0.8710 - val_loss: 942.5887 - val_accuracy: 0.8726\n",
      "Epoch 89/200\n",
      "learning rate scheduled to 0.0004088202706770971\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 941.0359 - accuracy: 0.8707 - val_loss: 939.4701 - val_accuracy: 0.8729\n",
      "Epoch 90/200\n",
      "learning rate scheduled to 0.00040473208122421056\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 937.9357 - accuracy: 0.8711 - val_loss: 936.3916 - val_accuracy: 0.8739\n",
      "Epoch 91/200\n",
      "learning rate scheduled to 0.0004006847483105957\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 934.8757 - accuracy: 0.8716 - val_loss: 933.3459 - val_accuracy: 0.8758\n",
      "Epoch 92/200\n",
      "learning rate scheduled to 0.0003966778973699547\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 931.8575 - accuracy: 0.8720 - val_loss: 930.3597 - val_accuracy: 0.8730\n",
      "Epoch 93/200\n",
      "learning rate scheduled to 0.0003927111250231974\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 928.8773 - accuracy: 0.8734 - val_loss: 927.4009 - val_accuracy: 0.8716\n",
      "Epoch 94/200\n",
      "learning rate scheduled to 0.0003887840278912336\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 925.9393 - accuracy: 0.8735 - val_loss: 924.4770 - val_accuracy: 0.8729\n",
      "Epoch 95/200\n",
      "learning rate scheduled to 0.000384896173782181\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 923.0392 - accuracy: 0.8740 - val_loss: 921.5980 - val_accuracy: 0.8732\n",
      "Epoch 96/200\n",
      "learning rate scheduled to 0.00038104721694253384\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 920.1770 - accuracy: 0.8733 - val_loss: 918.7554 - val_accuracy: 0.8730\n",
      "Epoch 97/200\n",
      "learning rate scheduled to 0.000377236753993202\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 917.3518 - accuracy: 0.8744 - val_loss: 915.9442 - val_accuracy: 0.8758\n",
      "Epoch 98/200\n",
      "learning rate scheduled to 0.0003734643815550953\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 914.5611 - accuracy: 0.8736 - val_loss: 913.1732 - val_accuracy: 0.8743\n",
      "Epoch 99/200\n",
      "learning rate scheduled to 0.0003697297250619158\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 911.8071 - accuracy: 0.8758 - val_loss: 910.4343 - val_accuracy: 0.8771\n",
      "Epoch 100/200\n",
      "learning rate scheduled to 0.0003660324387601577\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 909.0903 - accuracy: 0.8749 - val_loss: 907.7424 - val_accuracy: 0.8735\n",
      "Epoch 101/200\n",
      "learning rate scheduled to 0.00036237211927073074\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 906.4101 - accuracy: 0.8752 - val_loss: 905.0762 - val_accuracy: 0.8777\n",
      "Epoch 102/200\n",
      "learning rate scheduled to 0.0003587483920273371\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 903.7634 - accuracy: 0.8765 - val_loss: 902.4498 - val_accuracy: 0.8748\n",
      "Epoch 103/200\n",
      "learning rate scheduled to 0.0003551609112764709\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 901.1505 - accuracy: 0.8767 - val_loss: 899.8575 - val_accuracy: 0.8744\n",
      "Epoch 104/200\n",
      "learning rate scheduled to 0.0003516093024518341\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 898.5721 - accuracy: 0.8763 - val_loss: 897.2979 - val_accuracy: 0.8730\n",
      "Epoch 105/200\n",
      "learning rate scheduled to 0.0003480932197999209\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 896.0246 - accuracy: 0.8774 - val_loss: 894.7598 - val_accuracy: 0.8764\n",
      "Epoch 106/200\n",
      "learning rate scheduled to 0.0003446122887544334\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 893.5126 - accuracy: 0.8766 - val_loss: 892.2669 - val_accuracy: 0.8746\n",
      "Epoch 107/200\n",
      "learning rate scheduled to 0.00034116616356186566\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 891.0281 - accuracy: 0.8781 - val_loss: 889.8167 - val_accuracy: 0.8651\n",
      "Epoch 108/200\n",
      "learning rate scheduled to 0.00033775449846871195\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 888.5771 - accuracy: 0.8785 - val_loss: 887.3527 - val_accuracy: 0.8823\n",
      "Epoch 109/200\n",
      "learning rate scheduled to 0.0003343769477214664\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 886.1590 - accuracy: 0.8785 - val_loss: 884.9501 - val_accuracy: 0.8814\n",
      "Epoch 110/200\n",
      "learning rate scheduled to 0.0003310331655666232\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 883.7710 - accuracy: 0.8788 - val_loss: 882.5873 - val_accuracy: 0.8770\n",
      "Epoch 111/200\n",
      "learning rate scheduled to 0.00032772283506346864\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 881.4131 - accuracy: 0.8795 - val_loss: 880.2445 - val_accuracy: 0.8778\n",
      "Epoch 112/200\n",
      "learning rate scheduled to 0.00032444561045849693\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 879.0873 - accuracy: 0.8800 - val_loss: 877.9219 - val_accuracy: 0.8824\n",
      "Epoch 113/200\n",
      "learning rate scheduled to 0.00032120114599820226\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 876.7881 - accuracy: 0.8790 - val_loss: 875.6371 - val_accuracy: 0.8826\n",
      "Epoch 114/200\n",
      "learning rate scheduled to 0.00031798912474187093\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 874.5186 - accuracy: 0.8801 - val_loss: 873.3893 - val_accuracy: 0.8802\n",
      "Epoch 115/200\n",
      "learning rate scheduled to 0.0003148092297487892\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 872.2761 - accuracy: 0.8806 - val_loss: 871.1566 - val_accuracy: 0.8823\n",
      "Epoch 116/200\n",
      "learning rate scheduled to 0.0003116611440782435\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 870.0620 - accuracy: 0.8811 - val_loss: 868.9590 - val_accuracy: 0.8814\n",
      "Epoch 117/200\n",
      "learning rate scheduled to 0.000308544521976728\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 867.8752 - accuracy: 0.8813 - val_loss: 866.7902 - val_accuracy: 0.8800\n",
      "Epoch 118/200\n",
      "learning rate scheduled to 0.0003054590753163211\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 865.7159 - accuracy: 0.8814 - val_loss: 864.6364 - val_accuracy: 0.8833\n",
      "Epoch 119/200\n",
      "learning rate scheduled to 0.0003024044871563092\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 863.5844 - accuracy: 0.8821 - val_loss: 862.5212 - val_accuracy: 0.8839\n",
      "Epoch 120/200\n",
      "learning rate scheduled to 0.00029938044055597855\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 861.4811 - accuracy: 0.8819 - val_loss: 860.4346 - val_accuracy: 0.8803\n",
      "Epoch 121/200\n",
      "learning rate scheduled to 0.0002963866473874077\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 859.4014 - accuracy: 0.8825 - val_loss: 858.3678 - val_accuracy: 0.8826\n",
      "Epoch 122/200\n",
      "learning rate scheduled to 0.000293422790709883\n",
      "1014/1014 [==============================] - 74s 67ms/step - loss: 857.3497 - accuracy: 0.8822 - val_loss: 856.3292 - val_accuracy: 0.8829\n",
      "Epoch 123/200\n",
      "learning rate scheduled to 0.00029048855358269063\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 855.3219 - accuracy: 0.8831 - val_loss: 854.3126 - val_accuracy: 0.8838\n",
      "Epoch 124/200\n",
      "learning rate scheduled to 0.0002875836766907014\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 853.3188 - accuracy: 0.8839 - val_loss: 852.3250 - val_accuracy: 0.8808\n",
      "Epoch 125/200\n",
      "learning rate scheduled to 0.0002847078430932015\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 851.3395 - accuracy: 0.8832 - val_loss: 850.3611 - val_accuracy: 0.8814\n",
      "Epoch 126/200\n",
      "learning rate scheduled to 0.0002818607646622695\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 849.3860 - accuracy: 0.8829 - val_loss: 848.4164 - val_accuracy: 0.8809\n",
      "Epoch 127/200\n",
      "learning rate scheduled to 0.00027904215326998385\n",
      "1014/1014 [==============================] - 74s 67ms/step - loss: 847.4531 - accuracy: 0.8847 - val_loss: 846.4919 - val_accuracy: 0.8847\n",
      "Epoch 128/200\n",
      "learning rate scheduled to 0.00027625172078842296\n",
      "1014/1014 [==============================] - 68s 62ms/step - loss: 845.5479 - accuracy: 0.8839 - val_loss: 844.6005 - val_accuracy: 0.8837\n",
      "Epoch 129/200\n",
      "learning rate scheduled to 0.0002734892079024576\n",
      "1014/1014 [==============================] - 67s 61ms/step - loss: 843.6643 - accuracy: 0.8844 - val_loss: 842.7239 - val_accuracy: 0.8872\n",
      "Epoch 130/200\n",
      "learning rate scheduled to 0.0002707543264841661\n",
      "1014/1014 [==============================] - 67s 61ms/step - loss: 841.8054 - accuracy: 0.8846 - val_loss: 840.8839 - val_accuracy: 0.8837\n",
      "Epoch 131/200\n",
      "learning rate scheduled to 0.000268046788405627\n",
      "1014/1014 [==============================] - 68s 61ms/step - loss: 839.9678 - accuracy: 0.8847 - val_loss: 839.0527 - val_accuracy: 0.8843\n",
      "Epoch 132/200\n",
      "learning rate scheduled to 0.000265366334351711\n",
      "1014/1014 [==============================] - 68s 61ms/step - loss: 838.1542 - accuracy: 0.8848 - val_loss: 837.2480 - val_accuracy: 0.8861\n",
      "Epoch 133/200\n",
      "learning rate scheduled to 0.00026271267619449646\n",
      "1014/1014 [==============================] - 68s 61ms/step - loss: 836.3619 - accuracy: 0.8852 - val_loss: 835.4756 - val_accuracy: 0.8836\n",
      "Epoch 134/200\n",
      "learning rate scheduled to 0.00026008555461885406\n",
      "1014/1014 [==============================] - 67s 61ms/step - loss: 834.5905 - accuracy: 0.8857 - val_loss: 833.7048 - val_accuracy: 0.8883\n",
      "Epoch 135/200\n",
      "learning rate scheduled to 0.00025748471030965445\n",
      "1014/1014 [==============================] - 68s 61ms/step - loss: 832.8409 - accuracy: 0.8858 - val_loss: 831.9641 - val_accuracy: 0.8906\n",
      "Epoch 136/200\n",
      "learning rate scheduled to 0.0002549098551389761\n",
      "1014/1014 [==============================] - 67s 61ms/step - loss: 831.1101 - accuracy: 0.8861 - val_loss: 830.2487 - val_accuracy: 0.8866\n",
      "Epoch 137/200\n",
      "learning rate scheduled to 0.00025236075860448183\n",
      "1014/1014 [==============================] - 67s 61ms/step - loss: 829.4006 - accuracy: 0.8864 - val_loss: 828.5543 - val_accuracy: 0.8836\n",
      "Epoch 138/200\n",
      "learning rate scheduled to 0.0002498371613910422\n",
      "1014/1014 [==============================] - 68s 61ms/step - loss: 827.7115 - accuracy: 0.8870 - val_loss: 826.8689 - val_accuracy: 0.8873\n",
      "Epoch 139/200\n",
      "learning rate scheduled to 0.0002473388041835278\n",
      "1014/1014 [==============================] - 68s 61ms/step - loss: 826.0449 - accuracy: 0.8872 - val_loss: 825.2180 - val_accuracy: 0.8844\n",
      "Epoch 140/200\n",
      "learning rate scheduled to 0.0002448654276668094\n",
      "1014/1014 [==============================] - 67s 61ms/step - loss: 824.3961 - accuracy: 0.8877 - val_loss: 823.5811 - val_accuracy: 0.8846\n",
      "Epoch 141/200\n",
      "learning rate scheduled to 0.00024241677252575755\n",
      "1014/1014 [==============================] - 67s 61ms/step - loss: 822.7702 - accuracy: 0.8878 - val_loss: 821.9577 - val_accuracy: 0.8919\n",
      "Epoch 142/200\n",
      "learning rate scheduled to 0.00023999260825803502\n",
      "1014/1014 [==============================] - 67s 61ms/step - loss: 821.1635 - accuracy: 0.8879 - val_loss: 820.3641 - val_accuracy: 0.8872\n",
      "Epoch 143/200\n",
      "learning rate scheduled to 0.00023759267554851249\n",
      "1014/1014 [==============================] - 67s 61ms/step - loss: 819.5764 - accuracy: 0.8877 - val_loss: 818.7906 - val_accuracy: 0.8861\n",
      "Epoch 144/200\n",
      "learning rate scheduled to 0.0002352167438948527\n",
      "1014/1014 [==============================] - 67s 61ms/step - loss: 818.0060 - accuracy: 0.8874 - val_loss: 817.2237 - val_accuracy: 0.8888\n",
      "Epoch 145/200\n",
      "learning rate scheduled to 0.00023286458279471843\n",
      "1014/1014 [==============================] - 67s 60ms/step - loss: 816.4547 - accuracy: 0.8882 - val_loss: 815.6844 - val_accuracy: 0.8890\n",
      "Epoch 146/200\n",
      "learning rate scheduled to 0.00023053593293298035\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 814.9223 - accuracy: 0.8889 - val_loss: 814.1595 - val_accuracy: 0.8867\n",
      "Epoch 147/200\n",
      "learning rate scheduled to 0.0002282305782136973\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 813.4076 - accuracy: 0.8882 - val_loss: 812.6544 - val_accuracy: 0.8884\n",
      "Epoch 148/200\n",
      "learning rate scheduled to 0.00022594827372813598\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 811.9092 - accuracy: 0.8890 - val_loss: 811.1645 - val_accuracy: 0.8884\n",
      "Epoch 149/200\n",
      "learning rate scheduled to 0.00022368878897395915\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 810.4279 - accuracy: 0.8893 - val_loss: 809.6944 - val_accuracy: 0.8897\n",
      "Epoch 150/200\n",
      "learning rate scheduled to 0.00022145190785522573\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 808.9639 - accuracy: 0.8896 - val_loss: 808.2369 - val_accuracy: 0.8881\n",
      "Epoch 151/200\n",
      "learning rate scheduled to 0.00021923738546320237\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 807.5212 - accuracy: 0.8889 - val_loss: 806.7988 - val_accuracy: 0.8907\n",
      "Epoch 152/200\n",
      "learning rate scheduled to 0.00021704500570194797\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 806.0949 - accuracy: 0.8886 - val_loss: 805.3841 - val_accuracy: 0.8879\n",
      "Epoch 153/200\n",
      "learning rate scheduled to 0.00021487455247552135\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 804.6822 - accuracy: 0.8901 - val_loss: 803.9739 - val_accuracy: 0.8936\n",
      "Epoch 154/200\n",
      "learning rate scheduled to 0.00021272580968798138\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 803.2890 - accuracy: 0.8897 - val_loss: 802.5922 - val_accuracy: 0.8909\n",
      "Epoch 155/200\n",
      "learning rate scheduled to 0.00021059854683699086\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 801.9119 - accuracy: 0.8905 - val_loss: 801.2265 - val_accuracy: 0.8898\n",
      "Epoch 156/200\n",
      "learning rate scheduled to 0.0002084925622330047\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 800.5506 - accuracy: 0.8909 - val_loss: 799.8730 - val_accuracy: 0.8883\n",
      "Epoch 157/200\n",
      "learning rate scheduled to 0.0002064076397800818\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 799.2064 - accuracy: 0.8903 - val_loss: 798.5394 - val_accuracy: 0.8878\n",
      "Epoch 158/200\n",
      "learning rate scheduled to 0.000204343563382281\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 797.8776 - accuracy: 0.8908 - val_loss: 797.2144 - val_accuracy: 0.8913\n",
      "Epoch 159/200\n",
      "learning rate scheduled to 0.0002023001313500572\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 796.5616 - accuracy: 0.8914 - val_loss: 795.9055 - val_accuracy: 0.8916\n",
      "Epoch 160/200\n",
      "learning rate scheduled to 0.0002002771275874693\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 795.2631 - accuracy: 0.8912 - val_loss: 794.6128 - val_accuracy: 0.8929\n",
      "Epoch 161/200\n",
      "learning rate scheduled to 0.0001982743504049722\n",
      "1014/1014 [==============================] - 75s 68ms/step - loss: 793.9785 - accuracy: 0.8912 - val_loss: 793.3416 - val_accuracy: 0.8886\n",
      "Epoch 162/200\n",
      "learning rate scheduled to 0.00019629161251941696\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 792.7075 - accuracy: 0.8916 - val_loss: 792.0738 - val_accuracy: 0.8944\n",
      "Epoch 163/200\n",
      "learning rate scheduled to 0.0001943286978348624\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 791.4522 - accuracy: 0.8911 - val_loss: 790.8218 - val_accuracy: 0.8943\n",
      "Epoch 164/200\n",
      "learning rate scheduled to 0.00019238540466176346\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 790.2101 - accuracy: 0.8919 - val_loss: 789.5922 - val_accuracy: 0.8918\n",
      "Epoch 165/200\n",
      "learning rate scheduled to 0.00019046154571697116\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 788.9816 - accuracy: 0.8929 - val_loss: 788.3724 - val_accuracy: 0.8919\n",
      "Epoch 166/200\n",
      "learning rate scheduled to 0.0001885569337173365\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 787.7698 - accuracy: 0.8918 - val_loss: 787.1672 - val_accuracy: 0.8914\n",
      "Epoch 167/200\n",
      "learning rate scheduled to 0.00018667136697331444\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 786.5726 - accuracy: 0.8925 - val_loss: 785.9712 - val_accuracy: 0.8949\n",
      "Epoch 168/200\n",
      "learning rate scheduled to 0.00018480465820175596\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 785.3892 - accuracy: 0.8919 - val_loss: 784.7970 - val_accuracy: 0.8913\n",
      "Epoch 169/200\n",
      "learning rate scheduled to 0.000182956605713116\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 784.2177 - accuracy: 0.8936 - val_loss: 783.6370 - val_accuracy: 0.8930\n",
      "Epoch 170/200\n",
      "learning rate scheduled to 0.00018112703663064166\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 783.0618 - accuracy: 0.8932 - val_loss: 782.4856 - val_accuracy: 0.8936\n",
      "Epoch 171/200\n",
      "learning rate scheduled to 0.00017931576367118395\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 781.9198 - accuracy: 0.8928 - val_loss: 781.3572 - val_accuracy: 0.8910\n",
      "Epoch 172/200\n",
      "learning rate scheduled to 0.0001775225995515939\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 780.7898 - accuracy: 0.8927 - val_loss: 780.2228 - val_accuracy: 0.8965\n",
      "Epoch 173/200\n",
      "learning rate scheduled to 0.00017574737139511854\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 779.6728 - accuracy: 0.8932 - val_loss: 779.1207 - val_accuracy: 0.8912\n",
      "Epoch 174/200\n",
      "learning rate scheduled to 0.00017398989191860892\n",
      "1014/1014 [==============================] - 74s 68ms/step - loss: 778.5696 - accuracy: 0.8929 - val_loss: 778.0200 - val_accuracy: 0.8937\n",
      "Epoch 175/200\n",
      "learning rate scheduled to 0.00017224998824531213\n",
      "1014/1014 [==============================] - 71s 64ms/step - loss: 777.4769 - accuracy: 0.8939 - val_loss: 776.9336 - val_accuracy: 0.8951\n",
      "Epoch 176/200\n",
      "learning rate scheduled to 0.00017052748749847522\n",
      "1014/1014 [==============================] - 68s 62ms/step - loss: 776.3969 - accuracy: 0.8933 - val_loss: 775.8597 - val_accuracy: 0.8956\n",
      "Epoch 177/200\n",
      "learning rate scheduled to 0.00016882221680134535\n",
      "1014/1014 [==============================] - 68s 62ms/step - loss: 775.3287 - accuracy: 0.8945 - val_loss: 774.7969 - val_accuracy: 0.8955\n",
      "Epoch 178/200\n",
      "learning rate scheduled to 0.00016713398887077346\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 774.2739 - accuracy: 0.8935 - val_loss: 773.7478 - val_accuracy: 0.8938\n",
      "Epoch 179/200\n",
      "learning rate scheduled to 0.00016546264523640276\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 773.2261 - accuracy: 0.8945 - val_loss: 772.7047 - val_accuracy: 0.8966\n",
      "Epoch 180/200\n",
      "learning rate scheduled to 0.00016380801302148028\n",
      "1014/1014 [==============================] - 68s 62ms/step - loss: 772.1938 - accuracy: 0.8942 - val_loss: 771.6794 - val_accuracy: 0.8945\n",
      "Epoch 181/200\n",
      "learning rate scheduled to 0.00016216993375564926\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 771.1704 - accuracy: 0.8945 - val_loss: 770.6617 - val_accuracy: 0.8952\n",
      "Epoch 182/200\n",
      "learning rate scheduled to 0.00016054823456215672\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 770.1598 - accuracy: 0.8950 - val_loss: 769.6535 - val_accuracy: 0.8967\n",
      "Epoch 183/200\n",
      "learning rate scheduled to 0.00015894275697064586\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 769.1633 - accuracy: 0.8949 - val_loss: 768.6640 - val_accuracy: 0.8951\n",
      "Epoch 184/200\n",
      "learning rate scheduled to 0.00015735332810436374\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 768.1769 - accuracy: 0.8946 - val_loss: 767.6885 - val_accuracy: 0.8936\n",
      "Epoch 185/200\n",
      "learning rate scheduled to 0.00015577978949295356\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 767.2025 - accuracy: 0.8955 - val_loss: 766.7144 - val_accuracy: 0.8967\n",
      "Epoch 186/200\n",
      "learning rate scheduled to 0.00015422199707245453\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 766.2398 - accuracy: 0.8958 - val_loss: 765.7561 - val_accuracy: 0.8979\n",
      "Epoch 187/200\n",
      "learning rate scheduled to 0.00015267977796611375\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 765.2888 - accuracy: 0.8945 - val_loss: 764.8128 - val_accuracy: 0.8951\n",
      "Epoch 188/200\n",
      "learning rate scheduled to 0.00015115297370357439\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 764.3468 - accuracy: 0.8955 - val_loss: 763.8785 - val_accuracy: 0.8948\n",
      "Epoch 189/200\n",
      "learning rate scheduled to 0.00014964144022087568\n",
      "1014/1014 [==============================] - 68s 62ms/step - loss: 763.4161 - accuracy: 0.8953 - val_loss: 762.9564 - val_accuracy: 0.8950\n",
      "Epoch 190/200\n",
      "learning rate scheduled to 0.00014814501904766075\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 762.4955 - accuracy: 0.8959 - val_loss: 762.0336 - val_accuracy: 0.8990\n",
      "Epoch 191/200\n",
      "learning rate scheduled to 0.00014666356611996888\n",
      "1014/1014 [==============================] - 68s 62ms/step - loss: 761.5852 - accuracy: 0.8967 - val_loss: 761.1318 - val_accuracy: 0.8976\n",
      "Epoch 192/200\n",
      "learning rate scheduled to 0.00014519693737383933\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 760.6844 - accuracy: 0.8968 - val_loss: 760.2466 - val_accuracy: 0.8906\n",
      "Epoch 193/200\n",
      "learning rate scheduled to 0.0001437449743389152\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 759.7961 - accuracy: 0.8965 - val_loss: 759.3519 - val_accuracy: 0.8978\n",
      "Epoch 194/200\n",
      "learning rate scheduled to 0.00014230751854483968\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 758.9156 - accuracy: 0.8964 - val_loss: 758.4775 - val_accuracy: 0.8951\n",
      "Epoch 195/200\n",
      "learning rate scheduled to 0.00014088444033404812\n",
      "1014/1014 [==============================] - 73s 66ms/step - loss: 758.0461 - accuracy: 0.8961 - val_loss: 757.6118 - val_accuracy: 0.8951\n",
      "Epoch 196/200\n",
      "learning rate scheduled to 0.0001394755956425797\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 757.1847 - accuracy: 0.8962 - val_loss: 756.7560 - val_accuracy: 0.8976\n",
      "Epoch 197/200\n",
      "learning rate scheduled to 0.00013808084040647372\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 756.3322 - accuracy: 0.8965 - val_loss: 755.9015 - val_accuracy: 0.9007\n",
      "Epoch 198/200\n",
      "learning rate scheduled to 0.00013670003056176939\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 755.4884 - accuracy: 0.8959 - val_loss: 755.0716 - val_accuracy: 0.8936\n",
      "Epoch 199/200\n",
      "learning rate scheduled to 0.000135333036450902\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 754.6528 - accuracy: 0.8965 - val_loss: 754.2381 - val_accuracy: 0.8949\n",
      "Epoch 200/200\n",
      "learning rate scheduled to 0.00013397969960351474\n",
      "1014/1014 [==============================] - 71s 64ms/step - loss: 753.8255 - accuracy: 0.8969 - val_loss: 753.4128 - val_accuracy: 0.8993\n"
     ]
    }
   ],
   "source": [
    "history_original_siamese_model_3 = original_siamese_model_3.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                             model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 11s 21ms/step - loss: 753.4119 - accuracy: 0.8999\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model_3.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Run - 30k Pairs gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_30k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_30k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 105, 105, 1)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 96, 96, 64)        6464      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,947,648\n",
      "Trainable params: 38,947,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_4 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschauppi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1972vkrr\" target=\"_blank\">easy-eon-8</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 30k - Grayscale\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_siamese_model_4.compile(loss=config.loss_function,\n",
    "                                 optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_30k_Gray\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "learning rate scheduled to 0.0009900000470224768\n",
      "  6/203 [..............................] - ETA: 8s - loss: 1510.6082 - accuracy: 0.4905WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0220s vs `on_train_batch_end` time: 0.0229s). Check your callbacks.\n",
      "203/203 [==============================] - 12s 55ms/step - loss: 1509.4403 - accuracy: 0.5015 - val_loss: 1508.2151 - val_accuracy: 0.5015\n",
      "Epoch 2/200\n",
      "learning rate scheduled to 0.000980100086890161\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1507.0282 - accuracy: 0.5073 - val_loss: 1505.8185 - val_accuracy: 0.5021\n",
      "Epoch 3/200\n",
      "learning rate scheduled to 0.0009702991275116801\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1504.6437 - accuracy: 0.5033 - val_loss: 1503.4476 - val_accuracy: 0.4926\n",
      "Epoch 4/200\n",
      "learning rate scheduled to 0.0009605961316265165\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1502.2870 - accuracy: 0.5165 - val_loss: 1501.1044 - val_accuracy: 0.5160\n",
      "Epoch 5/200\n",
      "learning rate scheduled to 0.0009509901772253215\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1499.9572 - accuracy: 0.5246 - val_loss: 1498.7887 - val_accuracy: 0.5206\n",
      "Epoch 6/200\n",
      "learning rate scheduled to 0.0009414802846731617\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1497.6536 - accuracy: 0.5256 - val_loss: 1496.4982 - val_accuracy: 0.5207\n",
      "Epoch 7/200\n",
      "learning rate scheduled to 0.0009320654743351042\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1495.3767 - accuracy: 0.5234 - val_loss: 1494.2349 - val_accuracy: 0.5221\n",
      "Epoch 8/200\n",
      "learning rate scheduled to 0.0009227448242017999\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1493.1257 - accuracy: 0.5302 - val_loss: 1491.9957 - val_accuracy: 0.5241\n",
      "Epoch 9/200\n",
      "learning rate scheduled to 0.0009135173546383158\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1490.8995 - accuracy: 0.5215 - val_loss: 1489.7816 - val_accuracy: 0.5262\n",
      "Epoch 10/200\n",
      "learning rate scheduled to 0.0009043822012608871\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1488.6998 - accuracy: 0.5208 - val_loss: 1487.5941 - val_accuracy: 0.5273\n",
      "Epoch 11/200\n",
      "learning rate scheduled to 0.0008953383844345808\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1486.5243 - accuracy: 0.5319 - val_loss: 1485.4326 - val_accuracy: 0.5515\n",
      "Epoch 12/200\n",
      "learning rate scheduled to 0.000886384982150048\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1484.3737 - accuracy: 0.5488 - val_loss: 1483.2953 - val_accuracy: 0.5485\n",
      "Epoch 13/200\n",
      "learning rate scheduled to 0.0008775211300235242\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1482.2482 - accuracy: 0.5617 - val_loss: 1481.1794 - val_accuracy: 0.5654\n",
      "Epoch 14/200\n",
      "learning rate scheduled to 0.0008687459060456604\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1480.1456 - accuracy: 0.5658 - val_loss: 1479.0908 - val_accuracy: 0.5732\n",
      "Epoch 15/200\n",
      "learning rate scheduled to 0.0008600584458326921\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1478.0690 - accuracy: 0.5710 - val_loss: 1477.0232 - val_accuracy: 0.5843\n",
      "Epoch 16/200\n",
      "learning rate scheduled to 0.0008514578850008547\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1476.0139 - accuracy: 0.5787 - val_loss: 1474.9812 - val_accuracy: 0.5948\n",
      "Epoch 17/200\n",
      "learning rate scheduled to 0.0008429433015407995\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1473.9847 - accuracy: 0.5814 - val_loss: 1472.9683 - val_accuracy: 0.5886\n",
      "Epoch 18/200\n",
      "learning rate scheduled to 0.0008345138886943459\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1471.9773 - accuracy: 0.5859 - val_loss: 1470.9718 - val_accuracy: 0.5843\n",
      "Epoch 19/200\n",
      "learning rate scheduled to 0.0008261687244521454\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1469.9944 - accuracy: 0.5882 - val_loss: 1468.9968 - val_accuracy: 0.5858\n",
      "Epoch 20/200\n",
      "learning rate scheduled to 0.0008179070596816018\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1468.0322 - accuracy: 0.5879 - val_loss: 1467.0482 - val_accuracy: 0.5875\n",
      "Epoch 21/200\n",
      "learning rate scheduled to 0.0008097279723733664\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1466.0930 - accuracy: 0.5921 - val_loss: 1465.1215 - val_accuracy: 0.5920\n",
      "Epoch 22/200\n",
      "learning rate scheduled to 0.000801630713394843\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1464.1755 - accuracy: 0.5935 - val_loss: 1463.2141 - val_accuracy: 0.5990\n",
      "Epoch 23/200\n",
      "learning rate scheduled to 0.0007936144183622674\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1462.2808 - accuracy: 0.5947 - val_loss: 1461.3291 - val_accuracy: 0.5928\n",
      "Epoch 24/200\n",
      "learning rate scheduled to 0.0007856782805174589\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1460.4041 - accuracy: 0.6020 - val_loss: 1459.4640 - val_accuracy: 0.6037\n",
      "Epoch 25/200\n",
      "learning rate scheduled to 0.0007778214931022376\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1458.5511 - accuracy: 0.6038 - val_loss: 1457.6206 - val_accuracy: 0.5963\n",
      "Epoch 26/200\n",
      "learning rate scheduled to 0.0007700433069840073\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1456.7183 - accuracy: 0.6043 - val_loss: 1455.8013 - val_accuracy: 0.6039\n",
      "Epoch 27/200\n",
      "learning rate scheduled to 0.0007623428577790037\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1454.9061 - accuracy: 0.6104 - val_loss: 1453.9946 - val_accuracy: 0.6171\n",
      "Epoch 28/200\n",
      "learning rate scheduled to 0.0007547194539802149\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1453.1136 - accuracy: 0.6137 - val_loss: 1452.2104 - val_accuracy: 0.6254\n",
      "Epoch 29/200\n",
      "learning rate scheduled to 0.0007471722312038764\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1451.3372 - accuracy: 0.6209 - val_loss: 1450.4498 - val_accuracy: 0.6167\n",
      "Epoch 30/200\n",
      "learning rate scheduled to 0.0007397004979429766\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1449.5854 - accuracy: 0.6248 - val_loss: 1448.7058 - val_accuracy: 0.6269\n",
      "Epoch 31/200\n",
      "learning rate scheduled to 0.0007323035050649196\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1447.8516 - accuracy: 0.6313 - val_loss: 1446.9811 - val_accuracy: 0.6393\n",
      "Epoch 32/200\n",
      "learning rate scheduled to 0.000724980445811525\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1446.1365 - accuracy: 0.6344 - val_loss: 1445.2760 - val_accuracy: 0.6286\n",
      "Epoch 33/200\n",
      "learning rate scheduled to 0.0007177306286757812\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1444.4414 - accuracy: 0.6363 - val_loss: 1443.5919 - val_accuracy: 0.6365\n",
      "Epoch 34/200\n",
      "learning rate scheduled to 0.0007105533045250923\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1442.7643 - accuracy: 0.6402 - val_loss: 1441.9241 - val_accuracy: 0.6410\n",
      "Epoch 35/200\n",
      "learning rate scheduled to 0.0007034477818524464\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1441.1075 - accuracy: 0.6400 - val_loss: 1440.2775 - val_accuracy: 0.6369\n",
      "Epoch 36/200\n",
      "learning rate scheduled to 0.000696413311525248\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1439.4672 - accuracy: 0.6430 - val_loss: 1438.6465 - val_accuracy: 0.6423\n",
      "Epoch 37/200\n",
      "learning rate scheduled to 0.0006894492020364851\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1437.8459 - accuracy: 0.6474 - val_loss: 1437.0399 - val_accuracy: 0.6352\n",
      "Epoch 38/200\n",
      "learning rate scheduled to 0.0006825547042535618\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1436.2434 - accuracy: 0.6476 - val_loss: 1435.4417 - val_accuracy: 0.6444\n",
      "Epoch 39/200\n",
      "learning rate scheduled to 0.0006757291842950508\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1434.6582 - accuracy: 0.6483 - val_loss: 1433.8972 - val_accuracy: 0.6297\n",
      "Epoch 40/200\n",
      "learning rate scheduled to 0.0006689718930283561\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1433.0955 - accuracy: 0.6501 - val_loss: 1432.3058 - val_accuracy: 0.6520\n",
      "Epoch 41/200\n",
      "learning rate scheduled to 0.0006622821965720505\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1431.5420 - accuracy: 0.6542 - val_loss: 1430.7660 - val_accuracy: 0.6514\n",
      "Epoch 42/200\n",
      "learning rate scheduled to 0.0006556594034191221\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1430.0093 - accuracy: 0.6545 - val_loss: 1429.2399 - val_accuracy: 0.6621\n",
      "Epoch 43/200\n",
      "learning rate scheduled to 0.0006491028220625594\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1428.4930 - accuracy: 0.6591 - val_loss: 1427.7347 - val_accuracy: 0.6563\n",
      "Epoch 44/200\n",
      "learning rate scheduled to 0.0006426118186209351\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1426.9932 - accuracy: 0.6597 - val_loss: 1426.2444 - val_accuracy: 0.6531\n",
      "Epoch 45/200\n",
      "learning rate scheduled to 0.0006361857015872375\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1425.5115 - accuracy: 0.6606 - val_loss: 1424.7723 - val_accuracy: 0.6597\n",
      "Epoch 46/200\n",
      "learning rate scheduled to 0.0006298238370800391\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1424.0460 - accuracy: 0.6601 - val_loss: 1423.3079 - val_accuracy: 0.6599\n",
      "Epoch 47/200\n",
      "learning rate scheduled to 0.0006235255912179127\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1422.5961 - accuracy: 0.6653 - val_loss: 1421.8700 - val_accuracy: 0.6597\n",
      "Epoch 48/200\n",
      "learning rate scheduled to 0.000617290330119431\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1421.1615 - accuracy: 0.6634 - val_loss: 1420.4432 - val_accuracy: 0.6623\n",
      "Epoch 49/200\n",
      "learning rate scheduled to 0.0006111174199031666\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1419.7446 - accuracy: 0.6647 - val_loss: 1419.0321 - val_accuracy: 0.6712\n",
      "Epoch 50/200\n",
      "learning rate scheduled to 0.0006050062266876921\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1418.3424 - accuracy: 0.6675 - val_loss: 1417.6973 - val_accuracy: 0.6314\n",
      "Epoch 51/200\n",
      "learning rate scheduled to 0.0005989561742171646\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1416.9570 - accuracy: 0.6659 - val_loss: 1416.2668 - val_accuracy: 0.6638\n",
      "Epoch 52/200\n",
      "learning rate scheduled to 0.0005929666286101564\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1415.5850 - accuracy: 0.6667 - val_loss: 1414.9261 - val_accuracy: 0.6488\n",
      "Epoch 53/200\n",
      "learning rate scheduled to 0.0005870369559852406\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1414.2285 - accuracy: 0.6675 - val_loss: 1413.5513 - val_accuracy: 0.6691\n",
      "Epoch 54/200\n",
      "learning rate scheduled to 0.000581166580086574\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1412.8865 - accuracy: 0.6688 - val_loss: 1412.2461 - val_accuracy: 0.6495\n",
      "Epoch 55/200\n",
      "learning rate scheduled to 0.0005753549246583134\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1411.5610 - accuracy: 0.6682 - val_loss: 1410.8932 - val_accuracy: 0.6663\n",
      "Epoch 56/200\n",
      "learning rate scheduled to 0.0005696013558190316\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1410.2473 - accuracy: 0.6709 - val_loss: 1409.5908 - val_accuracy: 0.6595\n",
      "Epoch 57/200\n",
      "learning rate scheduled to 0.0005639053549384699\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1408.9492 - accuracy: 0.6681 - val_loss: 1408.3102 - val_accuracy: 0.6614\n",
      "Epoch 58/200\n",
      "learning rate scheduled to 0.0005582662881352008\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1407.6654 - accuracy: 0.6712 - val_loss: 1407.0249 - val_accuracy: 0.6738\n",
      "Epoch 59/200\n",
      "learning rate scheduled to 0.0005526836367789656\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1406.3962 - accuracy: 0.6699 - val_loss: 1405.7565 - val_accuracy: 0.6689\n",
      "Epoch 60/200\n",
      "learning rate scheduled to 0.0005471568246139213\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1405.1412 - accuracy: 0.6702 - val_loss: 1404.5189 - val_accuracy: 0.6618\n",
      "Epoch 61/200\n",
      "learning rate scheduled to 0.000541685275384225\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1403.8988 - accuracy: 0.6732 - val_loss: 1403.3021 - val_accuracy: 0.6521\n",
      "Epoch 62/200\n",
      "learning rate scheduled to 0.0005362684128340334\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1402.6691 - accuracy: 0.6749 - val_loss: 1402.1143 - val_accuracy: 0.6097\n",
      "Epoch 63/200\n",
      "learning rate scheduled to 0.0005309057183330878\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1401.4563 - accuracy: 0.6687 - val_loss: 1400.8491 - val_accuracy: 0.6691\n",
      "Epoch 64/200\n",
      "learning rate scheduled to 0.0005255966732511297\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1400.2512 - accuracy: 0.6763 - val_loss: 1399.7190 - val_accuracy: 0.6390\n",
      "Epoch 65/200\n",
      "learning rate scheduled to 0.0005203407013323158\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1399.0669 - accuracy: 0.6698 - val_loss: 1398.5291 - val_accuracy: 0.6103\n",
      "Epoch 66/200\n",
      "learning rate scheduled to 0.0005151372839463875\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1397.8867 - accuracy: 0.6740 - val_loss: 1397.3376 - val_accuracy: 0.6521\n",
      "Epoch 67/200\n",
      "learning rate scheduled to 0.0005099859024630859\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1396.7260 - accuracy: 0.6712 - val_loss: 1396.1422 - val_accuracy: 0.6718\n",
      "Epoch 68/200\n",
      "learning rate scheduled to 0.0005048860382521525\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1395.5723 - accuracy: 0.6765 - val_loss: 1395.0714 - val_accuracy: 0.5905\n",
      "Epoch 69/200\n",
      "learning rate scheduled to 0.0004998371726833284\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1394.4355 - accuracy: 0.6753 - val_loss: 1393.8724 - val_accuracy: 0.6704\n",
      "Epoch 70/200\n",
      "learning rate scheduled to 0.0004948387871263549\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1393.3058 - accuracy: 0.6785 - val_loss: 1392.7407 - val_accuracy: 0.6848\n",
      "Epoch 71/200\n",
      "learning rate scheduled to 0.0004898904205765575\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1392.1957 - accuracy: 0.6760 - val_loss: 1391.6606 - val_accuracy: 0.6525\n",
      "Epoch 72/200\n",
      "learning rate scheduled to 0.00048499149677809326\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1391.0951 - accuracy: 0.6742 - val_loss: 1390.5460 - val_accuracy: 0.6702\n",
      "Epoch 73/200\n",
      "learning rate scheduled to 0.00048014158353907986\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1390.0029 - accuracy: 0.6791 - val_loss: 1389.4504 - val_accuracy: 0.6812\n",
      "Epoch 74/200\n",
      "learning rate scheduled to 0.00047534016222925855\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1388.9249 - accuracy: 0.6782 - val_loss: 1388.4384 - val_accuracy: 0.6261\n",
      "Epoch 75/200\n",
      "learning rate scheduled to 0.0004705867718439549\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1387.8583 - accuracy: 0.6804 - val_loss: 1387.3459 - val_accuracy: 0.6659\n",
      "Epoch 76/200\n",
      "learning rate scheduled to 0.0004658808937529102\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1386.8043 - accuracy: 0.6773 - val_loss: 1386.3445 - val_accuracy: 0.6397\n",
      "Epoch 77/200\n",
      "learning rate scheduled to 0.0004612220957642421\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1385.7582 - accuracy: 0.6822 - val_loss: 1385.3202 - val_accuracy: 0.6286\n",
      "Epoch 78/200\n",
      "learning rate scheduled to 0.00045660988806048406\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1384.7280 - accuracy: 0.6777 - val_loss: 1384.2395 - val_accuracy: 0.6659\n",
      "Epoch 79/200\n",
      "learning rate scheduled to 0.0004520437808241695\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1383.7065 - accuracy: 0.6792 - val_loss: 1383.2064 - val_accuracy: 0.6650\n",
      "Epoch 80/200\n",
      "learning rate scheduled to 0.0004475233418634161\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1382.6965 - accuracy: 0.6774 - val_loss: 1382.2761 - val_accuracy: 0.5903\n",
      "Epoch 81/200\n",
      "learning rate scheduled to 0.0004430481101735495\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1381.7013 - accuracy: 0.6745 - val_loss: 1381.2622 - val_accuracy: 0.6386\n",
      "Epoch 82/200\n",
      "learning rate scheduled to 0.0004386176247498952\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1380.7094 - accuracy: 0.6802 - val_loss: 1380.2235 - val_accuracy: 0.6712\n",
      "Epoch 83/200\n",
      "learning rate scheduled to 0.00043423145340057087\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1379.7289 - accuracy: 0.6816 - val_loss: 1379.2933 - val_accuracy: 0.6472\n",
      "Epoch 84/200\n",
      "learning rate scheduled to 0.0004298891351209022\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1378.7628 - accuracy: 0.6776 - val_loss: 1378.2764 - val_accuracy: 0.6733\n",
      "Epoch 85/200\n",
      "learning rate scheduled to 0.00042559023771900686\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1377.8020 - accuracy: 0.6827 - val_loss: 1377.3372 - val_accuracy: 0.6748\n",
      "Epoch 86/200\n",
      "learning rate scheduled to 0.0004213343290030025\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1376.8552 - accuracy: 0.6813 - val_loss: 1376.4315 - val_accuracy: 0.6533\n",
      "Epoch 87/200\n",
      "learning rate scheduled to 0.00041712097678100687\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1375.9205 - accuracy: 0.6807 - val_loss: 1375.5498 - val_accuracy: 0.6259\n",
      "Epoch 88/200\n",
      "learning rate scheduled to 0.00041294977767392994\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1374.9927 - accuracy: 0.6811 - val_loss: 1374.5419 - val_accuracy: 0.6618\n",
      "Epoch 89/200\n",
      "learning rate scheduled to 0.0004088202706770971\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1374.0754 - accuracy: 0.6807 - val_loss: 1373.6178 - val_accuracy: 0.6746\n",
      "Epoch 90/200\n",
      "learning rate scheduled to 0.00040473208122421056\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1373.1680 - accuracy: 0.6827 - val_loss: 1372.7444 - val_accuracy: 0.6561\n",
      "Epoch 91/200\n",
      "learning rate scheduled to 0.0004006847483105957\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1372.2693 - accuracy: 0.6815 - val_loss: 1371.8822 - val_accuracy: 0.6220\n",
      "Epoch 92/200\n",
      "learning rate scheduled to 0.0003966778973699547\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1371.3813 - accuracy: 0.6835 - val_loss: 1370.9305 - val_accuracy: 0.6874\n",
      "Epoch 93/200\n",
      "learning rate scheduled to 0.0003927111250231974\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1370.5018 - accuracy: 0.6859 - val_loss: 1370.0966 - val_accuracy: 0.6484\n",
      "Epoch 94/200\n",
      "learning rate scheduled to 0.0003887840278912336\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1369.6331 - accuracy: 0.6835 - val_loss: 1369.1943 - val_accuracy: 0.6917\n",
      "Epoch 95/200\n",
      "learning rate scheduled to 0.000384896173782181\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1368.7740 - accuracy: 0.6812 - val_loss: 1368.3643 - val_accuracy: 0.6678\n",
      "Epoch 96/200\n",
      "learning rate scheduled to 0.00038104721694253384\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1367.9215 - accuracy: 0.6846 - val_loss: 1367.5090 - val_accuracy: 0.6646\n",
      "Epoch 97/200\n",
      "learning rate scheduled to 0.000377236753993202\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1367.0798 - accuracy: 0.6848 - val_loss: 1366.7057 - val_accuracy: 0.6416\n",
      "Epoch 98/200\n",
      "learning rate scheduled to 0.0003734643815550953\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1366.2479 - accuracy: 0.6847 - val_loss: 1365.8292 - val_accuracy: 0.6802\n",
      "Epoch 99/200\n",
      "learning rate scheduled to 0.0003697297250619158\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1365.4215 - accuracy: 0.6872 - val_loss: 1365.0481 - val_accuracy: 0.6591\n",
      "Epoch 100/200\n",
      "learning rate scheduled to 0.0003660324387601577\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1364.6066 - accuracy: 0.6849 - val_loss: 1364.2988 - val_accuracy: 0.5739\n",
      "Epoch 101/200\n",
      "learning rate scheduled to 0.00036237211927073074\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1363.8068 - accuracy: 0.6778 - val_loss: 1363.3947 - val_accuracy: 0.6838\n",
      "Epoch 102/200\n",
      "learning rate scheduled to 0.0003587483920273371\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1363.0020 - accuracy: 0.6837 - val_loss: 1362.5969 - val_accuracy: 0.6863\n",
      "Epoch 103/200\n",
      "learning rate scheduled to 0.0003551609112764709\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1362.2111 - accuracy: 0.6878 - val_loss: 1361.8169 - val_accuracy: 0.6934\n",
      "Epoch 104/200\n",
      "learning rate scheduled to 0.0003516093024518341\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1361.4301 - accuracy: 0.6872 - val_loss: 1361.0297 - val_accuracy: 0.6917\n",
      "Epoch 105/200\n",
      "learning rate scheduled to 0.0003480932197999209\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1360.6569 - accuracy: 0.6867 - val_loss: 1360.2600 - val_accuracy: 0.6917\n",
      "Epoch 106/200\n",
      "learning rate scheduled to 0.0003446122887544334\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1359.8911 - accuracy: 0.6890 - val_loss: 1359.5043 - val_accuracy: 0.6948\n",
      "Epoch 107/200\n",
      "learning rate scheduled to 0.00034116616356186566\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1359.1338 - accuracy: 0.6874 - val_loss: 1358.8555 - val_accuracy: 0.5754\n",
      "Epoch 108/200\n",
      "learning rate scheduled to 0.00033775449846871195\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1358.3850 - accuracy: 0.6875 - val_loss: 1358.0333 - val_accuracy: 0.6736\n",
      "Epoch 109/200\n",
      "learning rate scheduled to 0.0003343769477214664\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1357.6451 - accuracy: 0.6858 - val_loss: 1357.3302 - val_accuracy: 0.6571\n",
      "Epoch 110/200\n",
      "learning rate scheduled to 0.0003310331655666232\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1356.9111 - accuracy: 0.6876 - val_loss: 1356.5498 - val_accuracy: 0.6855\n",
      "Epoch 111/200\n",
      "learning rate scheduled to 0.00032772283506346864\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1356.1852 - accuracy: 0.6877 - val_loss: 1355.9739 - val_accuracy: 0.5997\n",
      "Epoch 112/200\n",
      "learning rate scheduled to 0.00032444561045849693\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1355.4706 - accuracy: 0.6919 - val_loss: 1355.2352 - val_accuracy: 0.5364\n",
      "Epoch 113/200\n",
      "learning rate scheduled to 0.00032120114599820226\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1354.7782 - accuracy: 0.6658 - val_loss: 1354.4050 - val_accuracy: 0.6829\n",
      "Epoch 114/200\n",
      "learning rate scheduled to 0.00031798912474187093\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1354.0563 - accuracy: 0.6908 - val_loss: 1353.7091 - val_accuracy: 0.6831\n",
      "Epoch 115/200\n",
      "learning rate scheduled to 0.0003148092297487892\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1353.3585 - accuracy: 0.6911 - val_loss: 1353.0134 - val_accuracy: 0.6914\n",
      "Epoch 116/200\n",
      "learning rate scheduled to 0.0003116611440782435\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1352.6670 - accuracy: 0.6942 - val_loss: 1352.3274 - val_accuracy: 0.6887\n",
      "Epoch 117/200\n",
      "learning rate scheduled to 0.000308544521976728\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1351.9882 - accuracy: 0.6900 - val_loss: 1351.6736 - val_accuracy: 0.6702\n",
      "Epoch 118/200\n",
      "learning rate scheduled to 0.0003054590753163211\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1351.3151 - accuracy: 0.6887 - val_loss: 1350.9966 - val_accuracy: 0.6727\n",
      "Epoch 119/200\n",
      "learning rate scheduled to 0.0003024044871563092\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1350.6497 - accuracy: 0.6894 - val_loss: 1350.3218 - val_accuracy: 0.6819\n",
      "Epoch 120/200\n",
      "learning rate scheduled to 0.00029938044055597855\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1349.9883 - accuracy: 0.6903 - val_loss: 1349.6533 - val_accuracy: 0.6983\n",
      "Epoch 121/200\n",
      "learning rate scheduled to 0.0002963866473874077\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1349.3357 - accuracy: 0.6896 - val_loss: 1349.0171 - val_accuracy: 0.6891\n",
      "Epoch 122/200\n",
      "learning rate scheduled to 0.000293422790709883\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1348.6898 - accuracy: 0.6914 - val_loss: 1348.4771 - val_accuracy: 0.6216\n",
      "Epoch 123/200\n",
      "learning rate scheduled to 0.00029048855358269063\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1348.0526 - accuracy: 0.6890 - val_loss: 1347.7946 - val_accuracy: 0.6516\n",
      "Epoch 124/200\n",
      "learning rate scheduled to 0.0002875836766907014\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1347.4198 - accuracy: 0.6894 - val_loss: 1347.1193 - val_accuracy: 0.6721\n",
      "Epoch 125/200\n",
      "learning rate scheduled to 0.0002847078430932015\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1346.7904 - accuracy: 0.6901 - val_loss: 1346.5345 - val_accuracy: 0.6574\n",
      "Epoch 126/200\n",
      "learning rate scheduled to 0.0002818607646622695\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1346.1726 - accuracy: 0.6894 - val_loss: 1345.8525 - val_accuracy: 0.6987\n",
      "Epoch 127/200\n",
      "learning rate scheduled to 0.00027904215326998385\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1345.5575 - accuracy: 0.6928 - val_loss: 1345.2643 - val_accuracy: 0.6812\n",
      "Epoch 128/200\n",
      "learning rate scheduled to 0.00027625172078842296\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1344.9504 - accuracy: 0.6938 - val_loss: 1344.6700 - val_accuracy: 0.6782\n",
      "Epoch 129/200\n",
      "learning rate scheduled to 0.0002734892079024576\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1344.3535 - accuracy: 0.6902 - val_loss: 1344.0530 - val_accuracy: 0.6878\n",
      "Epoch 130/200\n",
      "learning rate scheduled to 0.0002707543264841661\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1343.7582 - accuracy: 0.6922 - val_loss: 1343.6006 - val_accuracy: 0.6092\n",
      "Epoch 131/200\n",
      "learning rate scheduled to 0.000268046788405627\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1343.1754 - accuracy: 0.6878 - val_loss: 1343.0172 - val_accuracy: 0.5206\n",
      "Epoch 132/200\n",
      "learning rate scheduled to 0.000265366334351711\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1342.6758 - accuracy: 0.5856 - val_loss: 1342.3307 - val_accuracy: 0.6695\n",
      "Epoch 133/200\n",
      "learning rate scheduled to 0.00026271267619449646\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1342.0173 - accuracy: 0.6929 - val_loss: 1341.7295 - val_accuracy: 0.6902\n",
      "Epoch 134/200\n",
      "learning rate scheduled to 0.00026008555461885406\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1341.4459 - accuracy: 0.6927 - val_loss: 1341.2146 - val_accuracy: 0.6386\n",
      "Epoch 135/200\n",
      "learning rate scheduled to 0.00025748471030965445\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1340.8816 - accuracy: 0.6947 - val_loss: 1340.5874 - val_accuracy: 0.6982\n",
      "Epoch 136/200\n",
      "learning rate scheduled to 0.0002549098551389761\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1340.3251 - accuracy: 0.6906 - val_loss: 1340.0387 - val_accuracy: 0.6906\n",
      "Epoch 137/200\n",
      "learning rate scheduled to 0.00025236075860448183\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1339.7708 - accuracy: 0.6949 - val_loss: 1339.4993 - val_accuracy: 0.6833\n",
      "Epoch 138/200\n",
      "learning rate scheduled to 0.0002498371613910422\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1339.2234 - accuracy: 0.6943 - val_loss: 1338.9600 - val_accuracy: 0.6814\n",
      "Epoch 139/200\n",
      "learning rate scheduled to 0.0002473388041835278\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1338.6820 - accuracy: 0.6925 - val_loss: 1338.4279 - val_accuracy: 0.6827\n",
      "Epoch 140/200\n",
      "learning rate scheduled to 0.0002448654276668094\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1338.1473 - accuracy: 0.6946 - val_loss: 1337.8855 - val_accuracy: 0.6833\n",
      "Epoch 141/200\n",
      "learning rate scheduled to 0.00024241677252575755\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1337.6183 - accuracy: 0.6932 - val_loss: 1337.3556 - val_accuracy: 0.6925\n",
      "Epoch 142/200\n",
      "learning rate scheduled to 0.00023999260825803502\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1337.0948 - accuracy: 0.6930 - val_loss: 1336.8395 - val_accuracy: 0.6906\n",
      "Epoch 143/200\n",
      "learning rate scheduled to 0.00023759267554851249\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1336.5789 - accuracy: 0.6929 - val_loss: 1336.3173 - val_accuracy: 0.6859\n",
      "Epoch 144/200\n",
      "learning rate scheduled to 0.0002352167438948527\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1336.0624 - accuracy: 0.6970 - val_loss: 1335.8035 - val_accuracy: 0.6940\n",
      "Epoch 145/200\n",
      "learning rate scheduled to 0.00023286458279471843\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1335.5562 - accuracy: 0.6964 - val_loss: 1335.2974 - val_accuracy: 0.7012\n",
      "Epoch 146/200\n",
      "learning rate scheduled to 0.00023053593293298035\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1335.0557 - accuracy: 0.6934 - val_loss: 1334.7961 - val_accuracy: 0.7010\n",
      "Epoch 147/200\n",
      "learning rate scheduled to 0.0002282305782136973\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1334.5569 - accuracy: 0.6964 - val_loss: 1334.3107 - val_accuracy: 0.6982\n",
      "Epoch 148/200\n",
      "learning rate scheduled to 0.00022594827372813598\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1334.0635 - accuracy: 0.6948 - val_loss: 1333.9050 - val_accuracy: 0.6344\n",
      "Epoch 149/200\n",
      "learning rate scheduled to 0.00022368878897395915\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1333.5806 - accuracy: 0.6908 - val_loss: 1333.3330 - val_accuracy: 0.6980\n",
      "Epoch 150/200\n",
      "learning rate scheduled to 0.00022145190785522573\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1333.0942 - accuracy: 0.6950 - val_loss: 1332.8542 - val_accuracy: 0.6900\n",
      "Epoch 151/200\n",
      "learning rate scheduled to 0.00021923738546320237\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1332.6141 - accuracy: 0.6974 - val_loss: 1332.4182 - val_accuracy: 0.6631\n",
      "Epoch 152/200\n",
      "learning rate scheduled to 0.00021704500570194797\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1332.1437 - accuracy: 0.6972 - val_loss: 1331.9066 - val_accuracy: 0.6874\n",
      "Epoch 153/200\n",
      "learning rate scheduled to 0.00021487455247552135\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1331.6769 - accuracy: 0.6974 - val_loss: 1331.4821 - val_accuracy: 0.6702\n",
      "Epoch 154/200\n",
      "learning rate scheduled to 0.00021272580968798138\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1331.2152 - accuracy: 0.6964 - val_loss: 1330.9918 - val_accuracy: 0.6944\n",
      "Epoch 155/200\n",
      "learning rate scheduled to 0.00021059854683699086\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1330.7594 - accuracy: 0.6936 - val_loss: 1330.5311 - val_accuracy: 0.6982\n",
      "Epoch 156/200\n",
      "learning rate scheduled to 0.0002084925622330047\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1330.3064 - accuracy: 0.6951 - val_loss: 1330.1074 - val_accuracy: 0.6750\n",
      "Epoch 157/200\n",
      "learning rate scheduled to 0.0002064076397800818\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1329.8588 - accuracy: 0.6967 - val_loss: 1329.7164 - val_accuracy: 0.6167\n",
      "Epoch 158/200\n",
      "learning rate scheduled to 0.000204343563382281\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1329.4185 - accuracy: 0.6927 - val_loss: 1329.3256 - val_accuracy: 0.5437\n",
      "Epoch 159/200\n",
      "learning rate scheduled to 0.0002023001313500572\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1329.0001 - accuracy: 0.6718 - val_loss: 1328.7578 - val_accuracy: 0.6965\n",
      "Epoch 160/200\n",
      "learning rate scheduled to 0.0002002771275874693\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1328.5421 - accuracy: 0.6999 - val_loss: 1328.3357 - val_accuracy: 0.6872\n",
      "Epoch 161/200\n",
      "learning rate scheduled to 0.0001982743504049722\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1328.1106 - accuracy: 0.6987 - val_loss: 1327.9596 - val_accuracy: 0.6331\n",
      "Epoch 162/200\n",
      "learning rate scheduled to 0.00019629161251941696\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1327.6884 - accuracy: 0.6968 - val_loss: 1327.4810 - val_accuracy: 0.6899\n",
      "Epoch 163/200\n",
      "learning rate scheduled to 0.0001943286978348624\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1327.2654 - accuracy: 0.6964 - val_loss: 1327.0522 - val_accuracy: 0.6980\n",
      "Epoch 164/200\n",
      "learning rate scheduled to 0.00019238540466176346\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1326.8483 - accuracy: 0.6954 - val_loss: 1326.6434 - val_accuracy: 0.6914\n",
      "Epoch 165/200\n",
      "learning rate scheduled to 0.00019046154571697116\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1326.4358 - accuracy: 0.6974 - val_loss: 1326.2461 - val_accuracy: 0.6793\n",
      "Epoch 166/200\n",
      "learning rate scheduled to 0.0001885569337173365\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1326.0284 - accuracy: 0.6948 - val_loss: 1325.8148 - val_accuracy: 0.7098\n",
      "Epoch 167/200\n",
      "learning rate scheduled to 0.00018667136697331444\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1325.6211 - accuracy: 0.6995 - val_loss: 1325.4451 - val_accuracy: 0.6785\n",
      "Epoch 168/200\n",
      "learning rate scheduled to 0.00018480465820175596\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1325.2219 - accuracy: 0.6982 - val_loss: 1325.0383 - val_accuracy: 0.6846\n",
      "Epoch 169/200\n",
      "learning rate scheduled to 0.000182956605713116\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1324.8270 - accuracy: 0.6972 - val_loss: 1324.6503 - val_accuracy: 0.6712\n",
      "Epoch 170/200\n",
      "learning rate scheduled to 0.00018112703663064166\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1324.4379 - accuracy: 0.6962 - val_loss: 1324.2643 - val_accuracy: 0.6802\n",
      "Epoch 171/200\n",
      "learning rate scheduled to 0.00017931576367118395\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1324.0494 - accuracy: 0.6988 - val_loss: 1323.8518 - val_accuracy: 0.7006\n",
      "Epoch 172/200\n",
      "learning rate scheduled to 0.0001775225995515939\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1323.6650 - accuracy: 0.6973 - val_loss: 1323.4758 - val_accuracy: 0.6989\n",
      "Epoch 173/200\n",
      "learning rate scheduled to 0.00017574737139511854\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1323.2853 - accuracy: 0.6998 - val_loss: 1323.0920 - val_accuracy: 0.7055\n",
      "Epoch 174/200\n",
      "learning rate scheduled to 0.00017398989191860892\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1322.9124 - accuracy: 0.6986 - val_loss: 1322.7759 - val_accuracy: 0.6552\n",
      "Epoch 175/200\n",
      "learning rate scheduled to 0.00017224998824531213\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1322.5443 - accuracy: 0.6944 - val_loss: 1322.3671 - val_accuracy: 0.6880\n",
      "Epoch 176/200\n",
      "learning rate scheduled to 0.00017052748749847522\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1322.1733 - accuracy: 0.6969 - val_loss: 1321.9845 - val_accuracy: 0.6961\n",
      "Epoch 177/200\n",
      "learning rate scheduled to 0.00016882221680134535\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1321.8083 - accuracy: 0.6997 - val_loss: 1321.6392 - val_accuracy: 0.6883\n",
      "Epoch 178/200\n",
      "learning rate scheduled to 0.00016713398887077346\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1321.4509 - accuracy: 0.6963 - val_loss: 1321.2651 - val_accuracy: 0.6917\n",
      "Epoch 179/200\n",
      "learning rate scheduled to 0.00016546264523640276\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1321.0862 - accuracy: 0.7031 - val_loss: 1320.9282 - val_accuracy: 0.6906\n",
      "Epoch 180/200\n",
      "learning rate scheduled to 0.00016380801302148028\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1320.7360 - accuracy: 0.6974 - val_loss: 1320.5632 - val_accuracy: 0.7032\n",
      "Epoch 181/200\n",
      "learning rate scheduled to 0.00016216993375564926\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1320.3833 - accuracy: 0.7001 - val_loss: 1320.2109 - val_accuracy: 0.6948\n",
      "Epoch 182/200\n",
      "learning rate scheduled to 0.00016054823456215672\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1320.0374 - accuracy: 0.6996 - val_loss: 1319.8682 - val_accuracy: 0.7049\n",
      "Epoch 183/200\n",
      "learning rate scheduled to 0.00015894275697064586\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1319.6941 - accuracy: 0.6997 - val_loss: 1319.5293 - val_accuracy: 0.6906\n",
      "Epoch 184/200\n",
      "learning rate scheduled to 0.00015735332810436374\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1319.3564 - accuracy: 0.7008 - val_loss: 1319.1714 - val_accuracy: 0.7157\n",
      "Epoch 185/200\n",
      "learning rate scheduled to 0.00015577978949295356\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1319.0184 - accuracy: 0.7018 - val_loss: 1318.8563 - val_accuracy: 0.6914\n",
      "Epoch 186/200\n",
      "learning rate scheduled to 0.00015422199707245453\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1318.6907 - accuracy: 0.6987 - val_loss: 1318.5211 - val_accuracy: 0.6993\n",
      "Epoch 187/200\n",
      "learning rate scheduled to 0.00015267977796611375\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1318.3610 - accuracy: 0.6991 - val_loss: 1318.2012 - val_accuracy: 0.7029\n",
      "Epoch 188/200\n",
      "learning rate scheduled to 0.00015115297370357439\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1318.0366 - accuracy: 0.7005 - val_loss: 1317.8999 - val_accuracy: 0.6774\n",
      "Epoch 189/200\n",
      "learning rate scheduled to 0.00014964144022087568\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1317.7140 - accuracy: 0.7016 - val_loss: 1317.5558 - val_accuracy: 0.6974\n",
      "Epoch 190/200\n",
      "learning rate scheduled to 0.00014814501904766075\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1317.3983 - accuracy: 0.6965 - val_loss: 1317.2343 - val_accuracy: 0.7034\n",
      "Epoch 191/200\n",
      "learning rate scheduled to 0.00014666356611996888\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1317.0825 - accuracy: 0.7013 - val_loss: 1316.9268 - val_accuracy: 0.7059\n",
      "Epoch 192/200\n",
      "learning rate scheduled to 0.00014519693737383933\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1316.7694 - accuracy: 0.7028 - val_loss: 1316.6338 - val_accuracy: 0.6861\n",
      "Epoch 193/200\n",
      "learning rate scheduled to 0.0001437449743389152\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1316.4625 - accuracy: 0.6995 - val_loss: 1316.3761 - val_accuracy: 0.6640\n",
      "Epoch 194/200\n",
      "learning rate scheduled to 0.00014230751854483968\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1316.1606 - accuracy: 0.6972 - val_loss: 1316.0061 - val_accuracy: 0.6914\n",
      "Epoch 195/200\n",
      "learning rate scheduled to 0.00014088444033404812\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1315.8536 - accuracy: 0.7022 - val_loss: 1315.7009 - val_accuracy: 0.7053\n",
      "Epoch 196/200\n",
      "learning rate scheduled to 0.0001394755956425797\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1315.5541 - accuracy: 0.7031 - val_loss: 1315.4050 - val_accuracy: 0.7010\n",
      "Epoch 197/200\n",
      "learning rate scheduled to 0.00013808084040647372\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1315.2576 - accuracy: 0.7015 - val_loss: 1315.1155 - val_accuracy: 0.6970\n",
      "Epoch 198/200\n",
      "learning rate scheduled to 0.00013670003056176939\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1314.9650 - accuracy: 0.6997 - val_loss: 1314.8246 - val_accuracy: 0.6949\n",
      "Epoch 199/200\n",
      "learning rate scheduled to 0.000135333036450902\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1314.6730 - accuracy: 0.7024 - val_loss: 1314.5323 - val_accuracy: 0.6985\n",
      "Epoch 200/200\n",
      "learning rate scheduled to 0.00013397969960351474\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1314.3854 - accuracy: 0.7013 - val_loss: 1314.2648 - val_accuracy: 0.6778\n"
     ]
    }
   ],
   "source": [
    "history_original_siamese_model_4 = original_siamese_model_4.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                             model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 2s 18ms/step - loss: 1314.2748 - accuracy: 0.6682\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model_4.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fifth Run - 90K Pairs gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_90k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_90k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_5 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 90k - Grayscale\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_siamese_model_5.compile(loss=config.loss_function,\n",
    "                                 optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_90k_Gray\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_original_siamese_model_5 = original_siamese_model_5.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                             model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss, accuracy = original_siamese_model_5.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth Run - 150K Pairs Gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_150k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_150k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 105, 105, 1)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 96, 96, 64)        6464      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,947,648\n",
      "Trainable params: 38,947,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_6 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/gqcxwm0u\" target=\"_blank\">azure-bush-10</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 150k - Grayscale\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_siamese_model_6.compile(loss=config.loss_function,\n",
    "                                 optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_150k_Gray\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "learning rate scheduled to 0.0009900000470224768\n",
      "1014/1014 [==============================] - 63s 58ms/step - loss: 1504.8086 - accuracy: 0.5364 - val_loss: 1498.7686 - val_accuracy: 0.5598\n",
      "Epoch 2/200\n",
      "learning rate scheduled to 0.000980100086890161\n",
      "1014/1014 [==============================] - 62s 57ms/step - loss: 1492.8362 - accuracy: 0.5633 - val_loss: 1486.9006 - val_accuracy: 0.5545\n",
      "Epoch 3/200\n",
      "learning rate scheduled to 0.0009702991275116801\n",
      "1014/1014 [==============================] - 58s 54ms/step - loss: 1481.0712 - accuracy: 0.5507 - val_loss: 1475.2389 - val_accuracy: 0.5629\n",
      "Epoch 4/200\n",
      "learning rate scheduled to 0.0009605961316265165\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1469.5118 - accuracy: 0.5697 - val_loss: 1463.7858 - val_accuracy: 0.5808\n",
      "Epoch 5/200\n",
      "learning rate scheduled to 0.0009509901772253215\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1458.1604 - accuracy: 0.5860 - val_loss: 1452.5353 - val_accuracy: 0.5913\n",
      "Epoch 6/200\n",
      "learning rate scheduled to 0.0009414802846731617\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1447.0048 - accuracy: 0.6073 - val_loss: 1441.4744 - val_accuracy: 0.6235\n",
      "Epoch 7/200\n",
      "learning rate scheduled to 0.0009320654743351042\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1436.0367 - accuracy: 0.6344 - val_loss: 1430.6039 - val_accuracy: 0.6411\n",
      "Epoch 8/200\n",
      "learning rate scheduled to 0.0009227448242017999\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1425.2653 - accuracy: 0.6482 - val_loss: 1419.9216 - val_accuracy: 0.6589\n",
      "Epoch 9/200\n",
      "learning rate scheduled to 0.0009135173546383158\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1414.6776 - accuracy: 0.6609 - val_loss: 1409.4338 - val_accuracy: 0.6607\n",
      "Epoch 10/200\n",
      "learning rate scheduled to 0.0009043822012608871\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1404.2814 - accuracy: 0.6668 - val_loss: 1399.1301 - val_accuracy: 0.6701\n",
      "Epoch 11/200\n",
      "learning rate scheduled to 0.0008953383844345808\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1394.0664 - accuracy: 0.6695 - val_loss: 1388.9961 - val_accuracy: 0.6764\n",
      "Epoch 12/200\n",
      "learning rate scheduled to 0.000886384982150048\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1384.0281 - accuracy: 0.6726 - val_loss: 1379.0590 - val_accuracy: 0.6698\n",
      "Epoch 13/200\n",
      "learning rate scheduled to 0.0008775211300235242\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1374.1631 - accuracy: 0.6739 - val_loss: 1369.2682 - val_accuracy: 0.6789\n",
      "Epoch 14/200\n",
      "learning rate scheduled to 0.0008687459060456604\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1364.4642 - accuracy: 0.6785 - val_loss: 1359.6539 - val_accuracy: 0.6832\n",
      "Epoch 15/200\n",
      "learning rate scheduled to 0.0008600584458326921\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1354.9303 - accuracy: 0.6797 - val_loss: 1350.2062 - val_accuracy: 0.6813\n",
      "Epoch 16/200\n",
      "learning rate scheduled to 0.0008514578850008547\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1345.5599 - accuracy: 0.6813 - val_loss: 1340.9081 - val_accuracy: 0.6827\n",
      "Epoch 17/200\n",
      "learning rate scheduled to 0.0008429433015407995\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1336.3446 - accuracy: 0.6849 - val_loss: 1331.7800 - val_accuracy: 0.6869\n",
      "Epoch 18/200\n",
      "learning rate scheduled to 0.0008345138886943459\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1327.2864 - accuracy: 0.6860 - val_loss: 1322.7888 - val_accuracy: 0.6912\n",
      "Epoch 19/200\n",
      "learning rate scheduled to 0.0008261687244521454\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 1318.3784 - accuracy: 0.6872 - val_loss: 1313.9568 - val_accuracy: 0.6900\n",
      "Epoch 20/200\n",
      "learning rate scheduled to 0.0008179070596816018\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1309.6176 - accuracy: 0.6874 - val_loss: 1305.2686 - val_accuracy: 0.6939\n",
      "Epoch 21/200\n",
      "learning rate scheduled to 0.0008097279723733664\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1301.0043 - accuracy: 0.6882 - val_loss: 1296.7292 - val_accuracy: 0.6894\n",
      "Epoch 22/200\n",
      "learning rate scheduled to 0.000801630713394843\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1292.5312 - accuracy: 0.6894 - val_loss: 1288.3273 - val_accuracy: 0.6933\n",
      "Epoch 23/200\n",
      "learning rate scheduled to 0.0007936144183622674\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1284.1967 - accuracy: 0.6912 - val_loss: 1280.0577 - val_accuracy: 0.6961\n",
      "Epoch 24/200\n",
      "learning rate scheduled to 0.0007856782805174589\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1275.9982 - accuracy: 0.6916 - val_loss: 1271.9255 - val_accuracy: 0.6948\n",
      "Epoch 25/200\n",
      "learning rate scheduled to 0.0007778214931022376\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1267.9347 - accuracy: 0.6941 - val_loss: 1263.9385 - val_accuracy: 0.6894\n",
      "Epoch 26/200\n",
      "learning rate scheduled to 0.0007700433069840073\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1260.0028 - accuracy: 0.6953 - val_loss: 1256.0619 - val_accuracy: 0.7001\n",
      "Epoch 27/200\n",
      "learning rate scheduled to 0.0007623428577790037\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1252.1978 - accuracy: 0.6949 - val_loss: 1248.3395 - val_accuracy: 0.6885\n",
      "Epoch 28/200\n",
      "learning rate scheduled to 0.0007547194539802149\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1244.5189 - accuracy: 0.6952 - val_loss: 1240.7097 - val_accuracy: 0.6971\n",
      "Epoch 29/200\n",
      "learning rate scheduled to 0.0007471722312038764\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1236.9617 - accuracy: 0.6974 - val_loss: 1233.2245 - val_accuracy: 0.6890\n",
      "Epoch 30/200\n",
      "learning rate scheduled to 0.0007397004979429766\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1229.5275 - accuracy: 0.6971 - val_loss: 1225.8372 - val_accuracy: 0.7000\n",
      "Epoch 31/200\n",
      "learning rate scheduled to 0.0007323035050649196\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1222.2123 - accuracy: 0.6973 - val_loss: 1218.5826 - val_accuracy: 0.6958\n",
      "Epoch 32/200\n",
      "learning rate scheduled to 0.000724980445811525\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1215.0100 - accuracy: 0.6988 - val_loss: 1211.4360 - val_accuracy: 0.6998\n",
      "Epoch 33/200\n",
      "learning rate scheduled to 0.0007177306286757812\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1207.9235 - accuracy: 0.7000 - val_loss: 1204.4054 - val_accuracy: 0.7041\n",
      "Epoch 34/200\n",
      "learning rate scheduled to 0.0007105533045250923\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1200.9504 - accuracy: 0.6991 - val_loss: 1197.4828 - val_accuracy: 0.7069\n",
      "Epoch 35/200\n",
      "learning rate scheduled to 0.0007034477818524464\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1194.0836 - accuracy: 0.7009 - val_loss: 1190.6744 - val_accuracy: 0.7039\n",
      "Epoch 36/200\n",
      "learning rate scheduled to 0.000696413311525248\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1187.3252 - accuracy: 0.7008 - val_loss: 1183.9641 - val_accuracy: 0.7041\n",
      "Epoch 37/200\n",
      "learning rate scheduled to 0.0006894492020364851\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1180.6719 - accuracy: 0.7024 - val_loss: 1177.3696 - val_accuracy: 0.7044\n",
      "Epoch 38/200\n",
      "learning rate scheduled to 0.0006825547042535618\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1174.1234 - accuracy: 0.7025 - val_loss: 1170.8674 - val_accuracy: 0.7052\n",
      "Epoch 39/200\n",
      "learning rate scheduled to 0.0006757291842950508\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1167.6736 - accuracy: 0.7038 - val_loss: 1164.4690 - val_accuracy: 0.7077\n",
      "Epoch 40/200\n",
      "learning rate scheduled to 0.0006689718930283561\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1161.3248 - accuracy: 0.7053 - val_loss: 1158.1741 - val_accuracy: 0.7042\n",
      "Epoch 41/200\n",
      "learning rate scheduled to 0.0006622821965720505\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1155.0719 - accuracy: 0.7057 - val_loss: 1151.9700 - val_accuracy: 0.7053\n",
      "Epoch 42/200\n",
      "learning rate scheduled to 0.0006556594034191221\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1148.9167 - accuracy: 0.7056 - val_loss: 1145.8732 - val_accuracy: 0.6950\n",
      "Epoch 43/200\n",
      "learning rate scheduled to 0.0006491028220625594\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1142.8560 - accuracy: 0.7060 - val_loss: 1139.8405 - val_accuracy: 0.7136\n",
      "Epoch 44/200\n",
      "learning rate scheduled to 0.0006426118186209351\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1136.8849 - accuracy: 0.7080 - val_loss: 1133.9166 - val_accuracy: 0.7111\n",
      "Epoch 45/200\n",
      "learning rate scheduled to 0.0006361857015872375\n",
      "1014/1014 [==============================] - 63s 58ms/step - loss: 1131.0057 - accuracy: 0.7086 - val_loss: 1128.0826 - val_accuracy: 0.7131\n",
      "Epoch 46/200\n",
      "learning rate scheduled to 0.0006298238370800391\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1125.2144 - accuracy: 0.7090 - val_loss: 1122.3380 - val_accuracy: 0.7085\n",
      "Epoch 47/200\n",
      "learning rate scheduled to 0.0006235255912179127\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1119.5112 - accuracy: 0.7101 - val_loss: 1116.6765 - val_accuracy: 0.7136\n",
      "Epoch 48/200\n",
      "learning rate scheduled to 0.000617290330119431\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1113.8926 - accuracy: 0.7111 - val_loss: 1111.1046 - val_accuracy: 0.7116\n",
      "Epoch 49/200\n",
      "learning rate scheduled to 0.0006111174199031666\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1108.3593 - accuracy: 0.7109 - val_loss: 1105.6099 - val_accuracy: 0.7115\n",
      "Epoch 50/200\n",
      "learning rate scheduled to 0.0006050062266876921\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1102.9067 - accuracy: 0.7115 - val_loss: 1100.1970 - val_accuracy: 0.7177\n",
      "Epoch 51/200\n",
      "learning rate scheduled to 0.0005989561742171646\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1097.5358 - accuracy: 0.7151 - val_loss: 1094.8674 - val_accuracy: 0.7166\n",
      "Epoch 52/200\n",
      "learning rate scheduled to 0.0005929666286101564\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1092.2443 - accuracy: 0.7150 - val_loss: 1089.6217 - val_accuracy: 0.7113\n",
      "Epoch 53/200\n",
      "learning rate scheduled to 0.0005870369559852406\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1087.0309 - accuracy: 0.7155 - val_loss: 1084.4421 - val_accuracy: 0.7173\n",
      "Epoch 54/200\n",
      "learning rate scheduled to 0.000581166580086574\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1081.8925 - accuracy: 0.7174 - val_loss: 1079.3427 - val_accuracy: 0.7143\n",
      "Epoch 55/200\n",
      "learning rate scheduled to 0.0005753549246583134\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1076.8317 - accuracy: 0.7177 - val_loss: 1074.3181 - val_accuracy: 0.7186\n",
      "Epoch 56/200\n",
      "learning rate scheduled to 0.0005696013558190316\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1071.8442 - accuracy: 0.7195 - val_loss: 1069.3738 - val_accuracy: 0.7135\n",
      "Epoch 57/200\n",
      "learning rate scheduled to 0.0005639053549384699\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1066.9290 - accuracy: 0.7193 - val_loss: 1064.4924 - val_accuracy: 0.7180\n",
      "Epoch 58/200\n",
      "learning rate scheduled to 0.0005582662881352008\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1062.0856 - accuracy: 0.7198 - val_loss: 1059.6919 - val_accuracy: 0.7158\n",
      "Epoch 59/200\n",
      "learning rate scheduled to 0.0005526836367789656\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1057.3115 - accuracy: 0.7212 - val_loss: 1054.9360 - val_accuracy: 0.7245\n",
      "Epoch 60/200\n",
      "learning rate scheduled to 0.0005471568246139213\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1052.6060 - accuracy: 0.7220 - val_loss: 1050.2686 - val_accuracy: 0.7240\n",
      "Epoch 61/200\n",
      "learning rate scheduled to 0.000541685275384225\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1047.9683 - accuracy: 0.7244 - val_loss: 1045.6622 - val_accuracy: 0.7264\n",
      "Epoch 62/200\n",
      "learning rate scheduled to 0.0005362684128340334\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1043.3976 - accuracy: 0.7240 - val_loss: 1041.1202 - val_accuracy: 0.7287\n",
      "Epoch 63/200\n",
      "learning rate scheduled to 0.0005309057183330878\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1038.8915 - accuracy: 0.7253 - val_loss: 1036.6516 - val_accuracy: 0.7276\n",
      "Epoch 64/200\n",
      "learning rate scheduled to 0.0005255966732511297\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1034.4502 - accuracy: 0.7277 - val_loss: 1032.2417 - val_accuracy: 0.7283\n",
      "Epoch 65/200\n",
      "learning rate scheduled to 0.0005203407013323158\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1030.0734 - accuracy: 0.7270 - val_loss: 1027.8938 - val_accuracy: 0.7325\n",
      "Epoch 66/200\n",
      "learning rate scheduled to 0.0005151372839463875\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1025.7574 - accuracy: 0.7292 - val_loss: 1023.6140 - val_accuracy: 0.7304\n",
      "Epoch 67/200\n",
      "learning rate scheduled to 0.0005099859024630859\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1021.5018 - accuracy: 0.7291 - val_loss: 1019.3856 - val_accuracy: 0.7314\n",
      "Epoch 68/200\n",
      "learning rate scheduled to 0.0005048860382521525\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1017.3074 - accuracy: 0.7312 - val_loss: 1015.2281 - val_accuracy: 0.7281\n",
      "Epoch 69/200\n",
      "learning rate scheduled to 0.0004998371726833284\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1013.1705 - accuracy: 0.7313 - val_loss: 1011.1146 - val_accuracy: 0.7342\n",
      "Epoch 70/200\n",
      "learning rate scheduled to 0.0004948387871263549\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1009.0926 - accuracy: 0.7313 - val_loss: 1007.0660 - val_accuracy: 0.7350\n",
      "Epoch 71/200\n",
      "learning rate scheduled to 0.0004898904205765575\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1005.0701 - accuracy: 0.7335 - val_loss: 1003.0724 - val_accuracy: 0.7337\n",
      "Epoch 72/200\n",
      "learning rate scheduled to 0.00048499149677809326\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 1001.1065 - accuracy: 0.7320 - val_loss: 999.1322 - val_accuracy: 0.7375\n",
      "Epoch 73/200\n",
      "learning rate scheduled to 0.00048014158353907986\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 997.1956 - accuracy: 0.7341 - val_loss: 995.2486 - val_accuracy: 0.7369\n",
      "Epoch 74/200\n",
      "learning rate scheduled to 0.00047534016222925855\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 993.3391 - accuracy: 0.7357 - val_loss: 991.4223 - val_accuracy: 0.7401\n",
      "Epoch 75/200\n",
      "learning rate scheduled to 0.0004705867718439549\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 989.5377 - accuracy: 0.7360 - val_loss: 987.6451 - val_accuracy: 0.7412\n",
      "Epoch 76/200\n",
      "learning rate scheduled to 0.0004658808937529102\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 985.7866 - accuracy: 0.7382 - val_loss: 983.9227 - val_accuracy: 0.7393\n",
      "Epoch 77/200\n",
      "learning rate scheduled to 0.0004612220957642421\n",
      "1014/1014 [==============================] - 62s 57ms/step - loss: 982.0885 - accuracy: 0.7375 - val_loss: 980.2568 - val_accuracy: 0.7340\n",
      "Epoch 78/200\n",
      "learning rate scheduled to 0.00045660988806048406\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 978.4393 - accuracy: 0.7386 - val_loss: 976.6255 - val_accuracy: 0.7405\n",
      "Epoch 79/200\n",
      "learning rate scheduled to 0.0004520437808241695\n",
      "1014/1014 [==============================] - 63s 58ms/step - loss: 974.8393 - accuracy: 0.7408 - val_loss: 973.0493 - val_accuracy: 0.7432\n",
      "Epoch 80/200\n",
      "learning rate scheduled to 0.0004475233418634161\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 971.2911 - accuracy: 0.7405 - val_loss: 969.5261 - val_accuracy: 0.7400\n",
      "Epoch 81/200\n",
      "learning rate scheduled to 0.0004430481101735495\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 967.7906 - accuracy: 0.7413 - val_loss: 966.0494 - val_accuracy: 0.7447\n",
      "Epoch 82/200\n",
      "learning rate scheduled to 0.0004386176247498952\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 964.3373 - accuracy: 0.7418 - val_loss: 962.6176 - val_accuracy: 0.7437\n",
      "Epoch 83/200\n",
      "learning rate scheduled to 0.00043423145340057087\n",
      "1014/1014 [==============================] - 59s 54ms/step - loss: 960.9304 - accuracy: 0.7423 - val_loss: 959.2377 - val_accuracy: 0.7427\n",
      "Epoch 84/200\n",
      "learning rate scheduled to 0.0004298891351209022\n",
      "1014/1014 [==============================] - 61s 57ms/step - loss: 957.5671 - accuracy: 0.7433 - val_loss: 955.8925 - val_accuracy: 0.7486\n",
      "Epoch 85/200\n",
      "learning rate scheduled to 0.00042559023771900686\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 954.2522 - accuracy: 0.7444 - val_loss: 952.6114 - val_accuracy: 0.7373\n",
      "Epoch 86/200\n",
      "learning rate scheduled to 0.0004213343290030025\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 950.9813 - accuracy: 0.7439 - val_loss: 949.3579 - val_accuracy: 0.7421\n",
      "Epoch 87/200\n",
      "learning rate scheduled to 0.00041712097678100687\n",
      "1014/1014 [==============================] - 59s 55ms/step - loss: 947.7537 - accuracy: 0.7448 - val_loss: 946.1454 - val_accuracy: 0.7497\n",
      "Epoch 88/200\n",
      "learning rate scheduled to 0.00041294977767392994\n",
      "1014/1014 [==============================] - 58s 54ms/step - loss: 944.5704 - accuracy: 0.7451 - val_loss: 942.9940 - val_accuracy: 0.7396\n",
      "Epoch 89/200\n",
      "learning rate scheduled to 0.0004088202706770971\n",
      "1014/1014 [==============================] - 58s 54ms/step - loss: 941.4280 - accuracy: 0.7449 - val_loss: 939.8690 - val_accuracy: 0.7431\n",
      "Epoch 90/200\n",
      "learning rate scheduled to 0.00040473208122421056\n",
      "1014/1014 [==============================] - 58s 54ms/step - loss: 938.3274 - accuracy: 0.7455 - val_loss: 936.7852 - val_accuracy: 0.7465\n",
      "Epoch 91/200\n",
      "learning rate scheduled to 0.0004006847483105957\n",
      "1014/1014 [==============================] - 58s 54ms/step - loss: 935.2669 - accuracy: 0.7473 - val_loss: 933.7493 - val_accuracy: 0.7444\n",
      "Epoch 92/200\n",
      "learning rate scheduled to 0.0003966778973699547\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 932.2476 - accuracy: 0.7478 - val_loss: 930.7431 - val_accuracy: 0.7527\n",
      "Epoch 93/200\n",
      "learning rate scheduled to 0.0003927111250231974\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 929.2688 - accuracy: 0.7479 - val_loss: 927.7925 - val_accuracy: 0.7448\n",
      "Epoch 94/200\n",
      "learning rate scheduled to 0.0003887840278912336\n",
      "1014/1014 [==============================] - 63s 58ms/step - loss: 926.3297 - accuracy: 0.7488 - val_loss: 924.8740 - val_accuracy: 0.7456\n",
      "Epoch 95/200\n",
      "learning rate scheduled to 0.000384896173782181\n",
      "1014/1014 [==============================] - 60s 56ms/step - loss: 923.4279 - accuracy: 0.7491 - val_loss: 921.9869 - val_accuracy: 0.7497\n",
      "Epoch 96/200\n",
      "learning rate scheduled to 0.00038104721694253384\n",
      "1014/1014 [==============================] - 57s 52ms/step - loss: 920.5658 - accuracy: 0.7489 - val_loss: 919.1443 - val_accuracy: 0.7523\n",
      "Epoch 97/200\n",
      "learning rate scheduled to 0.000377236753993202\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 917.7388 - accuracy: 0.7501 - val_loss: 916.3282 - val_accuracy: 0.7572\n",
      "Epoch 98/200\n",
      "learning rate scheduled to 0.0003734643815550953\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 914.9503 - accuracy: 0.7493 - val_loss: 913.5627 - val_accuracy: 0.7527\n",
      "Epoch 99/200\n",
      "learning rate scheduled to 0.0003697297250619158\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 912.1964 - accuracy: 0.7502 - val_loss: 910.8262 - val_accuracy: 0.7539\n",
      "Epoch 100/200\n",
      "learning rate scheduled to 0.0003660324387601577\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 909.4803 - accuracy: 0.7509 - val_loss: 908.1285 - val_accuracy: 0.7493\n",
      "Epoch 101/200\n",
      "learning rate scheduled to 0.00036237211927073074\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 906.7980 - accuracy: 0.7505 - val_loss: 905.4625 - val_accuracy: 0.7529\n",
      "Epoch 102/200\n",
      "learning rate scheduled to 0.0003587483920273371\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 904.1494 - accuracy: 0.7510 - val_loss: 902.8309 - val_accuracy: 0.7526\n",
      "Epoch 103/200\n",
      "learning rate scheduled to 0.0003551609112764709\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 901.5364 - accuracy: 0.7525 - val_loss: 900.2344 - val_accuracy: 0.7560\n",
      "Epoch 104/200\n",
      "learning rate scheduled to 0.0003516093024518341\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 898.9583 - accuracy: 0.7522 - val_loss: 897.6779 - val_accuracy: 0.7494\n",
      "Epoch 105/200\n",
      "learning rate scheduled to 0.0003480932197999209\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 896.4111 - accuracy: 0.7530 - val_loss: 895.1441 - val_accuracy: 0.7535\n",
      "Epoch 106/200\n",
      "learning rate scheduled to 0.0003446122887544334\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 893.8955 - accuracy: 0.7548 - val_loss: 892.6454 - val_accuracy: 0.7554\n",
      "Epoch 107/200\n",
      "learning rate scheduled to 0.00034116616356186566\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 891.4137 - accuracy: 0.7541 - val_loss: 890.1759 - val_accuracy: 0.7574\n",
      "Epoch 108/200\n",
      "learning rate scheduled to 0.00033775449846871195\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 888.9640 - accuracy: 0.7524 - val_loss: 887.7477 - val_accuracy: 0.7520\n",
      "Epoch 109/200\n",
      "learning rate scheduled to 0.0003343769477214664\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 886.5447 - accuracy: 0.7541 - val_loss: 885.3395 - val_accuracy: 0.7551\n",
      "Epoch 110/200\n",
      "learning rate scheduled to 0.0003310331655666232\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 884.1554 - accuracy: 0.7556 - val_loss: 882.9656 - val_accuracy: 0.7589\n",
      "Epoch 111/200\n",
      "learning rate scheduled to 0.00032772283506346864\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 881.7963 - accuracy: 0.7567 - val_loss: 880.6180 - val_accuracy: 0.7605\n",
      "Epoch 112/200\n",
      "learning rate scheduled to 0.00032444561045849693\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 879.4678 - accuracy: 0.7563 - val_loss: 878.3072 - val_accuracy: 0.7625\n",
      "Epoch 113/200\n",
      "learning rate scheduled to 0.00032120114599820226\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 877.1689 - accuracy: 0.7583 - val_loss: 876.0202 - val_accuracy: 0.7634\n",
      "Epoch 114/200\n",
      "learning rate scheduled to 0.00031798912474187093\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 874.9014 - accuracy: 0.7567 - val_loss: 873.7632 - val_accuracy: 0.7611\n",
      "Epoch 115/200\n",
      "learning rate scheduled to 0.0003148092297487892\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 872.6574 - accuracy: 0.7574 - val_loss: 871.5383 - val_accuracy: 0.7625\n",
      "Epoch 116/200\n",
      "learning rate scheduled to 0.0003116611440782435\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 870.4429 - accuracy: 0.7564 - val_loss: 869.3391 - val_accuracy: 0.7567\n",
      "Epoch 117/200\n",
      "learning rate scheduled to 0.000308544521976728\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 868.2563 - accuracy: 0.7575 - val_loss: 867.1639 - val_accuracy: 0.7650\n",
      "Epoch 118/200\n",
      "learning rate scheduled to 0.0003054590753163211\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 866.0967 - accuracy: 0.7578 - val_loss: 865.0209 - val_accuracy: 0.7597\n",
      "Epoch 119/200\n",
      "learning rate scheduled to 0.0003024044871563092\n",
      "1014/1014 [==============================] - 59s 55ms/step - loss: 863.9652 - accuracy: 0.7590 - val_loss: 862.9053 - val_accuracy: 0.7609\n",
      "Epoch 120/200\n",
      "learning rate scheduled to 0.00029938044055597855\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 861.8604 - accuracy: 0.7591 - val_loss: 860.8132 - val_accuracy: 0.7574\n",
      "Epoch 121/200\n",
      "learning rate scheduled to 0.0002963866473874077\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 859.7822 - accuracy: 0.7588 - val_loss: 858.7468 - val_accuracy: 0.7604\n",
      "Epoch 122/200\n",
      "learning rate scheduled to 0.000293422790709883\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 857.7297 - accuracy: 0.7582 - val_loss: 856.7086 - val_accuracy: 0.7610\n",
      "Epoch 123/200\n",
      "learning rate scheduled to 0.00029048855358269063\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 855.6989 - accuracy: 0.7593 - val_loss: 854.6851 - val_accuracy: 0.7665\n",
      "Epoch 124/200\n",
      "learning rate scheduled to 0.0002875836766907014\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 853.6959 - accuracy: 0.7618 - val_loss: 852.6945 - val_accuracy: 0.7650\n",
      "Epoch 125/200\n",
      "learning rate scheduled to 0.0002847078430932015\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 851.7166 - accuracy: 0.7607 - val_loss: 850.7333 - val_accuracy: 0.7605\n",
      "Epoch 126/200\n",
      "learning rate scheduled to 0.0002818607646622695\n",
      "1014/1014 [==============================] - 56s 52ms/step - loss: 849.7615 - accuracy: 0.7612 - val_loss: 848.7880 - val_accuracy: 0.7647\n",
      "Epoch 127/200\n",
      "learning rate scheduled to 0.00027904215326998385\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 847.8306 - accuracy: 0.7616 - val_loss: 846.8654 - val_accuracy: 0.7657\n",
      "Epoch 128/200\n",
      "learning rate scheduled to 0.00027625172078842296\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 845.9248 - accuracy: 0.7613 - val_loss: 844.9741 - val_accuracy: 0.7655\n",
      "Epoch 129/200\n",
      "learning rate scheduled to 0.0002734892079024576\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 844.0405 - accuracy: 0.7627 - val_loss: 843.1078 - val_accuracy: 0.7590\n",
      "Epoch 130/200\n",
      "learning rate scheduled to 0.0002707543264841661\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 842.1805 - accuracy: 0.7617 - val_loss: 841.2534 - val_accuracy: 0.7657\n",
      "Epoch 131/200\n",
      "learning rate scheduled to 0.000268046788405627\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 840.3439 - accuracy: 0.7618 - val_loss: 839.4292 - val_accuracy: 0.7641\n",
      "Epoch 132/200\n",
      "learning rate scheduled to 0.000265366334351711\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 838.5287 - accuracy: 0.7632 - val_loss: 837.6258 - val_accuracy: 0.7639\n",
      "Epoch 133/200\n",
      "learning rate scheduled to 0.00026271267619449646\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 836.7354 - accuracy: 0.7647 - val_loss: 835.8406 - val_accuracy: 0.7663\n",
      "Epoch 134/200\n",
      "learning rate scheduled to 0.00026008555461885406\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 834.9651 - accuracy: 0.7639 - val_loss: 834.0764 - val_accuracy: 0.7663\n",
      "Epoch 135/200\n",
      "learning rate scheduled to 0.00025748471030965445\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 833.2139 - accuracy: 0.7640 - val_loss: 832.3427 - val_accuracy: 0.7653\n",
      "Epoch 136/200\n",
      "learning rate scheduled to 0.0002549098551389761\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 831.4832 - accuracy: 0.7650 - val_loss: 830.6226 - val_accuracy: 0.7644\n",
      "Epoch 137/200\n",
      "learning rate scheduled to 0.00025236075860448183\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 829.7750 - accuracy: 0.7642 - val_loss: 828.9268 - val_accuracy: 0.7620\n",
      "Epoch 138/200\n",
      "learning rate scheduled to 0.0002498371613910422\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 828.0850 - accuracy: 0.7660 - val_loss: 827.2414 - val_accuracy: 0.7702\n",
      "Epoch 139/200\n",
      "learning rate scheduled to 0.0002473388041835278\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 826.4167 - accuracy: 0.7653 - val_loss: 825.5856 - val_accuracy: 0.7681\n",
      "Epoch 140/200\n",
      "learning rate scheduled to 0.0002448654276668094\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 824.7704 - accuracy: 0.7652 - val_loss: 823.9517 - val_accuracy: 0.7652\n",
      "Epoch 141/200\n",
      "learning rate scheduled to 0.00024241677252575755\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 823.1432 - accuracy: 0.7656 - val_loss: 822.3292 - val_accuracy: 0.7676\n",
      "Epoch 142/200\n",
      "learning rate scheduled to 0.00023999260825803502\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 821.5354 - accuracy: 0.7648 - val_loss: 820.7377 - val_accuracy: 0.7651\n",
      "Epoch 143/200\n",
      "learning rate scheduled to 0.00023759267554851249\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 819.9462 - accuracy: 0.7660 - val_loss: 819.1524 - val_accuracy: 0.7696\n",
      "Epoch 144/200\n",
      "learning rate scheduled to 0.0002352167438948527\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 818.3766 - accuracy: 0.7660 - val_loss: 817.5911 - val_accuracy: 0.7718\n",
      "Epoch 145/200\n",
      "learning rate scheduled to 0.00023286458279471843\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 816.8246 - accuracy: 0.7681 - val_loss: 816.0518 - val_accuracy: 0.7671\n",
      "Epoch 146/200\n",
      "learning rate scheduled to 0.00023053593293298035\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 815.2927 - accuracy: 0.7672 - val_loss: 814.5287 - val_accuracy: 0.7678\n",
      "Epoch 147/200\n",
      "learning rate scheduled to 0.0002282305782136973\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 813.7769 - accuracy: 0.7672 - val_loss: 813.0220 - val_accuracy: 0.7680\n",
      "Epoch 148/200\n",
      "learning rate scheduled to 0.00022594827372813598\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 812.2785 - accuracy: 0.7689 - val_loss: 811.5323 - val_accuracy: 0.7698\n",
      "Epoch 149/200\n",
      "learning rate scheduled to 0.00022368878897395915\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 810.7992 - accuracy: 0.7677 - val_loss: 810.0601 - val_accuracy: 0.7667\n",
      "Epoch 150/200\n",
      "learning rate scheduled to 0.00022145190785522573\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 809.3354 - accuracy: 0.7679 - val_loss: 808.6072 - val_accuracy: 0.7671\n",
      "Epoch 151/200\n",
      "learning rate scheduled to 0.00021923738546320237\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 807.8872 - accuracy: 0.7699 - val_loss: 807.1721 - val_accuracy: 0.7683\n",
      "Epoch 152/200\n",
      "learning rate scheduled to 0.00021704500570194797\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 806.4606 - accuracy: 0.7688 - val_loss: 805.7498 - val_accuracy: 0.7676\n",
      "Epoch 153/200\n",
      "learning rate scheduled to 0.00021487455247552135\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 805.0499 - accuracy: 0.7700 - val_loss: 804.3458 - val_accuracy: 0.7719\n",
      "Epoch 154/200\n",
      "learning rate scheduled to 0.00021272580968798138\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 803.6555 - accuracy: 0.7703 - val_loss: 802.9594 - val_accuracy: 0.7721\n",
      "Epoch 155/200\n",
      "learning rate scheduled to 0.00021059854683699086\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 802.2798 - accuracy: 0.7700 - val_loss: 801.5906 - val_accuracy: 0.7722\n",
      "Epoch 156/200\n",
      "learning rate scheduled to 0.0002084925622330047\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 800.9169 - accuracy: 0.7707 - val_loss: 800.2348 - val_accuracy: 0.7748\n",
      "Epoch 157/200\n",
      "learning rate scheduled to 0.0002064076397800818\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 799.5713 - accuracy: 0.7717 - val_loss: 798.9026 - val_accuracy: 0.7671\n",
      "Epoch 158/200\n",
      "learning rate scheduled to 0.000204343563382281\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 798.2429 - accuracy: 0.7708 - val_loss: 797.5773 - val_accuracy: 0.7728\n",
      "Epoch 159/200\n",
      "learning rate scheduled to 0.0002023001313500572\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 796.9260 - accuracy: 0.7711 - val_loss: 796.2744 - val_accuracy: 0.7688\n",
      "Epoch 160/200\n",
      "learning rate scheduled to 0.0002002771275874693\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 795.6274 - accuracy: 0.7721 - val_loss: 794.9778 - val_accuracy: 0.7743\n",
      "Epoch 161/200\n",
      "learning rate scheduled to 0.0001982743504049722\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 794.3429 - accuracy: 0.7718 - val_loss: 793.6996 - val_accuracy: 0.7733\n",
      "Epoch 162/200\n",
      "learning rate scheduled to 0.00019629161251941696\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 793.0701 - accuracy: 0.7733 - val_loss: 792.4397 - val_accuracy: 0.7747\n",
      "Epoch 163/200\n",
      "learning rate scheduled to 0.0001943286978348624\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 791.8177 - accuracy: 0.7711 - val_loss: 791.1873 - val_accuracy: 0.7763\n",
      "Epoch 164/200\n",
      "learning rate scheduled to 0.00019238540466176346\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 790.5750 - accuracy: 0.7715 - val_loss: 789.9536 - val_accuracy: 0.7745\n",
      "Epoch 165/200\n",
      "learning rate scheduled to 0.00019046154571697116\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 789.3463 - accuracy: 0.7727 - val_loss: 788.7399 - val_accuracy: 0.7718\n",
      "Epoch 166/200\n",
      "learning rate scheduled to 0.0001885569337173365\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 788.1326 - accuracy: 0.7738 - val_loss: 787.5304 - val_accuracy: 0.7694\n",
      "Epoch 167/200\n",
      "learning rate scheduled to 0.00018667136697331444\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 786.9349 - accuracy: 0.7734 - val_loss: 786.3403 - val_accuracy: 0.7746\n",
      "Epoch 168/200\n",
      "learning rate scheduled to 0.00018480465820175596\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 785.7520 - accuracy: 0.7736 - val_loss: 785.1694 - val_accuracy: 0.7677\n",
      "Epoch 169/200\n",
      "learning rate scheduled to 0.000182956605713116\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 784.5809 - accuracy: 0.7738 - val_loss: 784.0046 - val_accuracy: 0.7707\n",
      "Epoch 170/200\n",
      "learning rate scheduled to 0.00018112703663064166\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 783.4250 - accuracy: 0.7742 - val_loss: 782.8470 - val_accuracy: 0.7791\n",
      "Epoch 171/200\n",
      "learning rate scheduled to 0.00017931576367118395\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 782.2811 - accuracy: 0.7761 - val_loss: 781.7112 - val_accuracy: 0.7766\n",
      "Epoch 172/200\n",
      "learning rate scheduled to 0.0001775225995515939\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 781.1506 - accuracy: 0.7757 - val_loss: 780.5977 - val_accuracy: 0.7701\n",
      "Epoch 173/200\n",
      "learning rate scheduled to 0.00017574737139511854\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 780.0337 - accuracy: 0.7757 - val_loss: 779.4746 - val_accuracy: 0.7773\n",
      "Epoch 174/200\n",
      "learning rate scheduled to 0.00017398989191860892\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 778.9284 - accuracy: 0.7760 - val_loss: 778.3737 - val_accuracy: 0.7796\n",
      "Epoch 175/200\n",
      "learning rate scheduled to 0.00017224998824531213\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 777.8363 - accuracy: 0.7759 - val_loss: 777.2882 - val_accuracy: 0.7801\n",
      "Epoch 176/200\n",
      "learning rate scheduled to 0.00017052748749847522\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 776.7567 - accuracy: 0.7757 - val_loss: 776.2210 - val_accuracy: 0.7709\n",
      "Epoch 177/200\n",
      "learning rate scheduled to 0.00016882221680134535\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 775.6874 - accuracy: 0.7774 - val_loss: 775.1517 - val_accuracy: 0.7828\n",
      "Epoch 178/200\n",
      "learning rate scheduled to 0.00016713398887077346\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 774.6310 - accuracy: 0.7774 - val_loss: 774.1047 - val_accuracy: 0.7754\n",
      "Epoch 179/200\n",
      "learning rate scheduled to 0.00016546264523640276\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 773.5859 - accuracy: 0.7777 - val_loss: 773.0629 - val_accuracy: 0.7815\n",
      "Epoch 180/200\n",
      "learning rate scheduled to 0.00016380801302148028\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 772.5522 - accuracy: 0.7785 - val_loss: 772.0347 - val_accuracy: 0.7782\n",
      "Epoch 181/200\n",
      "learning rate scheduled to 0.00016216993375564926\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 771.5305 - accuracy: 0.7776 - val_loss: 771.0217 - val_accuracy: 0.7777\n",
      "Epoch 182/200\n",
      "learning rate scheduled to 0.00016054823456215672\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 770.5180 - accuracy: 0.7789 - val_loss: 770.0158 - val_accuracy: 0.7814\n",
      "Epoch 183/200\n",
      "learning rate scheduled to 0.00015894275697064586\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 769.5229 - accuracy: 0.7770 - val_loss: 769.0262 - val_accuracy: 0.7764\n",
      "Epoch 184/200\n",
      "learning rate scheduled to 0.00015735332810436374\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 768.5342 - accuracy: 0.7791 - val_loss: 768.0446 - val_accuracy: 0.7789\n",
      "Epoch 185/200\n",
      "learning rate scheduled to 0.00015577978949295356\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 767.5610 - accuracy: 0.7782 - val_loss: 767.0746 - val_accuracy: 0.7790\n",
      "Epoch 186/200\n",
      "learning rate scheduled to 0.00015422199707245453\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 766.5975 - accuracy: 0.7790 - val_loss: 766.1132 - val_accuracy: 0.7827\n",
      "Epoch 187/200\n",
      "learning rate scheduled to 0.00015267977796611375\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 765.6457 - accuracy: 0.7789 - val_loss: 765.1730 - val_accuracy: 0.7769\n",
      "Epoch 188/200\n",
      "learning rate scheduled to 0.00015115297370357439\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 764.7035 - accuracy: 0.7786 - val_loss: 764.2337 - val_accuracy: 0.7818\n",
      "Epoch 189/200\n",
      "learning rate scheduled to 0.00014964144022087568\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 763.7731 - accuracy: 0.7787 - val_loss: 763.3092 - val_accuracy: 0.7786\n",
      "Epoch 190/200\n",
      "learning rate scheduled to 0.00014814501904766075\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 762.8525 - accuracy: 0.7795 - val_loss: 762.3926 - val_accuracy: 0.7800\n",
      "Epoch 191/200\n",
      "learning rate scheduled to 0.00014666356611996888\n",
      "1014/1014 [==============================] - 63s 59ms/step - loss: 761.9399 - accuracy: 0.7807 - val_loss: 761.4850 - val_accuracy: 0.7837\n",
      "Epoch 192/200\n",
      "learning rate scheduled to 0.00014519693737383933\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 761.0412 - accuracy: 0.7802 - val_loss: 760.5862 - val_accuracy: 0.7842\n",
      "Epoch 193/200\n",
      "learning rate scheduled to 0.0001437449743389152\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 760.1511 - accuracy: 0.7800 - val_loss: 759.7000 - val_accuracy: 0.7872\n",
      "Epoch 194/200\n",
      "learning rate scheduled to 0.00014230751854483968\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 759.2699 - accuracy: 0.7803 - val_loss: 758.8282 - val_accuracy: 0.7838\n",
      "Epoch 195/200\n",
      "learning rate scheduled to 0.00014088444033404812\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 758.3990 - accuracy: 0.7798 - val_loss: 757.9647 - val_accuracy: 0.7821\n",
      "Epoch 196/200\n",
      "learning rate scheduled to 0.0001394755956425797\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 757.5380 - accuracy: 0.7811 - val_loss: 757.1063 - val_accuracy: 0.7810\n",
      "Epoch 197/200\n",
      "learning rate scheduled to 0.00013808084040647372\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 756.6838 - accuracy: 0.7820 - val_loss: 756.2586 - val_accuracy: 0.7783\n",
      "Epoch 198/200\n",
      "learning rate scheduled to 0.00013670003056176939\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 755.8409 - accuracy: 0.7812 - val_loss: 755.4191 - val_accuracy: 0.7864\n",
      "Epoch 199/200\n",
      "learning rate scheduled to 0.000135333036450902\n",
      "1014/1014 [==============================] - 57s 53ms/step - loss: 755.0052 - accuracy: 0.7828 - val_loss: 754.5991 - val_accuracy: 0.7768\n",
      "Epoch 200/200\n",
      "learning rate scheduled to 0.00013397969960351474\n",
      "1014/1014 [==============================] - 59s 55ms/step - loss: 754.1797 - accuracy: 0.7818 - val_loss: 753.7651 - val_accuracy: 0.7837\n"
     ]
    }
   ],
   "source": [
    "history_original_siamese_model_6 = original_siamese_model_6.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                             model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 8s 19ms/step - loss: 753.7661 - accuracy: 0.7866\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model_6.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adapted_model(height, width, channels):\n",
    "\n",
    "    input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "    x = keras.layers.Conv2D(64, (3,3), activation=\"relu\",\n",
    "                            kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv1')(input)\n",
    "    x = keras.layers.Conv2D(64, (3,3), activation=\"relu\",\n",
    "                            kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv1')(x)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(128, (3,3), activation=\"relu\",\n",
    "                            kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv2')(x)\n",
    "    x = keras.layers.Conv2D(128, (3,3), activation=\"relu\",\n",
    "                            kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv2')(x)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(256, (3,3), activation=\"relu\",\n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv3')(x)\n",
    "    x = keras.layers.Conv2D(256, (3,3), activation=\"relu\",\n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv3')(x)\n",
    "    x = keras.layers.Conv2D(256, (3,3), activation=\"relu\",\n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv3')(x)\n",
    "    x = keras.layers.Conv2D(256, (3,3), activation=\"relu\",\n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv3')(x)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    output = keras.layers.Dense(256, activation=\"sigmoid\", kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.2),\n",
    "                           bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                           kernel_regularizer=keras.regularizers.l2(1e-3), name='Dense1')(x)\n",
    "\n",
    "    model = keras.models.Model(input, output)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Paper: Siamese Neural Networks for One-shot Image Recognition\n",
    "http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Images sizes: 40x40 / 75x75 / 105x105 / 120x120 / 150x150\n",
    "Images in RGB / Grayscale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import math\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from helper_functions import plot_training\n",
    "from helper_functions import create_tf_data_datasets_contrastive\n",
    "from helper_functions import create_tf_data_testset_contrastive\n",
    "from helper_functions import get_classification_report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Original Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First Run - 30k Pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_30k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_30k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_original_model(height, width, channels):\n",
    "\n",
    "    input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "    x = keras.layers.Conv2D(64, (10,10), activation=\"relu\",\n",
    "                            kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv1')(input)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(128, (7,7), activation=\"relu\",\n",
    "                            kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv2')(x)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(128, (4,4), activation=\"relu\",\n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv3')(x)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(256, (4,4), activation=\"relu\",\n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv4')(x)\n",
    "\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    output = keras.layers.Dense(4096, activation=\"sigmoid\", kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.2),\n",
    "                           bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                           kernel_regularizer=keras.regularizers.l2(1e-3), name='Dense1')(x)\n",
    "\n",
    "    model = keras.models.Model(input, output)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "original_model = get_original_model(height,width,channels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 105, 105, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 96, 96, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 30k\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "original_siamese_model.compile(loss=config.loss_function,\n",
    "                               optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_30k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "learning rate scheduled to 0.0009900000470224768\n",
      "  6/166 [>.............................] - ETA: 9s - loss: 1510.7972 - accuracy: 0.4883WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0265s vs `on_train_batch_end` time: 0.0313s). Check your callbacks.\n",
      "166/166 [==============================] - 14s 73ms/step - loss: 1509.8461 - accuracy: 0.5195 - val_loss: 1508.8473 - val_accuracy: 0.5477\n",
      "Epoch 2/200\n",
      "learning rate scheduled to 0.000980100086890161\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1507.8730 - accuracy: 0.5517 - val_loss: 1506.8850 - val_accuracy: 0.5713\n",
      "Epoch 3/200\n",
      "learning rate scheduled to 0.0009702991275116801\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1505.9220 - accuracy: 0.5641 - val_loss: 1504.9448 - val_accuracy: 0.5760\n",
      "Epoch 4/200\n",
      "learning rate scheduled to 0.0009605961316265165\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1503.9926 - accuracy: 0.5570 - val_loss: 1503.0267 - val_accuracy: 0.5584\n",
      "Epoch 5/200\n",
      "learning rate scheduled to 0.0009509901772253215\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1502.0851 - accuracy: 0.5658 - val_loss: 1501.1294 - val_accuracy: 0.5626\n",
      "Epoch 6/200\n",
      "learning rate scheduled to 0.0009414802846731617\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1500.1985 - accuracy: 0.5495 - val_loss: 1499.2531 - val_accuracy: 0.5622\n",
      "Epoch 7/200\n",
      "learning rate scheduled to 0.0009320654743351042\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1498.3329 - accuracy: 0.5498 - val_loss: 1497.3988 - val_accuracy: 0.5624\n",
      "Epoch 8/200\n",
      "learning rate scheduled to 0.0009227448242017999\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1496.4883 - accuracy: 0.5533 - val_loss: 1495.5640 - val_accuracy: 0.5464\n",
      "Epoch 9/200\n",
      "learning rate scheduled to 0.0009135173546383158\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1494.6643 - accuracy: 0.5528 - val_loss: 1493.7501 - val_accuracy: 0.5502\n",
      "Epoch 10/200\n",
      "learning rate scheduled to 0.0009043822012608871\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1492.8597 - accuracy: 0.5554 - val_loss: 1491.9556 - val_accuracy: 0.5545\n",
      "Epoch 11/200\n",
      "learning rate scheduled to 0.0008953383844345808\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1491.0754 - accuracy: 0.5547 - val_loss: 1490.1818 - val_accuracy: 0.5584\n",
      "Epoch 12/200\n",
      "learning rate scheduled to 0.000886384982150048\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1489.3113 - accuracy: 0.5561 - val_loss: 1488.4277 - val_accuracy: 0.5515\n",
      "Epoch 13/200\n",
      "learning rate scheduled to 0.0008775211300235242\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1487.5659 - accuracy: 0.5630 - val_loss: 1486.6920 - val_accuracy: 0.5588\n",
      "Epoch 14/200\n",
      "learning rate scheduled to 0.0008687459060456604\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1485.8402 - accuracy: 0.5646 - val_loss: 1484.9774 - val_accuracy: 0.5607\n",
      "Epoch 15/200\n",
      "learning rate scheduled to 0.0008600584458326921\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1484.1348 - accuracy: 0.5679 - val_loss: 1483.2804 - val_accuracy: 0.5683\n",
      "Epoch 16/200\n",
      "learning rate scheduled to 0.0008514578850008547\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1482.4471 - accuracy: 0.5776 - val_loss: 1481.6041 - val_accuracy: 0.5833\n",
      "Epoch 17/200\n",
      "learning rate scheduled to 0.0008429433015407995\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1480.7802 - accuracy: 0.5809 - val_loss: 1479.9431 - val_accuracy: 0.5924\n",
      "Epoch 18/200\n",
      "learning rate scheduled to 0.0008345138886943459\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1479.1293 - accuracy: 0.5868 - val_loss: 1478.3011 - val_accuracy: 0.5958\n",
      "Epoch 19/200\n",
      "learning rate scheduled to 0.0008261687244521454\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1477.4977 - accuracy: 0.5922 - val_loss: 1476.6799 - val_accuracy: 0.5952\n",
      "Epoch 20/200\n",
      "learning rate scheduled to 0.0008179070596816018\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1475.8861 - accuracy: 0.5899 - val_loss: 1475.0784 - val_accuracy: 0.5873\n",
      "Epoch 21/200\n",
      "learning rate scheduled to 0.0008097279723733664\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1474.2894 - accuracy: 0.5968 - val_loss: 1473.4921 - val_accuracy: 0.6029\n",
      "Epoch 22/200\n",
      "learning rate scheduled to 0.000801630713394843\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1472.7108 - accuracy: 0.6010 - val_loss: 1471.9152 - val_accuracy: 0.6082\n",
      "Epoch 23/200\n",
      "learning rate scheduled to 0.0007936144183622674\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1471.1504 - accuracy: 0.6053 - val_loss: 1470.3665 - val_accuracy: 0.6078\n",
      "Epoch 24/200\n",
      "learning rate scheduled to 0.0007856782805174589\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1469.6035 - accuracy: 0.6153 - val_loss: 1468.8282 - val_accuracy: 0.6186\n",
      "Epoch 25/200\n",
      "learning rate scheduled to 0.0007778214931022376\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1468.0759 - accuracy: 0.6203 - val_loss: 1467.3094 - val_accuracy: 0.6288\n",
      "Epoch 26/200\n",
      "learning rate scheduled to 0.0007700433069840073\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1466.5646 - accuracy: 0.6254 - val_loss: 1465.8110 - val_accuracy: 0.6192\n",
      "Epoch 27/200\n",
      "learning rate scheduled to 0.0007623428577790037\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1465.0709 - accuracy: 0.6268 - val_loss: 1464.3197 - val_accuracy: 0.6376\n",
      "Epoch 28/200\n",
      "learning rate scheduled to 0.0007547194539802149\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1463.5918 - accuracy: 0.6337 - val_loss: 1462.8486 - val_accuracy: 0.6456\n",
      "Epoch 29/200\n",
      "learning rate scheduled to 0.0007471722312038764\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1462.1288 - accuracy: 0.6404 - val_loss: 1461.3939 - val_accuracy: 0.6467\n",
      "Epoch 30/200\n",
      "learning rate scheduled to 0.0007397004979429766\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1460.6816 - accuracy: 0.6467 - val_loss: 1459.9513 - val_accuracy: 0.6544\n",
      "Epoch 31/200\n",
      "learning rate scheduled to 0.0007323035050649196\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1459.2504 - accuracy: 0.6517 - val_loss: 1458.5309 - val_accuracy: 0.6552\n",
      "Epoch 32/200\n",
      "learning rate scheduled to 0.000724980445811525\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1457.8333 - accuracy: 0.6589 - val_loss: 1457.1237 - val_accuracy: 0.6621\n",
      "Epoch 33/200\n",
      "learning rate scheduled to 0.0007177306286757812\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1456.4297 - accuracy: 0.6648 - val_loss: 1455.7256 - val_accuracy: 0.6699\n",
      "Epoch 34/200\n",
      "learning rate scheduled to 0.0007105533045250923\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1455.0441 - accuracy: 0.6740 - val_loss: 1454.3439 - val_accuracy: 0.6876\n",
      "Epoch 35/200\n",
      "learning rate scheduled to 0.0007034477818524464\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1453.6680 - accuracy: 0.6859 - val_loss: 1452.9767 - val_accuracy: 0.6844\n",
      "Epoch 36/200\n",
      "learning rate scheduled to 0.000696413311525248\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1452.3135 - accuracy: 0.6890 - val_loss: 1451.6378 - val_accuracy: 0.6863\n",
      "Epoch 37/200\n",
      "learning rate scheduled to 0.0006894492020364851\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1450.9730 - accuracy: 0.6921 - val_loss: 1450.2922 - val_accuracy: 0.7014\n",
      "Epoch 38/200\n",
      "learning rate scheduled to 0.0006825547042535618\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1449.6477 - accuracy: 0.7002 - val_loss: 1448.9771 - val_accuracy: 0.6989\n",
      "Epoch 39/200\n",
      "learning rate scheduled to 0.0006757291842950508\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1448.3364 - accuracy: 0.7056 - val_loss: 1447.6836 - val_accuracy: 0.7108\n",
      "Epoch 40/200\n",
      "learning rate scheduled to 0.0006689718930283561\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1447.0380 - accuracy: 0.7104 - val_loss: 1446.3938 - val_accuracy: 0.7053\n",
      "Epoch 41/200\n",
      "learning rate scheduled to 0.0006622821965720505\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1445.7533 - accuracy: 0.7161 - val_loss: 1445.1145 - val_accuracy: 0.7155\n",
      "Epoch 42/200\n",
      "learning rate scheduled to 0.0006556594034191221\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1444.4882 - accuracy: 0.7174 - val_loss: 1443.8448 - val_accuracy: 0.7257\n",
      "Epoch 43/200\n",
      "learning rate scheduled to 0.0006491028220625594\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1443.2343 - accuracy: 0.7187 - val_loss: 1442.6505 - val_accuracy: 0.6897\n",
      "Epoch 44/200\n",
      "learning rate scheduled to 0.0006426118186209351\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1441.9928 - accuracy: 0.7215 - val_loss: 1441.3646 - val_accuracy: 0.7362\n",
      "Epoch 45/200\n",
      "learning rate scheduled to 0.0006361857015872375\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1440.7684 - accuracy: 0.7241 - val_loss: 1440.1431 - val_accuracy: 0.7342\n",
      "Epoch 46/200\n",
      "learning rate scheduled to 0.0006298238370800391\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1439.5547 - accuracy: 0.7274 - val_loss: 1438.9360 - val_accuracy: 0.7400\n",
      "Epoch 47/200\n",
      "learning rate scheduled to 0.0006235255912179127\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1438.3555 - accuracy: 0.7256 - val_loss: 1437.7771 - val_accuracy: 0.7172\n",
      "Epoch 48/200\n",
      "learning rate scheduled to 0.000617290330119431\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1437.1694 - accuracy: 0.7277 - val_loss: 1436.5664 - val_accuracy: 0.7327\n",
      "Epoch 49/200\n",
      "learning rate scheduled to 0.0006111174199031666\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1435.9958 - accuracy: 0.7281 - val_loss: 1435.4087 - val_accuracy: 0.7308\n",
      "Epoch 50/200\n",
      "learning rate scheduled to 0.0006050062266876921\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1434.8298 - accuracy: 0.7327 - val_loss: 1434.2590 - val_accuracy: 0.7342\n",
      "Epoch 51/200\n",
      "learning rate scheduled to 0.0005989561742171646\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1433.6853 - accuracy: 0.7318 - val_loss: 1433.0975 - val_accuracy: 0.7396\n",
      "Epoch 52/200\n",
      "learning rate scheduled to 0.0005929666286101564\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1432.5488 - accuracy: 0.7328 - val_loss: 1431.9653 - val_accuracy: 0.7447\n",
      "Epoch 53/200\n",
      "learning rate scheduled to 0.0005870369559852406\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1431.4260 - accuracy: 0.7350 - val_loss: 1430.8665 - val_accuracy: 0.7344\n",
      "Epoch 54/200\n",
      "learning rate scheduled to 0.000581166580086574\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1430.3159 - accuracy: 0.7318 - val_loss: 1429.7407 - val_accuracy: 0.7538\n",
      "Epoch 55/200\n",
      "learning rate scheduled to 0.0005753549246583134\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1429.2163 - accuracy: 0.7354 - val_loss: 1428.6726 - val_accuracy: 0.7347\n",
      "Epoch 56/200\n",
      "learning rate scheduled to 0.0005696013558190316\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1428.1266 - accuracy: 0.7423 - val_loss: 1427.5834 - val_accuracy: 0.7464\n",
      "Epoch 57/200\n",
      "learning rate scheduled to 0.0005639053549384699\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1427.0515 - accuracy: 0.7421 - val_loss: 1426.5179 - val_accuracy: 0.7425\n",
      "Epoch 58/200\n",
      "learning rate scheduled to 0.0005582662881352008\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1425.9874 - accuracy: 0.7412 - val_loss: 1425.4668 - val_accuracy: 0.7234\n",
      "Epoch 59/200\n",
      "learning rate scheduled to 0.0005526836367789656\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1424.9360 - accuracy: 0.7407 - val_loss: 1424.4032 - val_accuracy: 0.7410\n",
      "Epoch 60/200\n",
      "learning rate scheduled to 0.0005471568246139213\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1423.8971 - accuracy: 0.7408 - val_loss: 1423.3633 - val_accuracy: 0.7459\n",
      "Epoch 61/200\n",
      "learning rate scheduled to 0.000541685275384225\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1422.8647 - accuracy: 0.7434 - val_loss: 1422.3595 - val_accuracy: 0.7291\n",
      "Epoch 62/200\n",
      "learning rate scheduled to 0.0005362684128340334\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1421.8483 - accuracy: 0.7408 - val_loss: 1421.3300 - val_accuracy: 0.7508\n",
      "Epoch 63/200\n",
      "learning rate scheduled to 0.0005309057183330878\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1420.8390 - accuracy: 0.7424 - val_loss: 1420.3521 - val_accuracy: 0.7377\n",
      "Epoch 64/200\n",
      "learning rate scheduled to 0.0005255966732511297\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1419.8370 - accuracy: 0.7483 - val_loss: 1419.3491 - val_accuracy: 0.7438\n",
      "Epoch 65/200\n",
      "learning rate scheduled to 0.0005203407013323158\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1418.8553 - accuracy: 0.7457 - val_loss: 1418.3788 - val_accuracy: 0.7191\n",
      "Epoch 66/200\n",
      "learning rate scheduled to 0.0005151372839463875\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1417.8779 - accuracy: 0.7455 - val_loss: 1417.3843 - val_accuracy: 0.7532\n",
      "Epoch 67/200\n",
      "learning rate scheduled to 0.0005099859024630859\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1416.9117 - accuracy: 0.7475 - val_loss: 1416.4457 - val_accuracy: 0.7438\n",
      "Epoch 68/200\n",
      "learning rate scheduled to 0.0005048860382521525\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1415.9556 - accuracy: 0.7497 - val_loss: 1415.4779 - val_accuracy: 0.7517\n",
      "Epoch 69/200\n",
      "learning rate scheduled to 0.0004998371726833284\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1415.0114 - accuracy: 0.7495 - val_loss: 1414.5400 - val_accuracy: 0.7400\n",
      "Epoch 70/200\n",
      "learning rate scheduled to 0.0004948387871263549\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1414.0732 - accuracy: 0.7536 - val_loss: 1413.6356 - val_accuracy: 0.7251\n",
      "Epoch 71/200\n",
      "learning rate scheduled to 0.0004898904205765575\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1413.1494 - accuracy: 0.7513 - val_loss: 1412.6749 - val_accuracy: 0.7611\n",
      "Epoch 72/200\n",
      "learning rate scheduled to 0.00048499149677809326\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1412.2343 - accuracy: 0.7544 - val_loss: 1411.7728 - val_accuracy: 0.7607\n",
      "Epoch 73/200\n",
      "learning rate scheduled to 0.00048014158353907986\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1411.3280 - accuracy: 0.7558 - val_loss: 1410.8782 - val_accuracy: 0.7492\n",
      "Epoch 74/200\n",
      "learning rate scheduled to 0.00047534016222925855\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1410.4314 - accuracy: 0.7547 - val_loss: 1409.9823 - val_accuracy: 0.7577\n",
      "Epoch 75/200\n",
      "learning rate scheduled to 0.0004705867718439549\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1409.5490 - accuracy: 0.7557 - val_loss: 1409.1305 - val_accuracy: 0.7315\n",
      "Epoch 76/200\n",
      "learning rate scheduled to 0.0004658808937529102\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1408.6715 - accuracy: 0.7545 - val_loss: 1408.2053 - val_accuracy: 0.7790\n",
      "Epoch 77/200\n",
      "learning rate scheduled to 0.0004612220957642421\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1407.8057 - accuracy: 0.7561 - val_loss: 1407.3727 - val_accuracy: 0.7583\n",
      "Epoch 78/200\n",
      "learning rate scheduled to 0.00045660988806048406\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1406.9438 - accuracy: 0.7565 - val_loss: 1406.4998 - val_accuracy: 0.7683\n",
      "Epoch 79/200\n",
      "learning rate scheduled to 0.0004520437808241695\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1406.0945 - accuracy: 0.7591 - val_loss: 1405.6794 - val_accuracy: 0.7623\n",
      "Epoch 80/200\n",
      "learning rate scheduled to 0.0004475233418634161\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1405.2565 - accuracy: 0.7582 - val_loss: 1404.8359 - val_accuracy: 0.7643\n",
      "Epoch 81/200\n",
      "learning rate scheduled to 0.0004430481101735495\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1404.4224 - accuracy: 0.7592 - val_loss: 1403.9932 - val_accuracy: 0.7730\n",
      "Epoch 82/200\n",
      "learning rate scheduled to 0.0004386176247498952\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1403.6021 - accuracy: 0.7604 - val_loss: 1403.1704 - val_accuracy: 0.7753\n",
      "Epoch 83/200\n",
      "learning rate scheduled to 0.00043423145340057087\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1402.7836 - accuracy: 0.7619 - val_loss: 1402.3737 - val_accuracy: 0.7700\n",
      "Epoch 84/200\n",
      "learning rate scheduled to 0.0004298891351209022\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1401.9827 - accuracy: 0.7611 - val_loss: 1401.5635 - val_accuracy: 0.7696\n",
      "Epoch 85/200\n",
      "learning rate scheduled to 0.00042559023771900686\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1401.1790 - accuracy: 0.7664 - val_loss: 1400.7910 - val_accuracy: 0.7609\n",
      "Epoch 86/200\n",
      "learning rate scheduled to 0.0004213343290030025\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1400.3965 - accuracy: 0.7624 - val_loss: 1399.9956 - val_accuracy: 0.7675\n",
      "Epoch 87/200\n",
      "learning rate scheduled to 0.00041712097678100687\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1399.6107 - accuracy: 0.7648 - val_loss: 1399.2184 - val_accuracy: 0.7747\n",
      "Epoch 88/200\n",
      "learning rate scheduled to 0.00041294977767392994\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1398.8433 - accuracy: 0.7670 - val_loss: 1398.4529 - val_accuracy: 0.7698\n",
      "Epoch 89/200\n",
      "learning rate scheduled to 0.0004088202706770971\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1398.0789 - accuracy: 0.7658 - val_loss: 1397.7241 - val_accuracy: 0.7494\n",
      "Epoch 90/200\n",
      "learning rate scheduled to 0.00040473208122421056\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1397.3246 - accuracy: 0.7650 - val_loss: 1396.9454 - val_accuracy: 0.7734\n",
      "Epoch 91/200\n",
      "learning rate scheduled to 0.0004006847483105957\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1396.5729 - accuracy: 0.7716 - val_loss: 1396.1936 - val_accuracy: 0.7666\n",
      "Epoch 92/200\n",
      "learning rate scheduled to 0.0003966778973699547\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1395.8354 - accuracy: 0.7674 - val_loss: 1395.4524 - val_accuracy: 0.7773\n",
      "Epoch 93/200\n",
      "learning rate scheduled to 0.0003927111250231974\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1395.1027 - accuracy: 0.7700 - val_loss: 1394.7374 - val_accuracy: 0.7790\n",
      "Epoch 94/200\n",
      "learning rate scheduled to 0.0003887840278912336\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1394.3784 - accuracy: 0.7726 - val_loss: 1394.0200 - val_accuracy: 0.7598\n",
      "Epoch 95/200\n",
      "learning rate scheduled to 0.000384896173782181\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1393.6625 - accuracy: 0.7720 - val_loss: 1393.2960 - val_accuracy: 0.7711\n",
      "Epoch 96/200\n",
      "learning rate scheduled to 0.00038104721694253384\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1392.9540 - accuracy: 0.7690 - val_loss: 1392.5890 - val_accuracy: 0.7787\n",
      "Epoch 97/200\n",
      "learning rate scheduled to 0.000377236753993202\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1392.2524 - accuracy: 0.7715 - val_loss: 1391.8923 - val_accuracy: 0.7755\n",
      "Epoch 98/200\n",
      "learning rate scheduled to 0.0003734643815550953\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1391.5518 - accuracy: 0.7760 - val_loss: 1391.2009 - val_accuracy: 0.7766\n",
      "Epoch 99/200\n",
      "learning rate scheduled to 0.0003697297250619158\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1390.8663 - accuracy: 0.7759 - val_loss: 1390.5237 - val_accuracy: 0.7787\n",
      "Epoch 100/200\n",
      "learning rate scheduled to 0.0003660324387601577\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1390.1864 - accuracy: 0.7729 - val_loss: 1389.8613 - val_accuracy: 0.7704\n",
      "Epoch 101/200\n",
      "learning rate scheduled to 0.00036237211927073074\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1389.5144 - accuracy: 0.7772 - val_loss: 1389.1753 - val_accuracy: 0.7813\n",
      "Epoch 102/200\n",
      "learning rate scheduled to 0.0003587483920273371\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1388.8490 - accuracy: 0.7781 - val_loss: 1388.5099 - val_accuracy: 0.7868\n",
      "Epoch 103/200\n",
      "learning rate scheduled to 0.0003551609112764709\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1388.1910 - accuracy: 0.7759 - val_loss: 1387.8534 - val_accuracy: 0.7824\n",
      "Epoch 104/200\n",
      "learning rate scheduled to 0.0003516093024518341\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1387.5383 - accuracy: 0.7773 - val_loss: 1387.2227 - val_accuracy: 0.7724\n",
      "Epoch 105/200\n",
      "learning rate scheduled to 0.0003480932197999209\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1386.8956 - accuracy: 0.7777 - val_loss: 1386.5726 - val_accuracy: 0.7700\n",
      "Epoch 106/200\n",
      "learning rate scheduled to 0.0003446122887544334\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1386.2585 - accuracy: 0.7760 - val_loss: 1385.9261 - val_accuracy: 0.7930\n",
      "Epoch 107/200\n",
      "learning rate scheduled to 0.00034116616356186566\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1385.6237 - accuracy: 0.7788 - val_loss: 1385.3113 - val_accuracy: 0.7807\n",
      "Epoch 108/200\n",
      "learning rate scheduled to 0.00033775449846871195\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1384.9978 - accuracy: 0.7786 - val_loss: 1384.6826 - val_accuracy: 0.7885\n",
      "Epoch 109/200\n",
      "learning rate scheduled to 0.0003343769477214664\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1384.3831 - accuracy: 0.7789 - val_loss: 1384.0945 - val_accuracy: 0.7551\n",
      "Epoch 110/200\n",
      "learning rate scheduled to 0.0003310331655666232\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1383.7725 - accuracy: 0.7778 - val_loss: 1383.4542 - val_accuracy: 0.7949\n",
      "Epoch 111/200\n",
      "learning rate scheduled to 0.00032772283506346864\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1383.1628 - accuracy: 0.7804 - val_loss: 1382.8774 - val_accuracy: 0.7717\n",
      "Epoch 112/200\n",
      "learning rate scheduled to 0.00032444561045849693\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1382.5648 - accuracy: 0.7824 - val_loss: 1382.2670 - val_accuracy: 0.7798\n",
      "Epoch 113/200\n",
      "learning rate scheduled to 0.00032120114599820226\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1381.9724 - accuracy: 0.7813 - val_loss: 1381.6614 - val_accuracy: 0.7924\n",
      "Epoch 114/200\n",
      "learning rate scheduled to 0.00031798912474187093\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1381.3816 - accuracy: 0.7845 - val_loss: 1381.0846 - val_accuracy: 0.7881\n",
      "Epoch 115/200\n",
      "learning rate scheduled to 0.0003148092297487892\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1380.8009 - accuracy: 0.7824 - val_loss: 1380.5101 - val_accuracy: 0.7773\n",
      "Epoch 116/200\n",
      "learning rate scheduled to 0.0003116611440782435\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1380.2246 - accuracy: 0.7855 - val_loss: 1379.9594 - val_accuracy: 0.7643\n",
      "Epoch 117/200\n",
      "learning rate scheduled to 0.000308544521976728\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1379.6591 - accuracy: 0.7822 - val_loss: 1379.3721 - val_accuracy: 0.7787\n",
      "Epoch 118/200\n",
      "learning rate scheduled to 0.0003054590753163211\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1379.0948 - accuracy: 0.7832 - val_loss: 1378.8143 - val_accuracy: 0.7749\n",
      "Epoch 119/200\n",
      "learning rate scheduled to 0.0003024044871563092\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1378.5391 - accuracy: 0.7840 - val_loss: 1378.2576 - val_accuracy: 0.7870\n",
      "Epoch 120/200\n",
      "learning rate scheduled to 0.00029938044055597855\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1377.9873 - accuracy: 0.7844 - val_loss: 1377.7101 - val_accuracy: 0.7847\n",
      "Epoch 121/200\n",
      "learning rate scheduled to 0.0002963866473874077\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1377.4423 - accuracy: 0.7863 - val_loss: 1377.1692 - val_accuracy: 0.7941\n",
      "Epoch 122/200\n",
      "learning rate scheduled to 0.000293422790709883\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1376.9028 - accuracy: 0.7850 - val_loss: 1376.6221 - val_accuracy: 0.7981\n",
      "Epoch 123/200\n",
      "learning rate scheduled to 0.00029048855358269063\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1376.3711 - accuracy: 0.7831 - val_loss: 1376.0962 - val_accuracy: 0.7881\n",
      "Epoch 124/200\n",
      "learning rate scheduled to 0.0002875836766907014\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1375.8386 - accuracy: 0.7868 - val_loss: 1375.5647 - val_accuracy: 0.7909\n",
      "Epoch 125/200\n",
      "learning rate scheduled to 0.0002847078430932015\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1375.3165 - accuracy: 0.7852 - val_loss: 1375.0527 - val_accuracy: 0.7881\n",
      "Epoch 126/200\n",
      "learning rate scheduled to 0.0002818607646622695\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1374.7979 - accuracy: 0.7867 - val_loss: 1374.5496 - val_accuracy: 0.7706\n",
      "Epoch 127/200\n",
      "learning rate scheduled to 0.00027904215326998385\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1374.2859 - accuracy: 0.7880 - val_loss: 1374.0280 - val_accuracy: 0.7854\n",
      "Epoch 128/200\n",
      "learning rate scheduled to 0.00027625172078842296\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1373.7782 - accuracy: 0.7884 - val_loss: 1373.5383 - val_accuracy: 0.7868\n",
      "Epoch 129/200\n",
      "learning rate scheduled to 0.0002734892079024576\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1373.2811 - accuracy: 0.7864 - val_loss: 1373.0177 - val_accuracy: 0.7920\n",
      "Epoch 130/200\n",
      "learning rate scheduled to 0.0002707543264841661\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1372.7800 - accuracy: 0.7866 - val_loss: 1372.5345 - val_accuracy: 0.7892\n",
      "Epoch 131/200\n",
      "learning rate scheduled to 0.000268046788405627\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1372.2899 - accuracy: 0.7878 - val_loss: 1372.0372 - val_accuracy: 0.7956\n",
      "Epoch 132/200\n",
      "learning rate scheduled to 0.000265366334351711\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1371.8047 - accuracy: 0.7885 - val_loss: 1371.5647 - val_accuracy: 0.7864\n",
      "Epoch 133/200\n",
      "learning rate scheduled to 0.00026271267619449646\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1371.3218 - accuracy: 0.7905 - val_loss: 1371.0784 - val_accuracy: 0.7968\n",
      "Epoch 134/200\n",
      "learning rate scheduled to 0.00026008555461885406\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1370.8469 - accuracy: 0.7900 - val_loss: 1370.6042 - val_accuracy: 0.7958\n",
      "Epoch 135/200\n",
      "learning rate scheduled to 0.00025748471030965445\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1370.3776 - accuracy: 0.7878 - val_loss: 1370.1443 - val_accuracy: 0.7903\n",
      "Epoch 136/200\n",
      "learning rate scheduled to 0.0002549098551389761\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1369.9099 - accuracy: 0.7882 - val_loss: 1369.6733 - val_accuracy: 0.7830\n",
      "Epoch 137/200\n",
      "learning rate scheduled to 0.00025236075860448183\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1369.4493 - accuracy: 0.7901 - val_loss: 1369.2080 - val_accuracy: 0.8002\n",
      "Epoch 138/200\n",
      "learning rate scheduled to 0.0002498371613910422\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1368.9890 - accuracy: 0.7908 - val_loss: 1368.7582 - val_accuracy: 0.7924\n",
      "Epoch 139/200\n",
      "learning rate scheduled to 0.0002473388041835278\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1368.5350 - accuracy: 0.7908 - val_loss: 1368.3187 - val_accuracy: 0.7875\n",
      "Epoch 140/200\n",
      "learning rate scheduled to 0.0002448654276668094\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1368.0896 - accuracy: 0.7902 - val_loss: 1367.8645 - val_accuracy: 0.7917\n",
      "Epoch 141/200\n",
      "learning rate scheduled to 0.00024241677252575755\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1367.6486 - accuracy: 0.7897 - val_loss: 1367.4257 - val_accuracy: 0.7866\n",
      "Epoch 142/200\n",
      "learning rate scheduled to 0.00023999260825803502\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1367.2098 - accuracy: 0.7911 - val_loss: 1366.9937 - val_accuracy: 0.7917\n",
      "Epoch 143/200\n",
      "learning rate scheduled to 0.00023759267554851249\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1366.7748 - accuracy: 0.7892 - val_loss: 1366.5511 - val_accuracy: 0.7975\n",
      "Epoch 144/200\n",
      "learning rate scheduled to 0.0002352167438948527\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1366.3483 - accuracy: 0.7881 - val_loss: 1366.1333 - val_accuracy: 0.7883\n",
      "Epoch 145/200\n",
      "learning rate scheduled to 0.00023286458279471843\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1365.9229 - accuracy: 0.7907 - val_loss: 1365.7125 - val_accuracy: 0.7864\n",
      "Epoch 146/200\n",
      "learning rate scheduled to 0.00023053593293298035\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1365.5027 - accuracy: 0.7911 - val_loss: 1365.3065 - val_accuracy: 0.7796\n",
      "Epoch 147/200\n",
      "learning rate scheduled to 0.0002282305782136973\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1365.0847 - accuracy: 0.7921 - val_loss: 1364.8807 - val_accuracy: 0.7883\n",
      "Epoch 148/200\n",
      "learning rate scheduled to 0.00022594827372813598\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1364.6733 - accuracy: 0.7952 - val_loss: 1364.4805 - val_accuracy: 0.7902\n",
      "Epoch 149/200\n",
      "learning rate scheduled to 0.00022368878897395915\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1364.2687 - accuracy: 0.7893 - val_loss: 1364.0598 - val_accuracy: 0.7930\n",
      "Epoch 150/200\n",
      "learning rate scheduled to 0.00022145190785522573\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1363.8630 - accuracy: 0.7902 - val_loss: 1363.6476 - val_accuracy: 0.8015\n",
      "Epoch 151/200\n",
      "learning rate scheduled to 0.00021923738546320237\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1363.4647 - accuracy: 0.7909 - val_loss: 1363.2593 - val_accuracy: 0.7915\n",
      "Epoch 152/200\n",
      "learning rate scheduled to 0.00021704500570194797\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1363.0684 - accuracy: 0.7928 - val_loss: 1362.8740 - val_accuracy: 0.7968\n",
      "Epoch 153/200\n",
      "learning rate scheduled to 0.00021487455247552135\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1362.6759 - accuracy: 0.7921 - val_loss: 1362.4761 - val_accuracy: 0.7962\n",
      "Epoch 154/200\n",
      "learning rate scheduled to 0.00021272580968798138\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1362.2875 - accuracy: 0.7936 - val_loss: 1362.0979 - val_accuracy: 0.7892\n",
      "Epoch 155/200\n",
      "learning rate scheduled to 0.00021059854683699086\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1361.9049 - accuracy: 0.7942 - val_loss: 1361.7045 - val_accuracy: 0.7996\n",
      "Epoch 156/200\n",
      "learning rate scheduled to 0.0002084925622330047\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1361.5251 - accuracy: 0.7929 - val_loss: 1361.3285 - val_accuracy: 0.7985\n",
      "Epoch 157/200\n",
      "learning rate scheduled to 0.0002064076397800818\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1361.1531 - accuracy: 0.7937 - val_loss: 1360.9878 - val_accuracy: 0.7696\n",
      "Epoch 158/200\n",
      "learning rate scheduled to 0.000204343563382281\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1360.7814 - accuracy: 0.7926 - val_loss: 1360.5905 - val_accuracy: 0.7960\n",
      "Epoch 159/200\n",
      "learning rate scheduled to 0.0002023001313500572\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1360.4125 - accuracy: 0.7934 - val_loss: 1360.2452 - val_accuracy: 0.7847\n",
      "Epoch 160/200\n",
      "learning rate scheduled to 0.0002002771275874693\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1360.0487 - accuracy: 0.7962 - val_loss: 1359.9019 - val_accuracy: 0.7687\n",
      "Epoch 161/200\n",
      "learning rate scheduled to 0.0001982743504049722\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1359.6895 - accuracy: 0.7955 - val_loss: 1359.4985 - val_accuracy: 0.8056\n",
      "Epoch 162/200\n",
      "learning rate scheduled to 0.00019629161251941696\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1359.3357 - accuracy: 0.7936 - val_loss: 1359.1512 - val_accuracy: 0.8002\n",
      "Epoch 163/200\n",
      "learning rate scheduled to 0.0001943286978348624\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1358.9774 - accuracy: 0.7960 - val_loss: 1358.7944 - val_accuracy: 0.7945\n",
      "Epoch 164/200\n",
      "learning rate scheduled to 0.00019238540466176346\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1358.6318 - accuracy: 0.7918 - val_loss: 1358.4515 - val_accuracy: 0.7986\n",
      "Epoch 165/200\n",
      "learning rate scheduled to 0.00019046154571697116\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1358.2833 - accuracy: 0.7938 - val_loss: 1358.1013 - val_accuracy: 0.8028\n",
      "Epoch 166/200\n",
      "learning rate scheduled to 0.0001885569337173365\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1357.9418 - accuracy: 0.7959 - val_loss: 1357.7667 - val_accuracy: 0.7898\n",
      "Epoch 167/200\n",
      "learning rate scheduled to 0.00018667136697331444\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1357.6040 - accuracy: 0.7933 - val_loss: 1357.4373 - val_accuracy: 0.7934\n",
      "Epoch 168/200\n",
      "learning rate scheduled to 0.00018480465820175596\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1357.2712 - accuracy: 0.7936 - val_loss: 1357.0946 - val_accuracy: 0.7992\n",
      "Epoch 169/200\n",
      "learning rate scheduled to 0.000182956605713116\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1356.9393 - accuracy: 0.7936 - val_loss: 1356.7743 - val_accuracy: 0.7919\n",
      "Epoch 170/200\n",
      "learning rate scheduled to 0.00018112703663064166\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1356.6100 - accuracy: 0.7963 - val_loss: 1356.4377 - val_accuracy: 0.8028\n",
      "Epoch 171/200\n",
      "learning rate scheduled to 0.00017931576367118395\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1356.2865 - accuracy: 0.7922 - val_loss: 1356.1178 - val_accuracy: 0.7975\n",
      "Epoch 172/200\n",
      "learning rate scheduled to 0.0001775225995515939\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1355.9623 - accuracy: 0.7959 - val_loss: 1355.8020 - val_accuracy: 0.7960\n",
      "Epoch 173/200\n",
      "learning rate scheduled to 0.00017574737139511854\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1355.6454 - accuracy: 0.7962 - val_loss: 1355.4958 - val_accuracy: 0.7896\n",
      "Epoch 174/200\n",
      "learning rate scheduled to 0.00017398989191860892\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1355.3329 - accuracy: 0.7943 - val_loss: 1355.1744 - val_accuracy: 0.7977\n",
      "Epoch 175/200\n",
      "learning rate scheduled to 0.00017224998824531213\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1355.0194 - accuracy: 0.7967 - val_loss: 1354.8586 - val_accuracy: 0.8047\n",
      "Epoch 176/200\n",
      "learning rate scheduled to 0.00017052748749847522\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1354.7129 - accuracy: 0.7950 - val_loss: 1354.5573 - val_accuracy: 0.7977\n",
      "Epoch 177/200\n",
      "learning rate scheduled to 0.00016882221680134535\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1354.4061 - accuracy: 0.7947 - val_loss: 1354.2593 - val_accuracy: 0.7924\n",
      "Epoch 178/200\n",
      "learning rate scheduled to 0.00016713398887077346\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1354.1034 - accuracy: 0.7946 - val_loss: 1353.9506 - val_accuracy: 0.7939\n",
      "Epoch 179/200\n",
      "learning rate scheduled to 0.00016546264523640276\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1353.8018 - accuracy: 0.7989 - val_loss: 1353.6405 - val_accuracy: 0.7998\n",
      "Epoch 180/200\n",
      "learning rate scheduled to 0.00016380801302148028\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1353.5048 - accuracy: 0.7971 - val_loss: 1353.3633 - val_accuracy: 0.7888\n",
      "Epoch 181/200\n",
      "learning rate scheduled to 0.00016216993375564926\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1353.2123 - accuracy: 0.7964 - val_loss: 1353.0725 - val_accuracy: 0.7817\n",
      "Epoch 182/200\n",
      "learning rate scheduled to 0.00016054823456215672\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1352.9241 - accuracy: 0.7940 - val_loss: 1352.7775 - val_accuracy: 0.7977\n",
      "Epoch 183/200\n",
      "learning rate scheduled to 0.00015894275697064586\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1352.6340 - accuracy: 0.7958 - val_loss: 1352.4875 - val_accuracy: 0.7941\n",
      "Epoch 184/200\n",
      "learning rate scheduled to 0.00015735332810436374\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1352.3513 - accuracy: 0.7947 - val_loss: 1352.2054 - val_accuracy: 0.7905\n",
      "Epoch 185/200\n",
      "learning rate scheduled to 0.00015577978949295356\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1352.0699 - accuracy: 0.7968 - val_loss: 1351.9360 - val_accuracy: 0.7947\n",
      "Epoch 186/200\n",
      "learning rate scheduled to 0.00015422199707245453\n",
      "166/166 [==============================] - 13s 73ms/step - loss: 1351.7932 - accuracy: 0.7956 - val_loss: 1351.6587 - val_accuracy: 0.7920\n",
      "Epoch 187/200\n",
      "learning rate scheduled to 0.00015267977796611375\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1351.5176 - accuracy: 0.7953 - val_loss: 1351.3815 - val_accuracy: 0.7879\n",
      "Epoch 188/200\n",
      "learning rate scheduled to 0.00015115297370357439\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1351.2461 - accuracy: 0.7964 - val_loss: 1351.1014 - val_accuracy: 0.8034\n",
      "Epoch 189/200\n",
      "learning rate scheduled to 0.00014964144022087568\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1350.9766 - accuracy: 0.7956 - val_loss: 1350.8362 - val_accuracy: 0.8002\n",
      "Epoch 190/200\n",
      "learning rate scheduled to 0.00014814501904766075\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1350.7079 - accuracy: 0.7954 - val_loss: 1350.5720 - val_accuracy: 0.7937\n",
      "Epoch 191/200\n",
      "learning rate scheduled to 0.00014666356611996888\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1350.4470 - accuracy: 0.7931 - val_loss: 1350.3146 - val_accuracy: 0.7975\n",
      "Epoch 192/200\n",
      "learning rate scheduled to 0.00014519693737383933\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1350.1842 - accuracy: 0.7954 - val_loss: 1350.0581 - val_accuracy: 0.7960\n",
      "Epoch 193/200\n",
      "learning rate scheduled to 0.0001437449743389152\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1349.9260 - accuracy: 0.7943 - val_loss: 1349.8063 - val_accuracy: 0.7909\n",
      "Epoch 194/200\n",
      "learning rate scheduled to 0.00014230751854483968\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1349.6687 - accuracy: 0.7982 - val_loss: 1349.5514 - val_accuracy: 0.7870\n",
      "Epoch 195/200\n",
      "learning rate scheduled to 0.00014088444033404812\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1349.4161 - accuracy: 0.7967 - val_loss: 1349.2786 - val_accuracy: 0.8011\n",
      "Epoch 196/200\n",
      "learning rate scheduled to 0.0001394755956425797\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1349.1630 - accuracy: 0.7984 - val_loss: 1349.0345 - val_accuracy: 0.7998\n",
      "Epoch 197/200\n",
      "learning rate scheduled to 0.00013808084040647372\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1348.9170 - accuracy: 0.7957 - val_loss: 1348.7848 - val_accuracy: 0.8020\n",
      "Epoch 198/200\n",
      "learning rate scheduled to 0.00013670003056176939\n",
      "166/166 [==============================] - 13s 72ms/step - loss: 1348.6669 - accuracy: 0.7979 - val_loss: 1348.5424 - val_accuracy: 0.7885\n",
      "Epoch 199/200\n",
      "learning rate scheduled to 0.000135333036450902\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1348.4230 - accuracy: 0.7962 - val_loss: 1348.2994 - val_accuracy: 0.8005\n",
      "Epoch 200/200\n",
      "learning rate scheduled to 0.00013397969960351474\n",
      "166/166 [==============================] - 13s 71ms/step - loss: 1348.1787 - accuracy: 0.7969 - val_loss: 1348.0508 - val_accuracy: 0.8047\n"
     ]
    }
   ],
   "source": [
    "history_original_siamese_model = original_siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                              model_checkpoint_callback, WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 2s 23ms/step - loss: 1348.0594 - accuracy: 0.7954\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Second Run - 90k Pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_90k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_90k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 105, 105, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 96, 96, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_2 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1te8jbgy\" target=\"_blank\">driven-snow-5</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 90k\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "original_siamese_model_2.compile(loss=config.loss_function,\n",
    "                               optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_90k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "learning rate scheduled to 0.0009900000470224768\n",
      "610/610 [==============================] - 45s 64ms/step - loss: 1507.0350 - accuracy: 0.5532 - val_loss: 1503.3882 - val_accuracy: 0.5779\n",
      "Epoch 2/200\n",
      "learning rate scheduled to 0.000980100086890161\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1499.8073 - accuracy: 0.5696 - val_loss: 1496.2098 - val_accuracy: 0.5619\n",
      "Epoch 3/200\n",
      "learning rate scheduled to 0.0009702991275116801\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1492.6814 - accuracy: 0.5683 - val_loss: 1489.1345 - val_accuracy: 0.5802\n",
      "Epoch 4/200\n",
      "learning rate scheduled to 0.0009605961316265165\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1485.6611 - accuracy: 0.5803 - val_loss: 1482.1699 - val_accuracy: 0.5906\n",
      "Epoch 5/200\n",
      "learning rate scheduled to 0.0009509901772253215\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1478.7444 - accuracy: 0.5917 - val_loss: 1475.3059 - val_accuracy: 0.5928\n",
      "Epoch 6/200\n",
      "learning rate scheduled to 0.0009414802846731617\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1471.9305 - accuracy: 0.6039 - val_loss: 1468.5360 - val_accuracy: 0.6190\n",
      "Epoch 7/200\n",
      "learning rate scheduled to 0.0009320654743351042\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1465.2065 - accuracy: 0.6267 - val_loss: 1461.8644 - val_accuracy: 0.6321\n",
      "Epoch 8/200\n",
      "learning rate scheduled to 0.0009227448242017999\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1458.5757 - accuracy: 0.6484 - val_loss: 1455.2772 - val_accuracy: 0.6582\n",
      "Epoch 9/200\n",
      "learning rate scheduled to 0.0009135173546383158\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1452.0315 - accuracy: 0.6749 - val_loss: 1448.7897 - val_accuracy: 0.6790\n",
      "Epoch 10/200\n",
      "learning rate scheduled to 0.0009043822012608871\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1445.5803 - accuracy: 0.7005 - val_loss: 1442.4174 - val_accuracy: 0.6789\n",
      "Epoch 11/200\n",
      "learning rate scheduled to 0.0008953383844345808\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1439.2347 - accuracy: 0.7116 - val_loss: 1436.0679 - val_accuracy: 0.7206\n",
      "Epoch 12/200\n",
      "learning rate scheduled to 0.000886384982150048\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1432.9813 - accuracy: 0.7225 - val_loss: 1429.9125 - val_accuracy: 0.7014\n",
      "Epoch 13/200\n",
      "learning rate scheduled to 0.0008775211300235242\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1426.8206 - accuracy: 0.7287 - val_loss: 1423.9325 - val_accuracy: 0.5117\n",
      "Epoch 14/200\n",
      "learning rate scheduled to 0.0008687459060456604\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1420.7604 - accuracy: 0.7171 - val_loss: 1417.7642 - val_accuracy: 0.7122\n",
      "Epoch 15/200\n",
      "learning rate scheduled to 0.0008600584458326921\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1414.7615 - accuracy: 0.7413 - val_loss: 1411.9445 - val_accuracy: 0.5789\n",
      "Epoch 16/200\n",
      "learning rate scheduled to 0.0008514578850008547\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1408.8625 - accuracy: 0.7464 - val_loss: 1405.9989 - val_accuracy: 0.7011\n",
      "Epoch 17/200\n",
      "learning rate scheduled to 0.0008429433015407995\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1403.0518 - accuracy: 0.7462 - val_loss: 1400.1442 - val_accuracy: 0.7484\n",
      "Epoch 18/200\n",
      "learning rate scheduled to 0.0008345138886943459\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1397.3131 - accuracy: 0.7526 - val_loss: 1394.4819 - val_accuracy: 0.7435\n",
      "Epoch 19/200\n",
      "learning rate scheduled to 0.0008261687244521454\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1391.6644 - accuracy: 0.7538 - val_loss: 1388.8376 - val_accuracy: 0.7611\n",
      "Epoch 20/200\n",
      "learning rate scheduled to 0.0008179070596816018\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1386.0854 - accuracy: 0.7590 - val_loss: 1383.4348 - val_accuracy: 0.6671\n",
      "Epoch 21/200\n",
      "learning rate scheduled to 0.0008097279723733664\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1380.5967 - accuracy: 0.7584 - val_loss: 1377.8844 - val_accuracy: 0.7514\n",
      "Epoch 22/200\n",
      "learning rate scheduled to 0.000801630713394843\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1375.1671 - accuracy: 0.7691 - val_loss: 1372.4764 - val_accuracy: 0.7705\n",
      "Epoch 23/200\n",
      "learning rate scheduled to 0.0007936144183622674\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1369.8274 - accuracy: 0.7668 - val_loss: 1367.1617 - val_accuracy: 0.7666\n",
      "Epoch 24/200\n",
      "learning rate scheduled to 0.0007856782805174589\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1364.5526 - accuracy: 0.7740 - val_loss: 1361.9307 - val_accuracy: 0.7708\n",
      "Epoch 25/200\n",
      "learning rate scheduled to 0.0007778214931022376\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1359.3542 - accuracy: 0.7776 - val_loss: 1356.8906 - val_accuracy: 0.6782\n",
      "Epoch 26/200\n",
      "learning rate scheduled to 0.0007700433069840073\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1354.2271 - accuracy: 0.7795 - val_loss: 1351.6852 - val_accuracy: 0.7811\n",
      "Epoch 27/200\n",
      "learning rate scheduled to 0.0007623428577790037\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1349.1678 - accuracy: 0.7838 - val_loss: 1346.6478 - val_accuracy: 0.7867\n",
      "Epoch 28/200\n",
      "learning rate scheduled to 0.0007547194539802149\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1344.1853 - accuracy: 0.7830 - val_loss: 1341.7061 - val_accuracy: 0.7760\n",
      "Epoch 29/200\n",
      "learning rate scheduled to 0.0007471722312038764\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1339.2679 - accuracy: 0.7841 - val_loss: 1336.8086 - val_accuracy: 0.7930\n",
      "Epoch 30/200\n",
      "learning rate scheduled to 0.0007397004979429766\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1334.4143 - accuracy: 0.7884 - val_loss: 1332.3622 - val_accuracy: 0.4982\n",
      "Epoch 31/200\n",
      "learning rate scheduled to 0.0007323035050649196\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1329.6329 - accuracy: 0.7857 - val_loss: 1327.2433 - val_accuracy: 0.7890\n",
      "Epoch 32/200\n",
      "learning rate scheduled to 0.000724980445811525\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1324.9077 - accuracy: 0.7906 - val_loss: 1322.5947 - val_accuracy: 0.7646\n",
      "Epoch 33/200\n",
      "learning rate scheduled to 0.0007177306286757812\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1320.2485 - accuracy: 0.7923 - val_loss: 1317.9796 - val_accuracy: 0.7568\n",
      "Epoch 34/200\n",
      "learning rate scheduled to 0.0007105533045250923\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1315.6534 - accuracy: 0.7938 - val_loss: 1313.3619 - val_accuracy: 0.7985\n",
      "Epoch 35/200\n",
      "learning rate scheduled to 0.0007034477818524464\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1311.1200 - accuracy: 0.7961 - val_loss: 1309.2632 - val_accuracy: 0.5048\n",
      "Epoch 36/200\n",
      "learning rate scheduled to 0.000696413311525248\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1306.6477 - accuracy: 0.7966 - val_loss: 1304.4231 - val_accuracy: 0.7940\n",
      "Epoch 37/200\n",
      "learning rate scheduled to 0.0006894492020364851\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1302.2341 - accuracy: 0.7983 - val_loss: 1300.0413 - val_accuracy: 0.7912\n",
      "Epoch 38/200\n",
      "learning rate scheduled to 0.0006825547042535618\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1297.8794 - accuracy: 0.7992 - val_loss: 1295.7295 - val_accuracy: 0.7862\n",
      "Epoch 39/200\n",
      "learning rate scheduled to 0.0006757291842950508\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1293.5837 - accuracy: 0.8019 - val_loss: 1291.4979 - val_accuracy: 0.7638\n",
      "Epoch 40/200\n",
      "learning rate scheduled to 0.0006689718930283561\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1289.3441 - accuracy: 0.8030 - val_loss: 1287.2780 - val_accuracy: 0.7711\n",
      "Epoch 41/200\n",
      "learning rate scheduled to 0.0006622821965720505\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1285.1630 - accuracy: 0.8022 - val_loss: 1283.1990 - val_accuracy: 0.7346\n",
      "Epoch 42/200\n",
      "learning rate scheduled to 0.0006556594034191221\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1281.0383 - accuracy: 0.8036 - val_loss: 1278.9851 - val_accuracy: 0.8039\n",
      "Epoch 43/200\n",
      "learning rate scheduled to 0.0006491028220625594\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1276.9648 - accuracy: 0.8040 - val_loss: 1275.1462 - val_accuracy: 0.6613\n",
      "Epoch 44/200\n",
      "learning rate scheduled to 0.0006426118186209351\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1272.9481 - accuracy: 0.8042 - val_loss: 1271.0267 - val_accuracy: 0.7601\n",
      "Epoch 45/200\n",
      "learning rate scheduled to 0.0006361857015872375\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1268.9812 - accuracy: 0.8060 - val_loss: 1266.9989 - val_accuracy: 0.8101\n",
      "Epoch 46/200\n",
      "learning rate scheduled to 0.0006298238370800391\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1265.0660 - accuracy: 0.8077 - val_loss: 1263.2086 - val_accuracy: 0.7453\n",
      "Epoch 47/200\n",
      "learning rate scheduled to 0.0006235255912179127\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1261.2062 - accuracy: 0.8076 - val_loss: 1259.3153 - val_accuracy: 0.7862\n",
      "Epoch 48/200\n",
      "learning rate scheduled to 0.000617290330119431\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1257.3931 - accuracy: 0.8064 - val_loss: 1255.7924 - val_accuracy: 0.6621\n",
      "Epoch 49/200\n",
      "learning rate scheduled to 0.0006111174199031666\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1253.6295 - accuracy: 0.8077 - val_loss: 1252.0464 - val_accuracy: 0.6237\n",
      "Epoch 50/200\n",
      "learning rate scheduled to 0.0006050062266876921\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1249.9150 - accuracy: 0.8088 - val_loss: 1248.0654 - val_accuracy: 0.8070\n",
      "Epoch 51/200\n",
      "learning rate scheduled to 0.0005989561742171646\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1246.2469 - accuracy: 0.8105 - val_loss: 1244.4200 - val_accuracy: 0.8149\n",
      "Epoch 52/200\n",
      "learning rate scheduled to 0.0005929666286101564\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1242.6287 - accuracy: 0.8113 - val_loss: 1240.8542 - val_accuracy: 0.7861\n",
      "Epoch 53/200\n",
      "learning rate scheduled to 0.0005870369559852406\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1239.0557 - accuracy: 0.8118 - val_loss: 1237.2771 - val_accuracy: 0.8110\n",
      "Epoch 54/200\n",
      "learning rate scheduled to 0.000581166580086574\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1235.5272 - accuracy: 0.8122 - val_loss: 1233.7715 - val_accuracy: 0.8144\n",
      "Epoch 55/200\n",
      "learning rate scheduled to 0.0005753549246583134\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1232.0459 - accuracy: 0.8137 - val_loss: 1230.3989 - val_accuracy: 0.7556\n",
      "Epoch 56/200\n",
      "learning rate scheduled to 0.0005696013558190316\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1228.6112 - accuracy: 0.8122 - val_loss: 1226.8976 - val_accuracy: 0.8139\n",
      "Epoch 57/200\n",
      "learning rate scheduled to 0.0005639053549384699\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1225.2148 - accuracy: 0.8154 - val_loss: 1223.6423 - val_accuracy: 0.7378\n",
      "Epoch 58/200\n",
      "learning rate scheduled to 0.0005582662881352008\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1221.8671 - accuracy: 0.8142 - val_loss: 1220.2166 - val_accuracy: 0.8042\n",
      "Epoch 59/200\n",
      "learning rate scheduled to 0.0005526836367789656\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1218.5553 - accuracy: 0.8171 - val_loss: 1216.9401 - val_accuracy: 0.7960\n",
      "Epoch 60/200\n",
      "learning rate scheduled to 0.0005471568246139213\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1215.2919 - accuracy: 0.8160 - val_loss: 1214.3577 - val_accuracy: 0.5004\n",
      "Epoch 61/200\n",
      "learning rate scheduled to 0.000541685275384225\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1212.0751 - accuracy: 0.8131 - val_loss: 1210.4591 - val_accuracy: 0.8192\n",
      "Epoch 62/200\n",
      "learning rate scheduled to 0.0005362684128340334\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1208.8845 - accuracy: 0.8170 - val_loss: 1207.3274 - val_accuracy: 0.8000\n",
      "Epoch 63/200\n",
      "learning rate scheduled to 0.0005309057183330878\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1205.7416 - accuracy: 0.8178 - val_loss: 1204.2291 - val_accuracy: 0.7894\n",
      "Epoch 64/200\n",
      "learning rate scheduled to 0.0005255966732511297\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1202.6344 - accuracy: 0.8182 - val_loss: 1201.0917 - val_accuracy: 0.8188\n",
      "Epoch 65/200\n",
      "learning rate scheduled to 0.0005203407013323158\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1199.5699 - accuracy: 0.8205 - val_loss: 1198.0327 - val_accuracy: 0.8232\n",
      "Epoch 66/200\n",
      "learning rate scheduled to 0.0005151372839463875\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1196.5426 - accuracy: 0.8198 - val_loss: 1195.1063 - val_accuracy: 0.7798\n",
      "Epoch 67/200\n",
      "learning rate scheduled to 0.0005099859024630859\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1193.5537 - accuracy: 0.8202 - val_loss: 1192.0641 - val_accuracy: 0.8219\n",
      "Epoch 68/200\n",
      "learning rate scheduled to 0.0005048860382521525\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1190.6021 - accuracy: 0.8213 - val_loss: 1189.1271 - val_accuracy: 0.8257\n",
      "Epoch 69/200\n",
      "learning rate scheduled to 0.0004998371726833284\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1187.6862 - accuracy: 0.8217 - val_loss: 1186.3461 - val_accuracy: 0.7636\n",
      "Epoch 70/200\n",
      "learning rate scheduled to 0.0004948387871263549\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1184.8069 - accuracy: 0.8232 - val_loss: 1183.3694 - val_accuracy: 0.8249\n",
      "Epoch 71/200\n",
      "learning rate scheduled to 0.0004898904205765575\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1181.9645 - accuracy: 0.8212 - val_loss: 1180.5573 - val_accuracy: 0.8160\n",
      "Epoch 72/200\n",
      "learning rate scheduled to 0.00048499149677809326\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1179.1549 - accuracy: 0.8236 - val_loss: 1177.7479 - val_accuracy: 0.8287\n",
      "Epoch 73/200\n",
      "learning rate scheduled to 0.00048014158353907986\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1176.3818 - accuracy: 0.8235 - val_loss: 1174.9929 - val_accuracy: 0.8285\n",
      "Epoch 74/200\n",
      "learning rate scheduled to 0.00047534016222925855\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1173.6417 - accuracy: 0.8254 - val_loss: 1172.2737 - val_accuracy: 0.8278\n",
      "Epoch 75/200\n",
      "learning rate scheduled to 0.0004705867718439549\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1170.9366 - accuracy: 0.8248 - val_loss: 1169.6934 - val_accuracy: 0.7668\n",
      "Epoch 76/200\n",
      "learning rate scheduled to 0.0004658808937529102\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1168.2642 - accuracy: 0.8251 - val_loss: 1166.9368 - val_accuracy: 0.8224\n",
      "Epoch 77/200\n",
      "learning rate scheduled to 0.0004612220957642421\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1165.6234 - accuracy: 0.8256 - val_loss: 1164.3176 - val_accuracy: 0.8191\n",
      "Epoch 78/200\n",
      "learning rate scheduled to 0.00045660988806048406\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1163.0153 - accuracy: 0.8261 - val_loss: 1161.7191 - val_accuracy: 0.8261\n",
      "Epoch 79/200\n",
      "learning rate scheduled to 0.0004520437808241695\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1160.4387 - accuracy: 0.8291 - val_loss: 1159.1459 - val_accuracy: 0.8338\n",
      "Epoch 80/200\n",
      "learning rate scheduled to 0.0004475233418634161\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1157.8934 - accuracy: 0.8282 - val_loss: 1156.6318 - val_accuracy: 0.8237\n",
      "Epoch 81/200\n",
      "learning rate scheduled to 0.0004430481101735495\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1155.3828 - accuracy: 0.8286 - val_loss: 1154.2388 - val_accuracy: 0.7659\n",
      "Epoch 82/200\n",
      "learning rate scheduled to 0.0004386176247498952\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1152.8998 - accuracy: 0.8290 - val_loss: 1151.6813 - val_accuracy: 0.8149\n",
      "Epoch 83/200\n",
      "learning rate scheduled to 0.00043423145340057087\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1150.4443 - accuracy: 0.8305 - val_loss: 1149.2217 - val_accuracy: 0.8292\n",
      "Epoch 84/200\n",
      "learning rate scheduled to 0.0004298891351209022\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1148.0221 - accuracy: 0.8306 - val_loss: 1146.8552 - val_accuracy: 0.8079\n",
      "Epoch 85/200\n",
      "learning rate scheduled to 0.00042559023771900686\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1145.6272 - accuracy: 0.8320 - val_loss: 1144.5083 - val_accuracy: 0.7868\n",
      "Epoch 86/200\n",
      "learning rate scheduled to 0.0004213343290030025\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1143.2610 - accuracy: 0.8333 - val_loss: 1142.0750 - val_accuracy: 0.8379\n",
      "Epoch 87/200\n",
      "learning rate scheduled to 0.00041712097678100687\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1140.9270 - accuracy: 0.8323 - val_loss: 1139.7670 - val_accuracy: 0.8286\n",
      "Epoch 88/200\n",
      "learning rate scheduled to 0.00041294977767392994\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1138.6189 - accuracy: 0.8322 - val_loss: 1137.4767 - val_accuracy: 0.8256\n",
      "Epoch 89/200\n",
      "learning rate scheduled to 0.0004088202706770971\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1136.3365 - accuracy: 0.8342 - val_loss: 1135.2081 - val_accuracy: 0.8252\n",
      "Epoch 90/200\n",
      "learning rate scheduled to 0.00040473208122421056\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1134.0814 - accuracy: 0.8363 - val_loss: 1132.9755 - val_accuracy: 0.8236\n",
      "Epoch 91/200\n",
      "learning rate scheduled to 0.0004006847483105957\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1131.8536 - accuracy: 0.8363 - val_loss: 1130.7465 - val_accuracy: 0.8337\n",
      "Epoch 92/200\n",
      "learning rate scheduled to 0.0003966778973699547\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1129.6555 - accuracy: 0.8354 - val_loss: 1128.5698 - val_accuracy: 0.8252\n",
      "Epoch 93/200\n",
      "learning rate scheduled to 0.0003927111250231974\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1127.4822 - accuracy: 0.8356 - val_loss: 1126.3971 - val_accuracy: 0.8343\n",
      "Epoch 94/200\n",
      "learning rate scheduled to 0.0003887840278912336\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1125.3334 - accuracy: 0.8353 - val_loss: 1124.2821 - val_accuracy: 0.8236\n",
      "Epoch 95/200\n",
      "learning rate scheduled to 0.000384896173782181\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1123.2112 - accuracy: 0.8371 - val_loss: 1122.1509 - val_accuracy: 0.8348\n",
      "Epoch 96/200\n",
      "learning rate scheduled to 0.00038104721694253384\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1121.1149 - accuracy: 0.8369 - val_loss: 1120.1112 - val_accuracy: 0.8127\n",
      "Epoch 97/200\n",
      "learning rate scheduled to 0.000377236753993202\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1119.0403 - accuracy: 0.8390 - val_loss: 1118.0361 - val_accuracy: 0.8176\n",
      "Epoch 98/200\n",
      "learning rate scheduled to 0.0003734643815550953\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1116.9944 - accuracy: 0.8385 - val_loss: 1115.9784 - val_accuracy: 0.8341\n",
      "Epoch 99/200\n",
      "learning rate scheduled to 0.0003697297250619158\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1114.9668 - accuracy: 0.8398 - val_loss: 1113.9742 - val_accuracy: 0.8292\n",
      "Epoch 100/200\n",
      "learning rate scheduled to 0.0003660324387601577\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1112.9680 - accuracy: 0.8393 - val_loss: 1111.9747 - val_accuracy: 0.8375\n",
      "Epoch 101/200\n",
      "learning rate scheduled to 0.00036237211927073074\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1110.9917 - accuracy: 0.8406 - val_loss: 1109.9973 - val_accuracy: 0.8464\n",
      "Epoch 102/200\n",
      "learning rate scheduled to 0.0003587483920273371\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1109.0380 - accuracy: 0.8408 - val_loss: 1108.0706 - val_accuracy: 0.8364\n",
      "Epoch 103/200\n",
      "learning rate scheduled to 0.0003551609112764709\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1107.1090 - accuracy: 0.8415 - val_loss: 1106.1476 - val_accuracy: 0.8423\n",
      "Epoch 104/200\n",
      "learning rate scheduled to 0.0003516093024518341\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1105.2012 - accuracy: 0.8410 - val_loss: 1104.2662 - val_accuracy: 0.8316\n",
      "Epoch 105/200\n",
      "learning rate scheduled to 0.0003480932197999209\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1103.3152 - accuracy: 0.8414 - val_loss: 1102.3723 - val_accuracy: 0.8418\n",
      "Epoch 106/200\n",
      "learning rate scheduled to 0.0003446122887544334\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1101.4526 - accuracy: 0.8426 - val_loss: 1100.5234 - val_accuracy: 0.8413\n",
      "Epoch 107/200\n",
      "learning rate scheduled to 0.00034116616356186566\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1099.6096 - accuracy: 0.8422 - val_loss: 1098.7008 - val_accuracy: 0.8373\n",
      "Epoch 108/200\n",
      "learning rate scheduled to 0.00033775449846871195\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1097.7881 - accuracy: 0.8442 - val_loss: 1096.8916 - val_accuracy: 0.8382\n",
      "Epoch 109/200\n",
      "learning rate scheduled to 0.0003343769477214664\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1095.9888 - accuracy: 0.8449 - val_loss: 1095.0864 - val_accuracy: 0.8476\n",
      "Epoch 110/200\n",
      "learning rate scheduled to 0.0003310331655666232\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1094.2123 - accuracy: 0.8446 - val_loss: 1093.3250 - val_accuracy: 0.8441\n",
      "Epoch 111/200\n",
      "learning rate scheduled to 0.00032772283506346864\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1092.4554 - accuracy: 0.8456 - val_loss: 1091.5901 - val_accuracy: 0.8375\n",
      "Epoch 112/200\n",
      "learning rate scheduled to 0.00032444561045849693\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1090.7177 - accuracy: 0.8455 - val_loss: 1089.8679 - val_accuracy: 0.8338\n",
      "Epoch 113/200\n",
      "learning rate scheduled to 0.00032120114599820226\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1089.0028 - accuracy: 0.8455 - val_loss: 1088.2227 - val_accuracy: 0.8072\n",
      "Epoch 114/200\n",
      "learning rate scheduled to 0.00031798912474187093\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1087.3046 - accuracy: 0.8460 - val_loss: 1086.4847 - val_accuracy: 0.8344\n",
      "Epoch 115/200\n",
      "learning rate scheduled to 0.0003148092297487892\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1085.6250 - accuracy: 0.8475 - val_loss: 1084.7831 - val_accuracy: 0.8529\n",
      "Epoch 116/200\n",
      "learning rate scheduled to 0.0003116611440782435\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1083.9661 - accuracy: 0.8457 - val_loss: 1083.1378 - val_accuracy: 0.8496\n",
      "Epoch 117/200\n",
      "learning rate scheduled to 0.000308544521976728\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1082.3269 - accuracy: 0.8474 - val_loss: 1081.5848 - val_accuracy: 0.8068\n",
      "Epoch 118/200\n",
      "learning rate scheduled to 0.0003054590753163211\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1080.7068 - accuracy: 0.8471 - val_loss: 1079.9425 - val_accuracy: 0.8206\n",
      "Epoch 119/200\n",
      "learning rate scheduled to 0.0003024044871563092\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1079.1039 - accuracy: 0.8488 - val_loss: 1078.3130 - val_accuracy: 0.8419\n",
      "Epoch 120/200\n",
      "learning rate scheduled to 0.00029938044055597855\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1077.5222 - accuracy: 0.8490 - val_loss: 1076.7236 - val_accuracy: 0.8556\n",
      "Epoch 121/200\n",
      "learning rate scheduled to 0.0002963866473874077\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1075.9584 - accuracy: 0.8479 - val_loss: 1075.1970 - val_accuracy: 0.8334\n",
      "Epoch 122/200\n",
      "learning rate scheduled to 0.000293422790709883\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1074.4094 - accuracy: 0.8499 - val_loss: 1073.6345 - val_accuracy: 0.8543\n",
      "Epoch 123/200\n",
      "learning rate scheduled to 0.00029048855358269063\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1072.8792 - accuracy: 0.8500 - val_loss: 1072.1644 - val_accuracy: 0.8206\n",
      "Epoch 124/200\n",
      "learning rate scheduled to 0.0002875836766907014\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1071.3691 - accuracy: 0.8499 - val_loss: 1070.6438 - val_accuracy: 0.8343\n",
      "Epoch 125/200\n",
      "learning rate scheduled to 0.0002847078430932015\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1069.8728 - accuracy: 0.8498 - val_loss: 1069.1395 - val_accuracy: 0.8418\n",
      "Epoch 126/200\n",
      "learning rate scheduled to 0.0002818607646622695\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1068.3940 - accuracy: 0.8495 - val_loss: 1067.6678 - val_accuracy: 0.8428\n",
      "Epoch 127/200\n",
      "learning rate scheduled to 0.00027904215326998385\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1066.9331 - accuracy: 0.8506 - val_loss: 1066.2031 - val_accuracy: 0.8496\n",
      "Epoch 128/200\n",
      "learning rate scheduled to 0.00027625172078842296\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1065.4872 - accuracy: 0.8502 - val_loss: 1064.8131 - val_accuracy: 0.8252\n",
      "Epoch 129/200\n",
      "learning rate scheduled to 0.0002734892079024576\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1064.0582 - accuracy: 0.8524 - val_loss: 1063.3521 - val_accuracy: 0.8464\n",
      "Epoch 130/200\n",
      "learning rate scheduled to 0.0002707543264841661\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1062.6478 - accuracy: 0.8520 - val_loss: 1061.9620 - val_accuracy: 0.8401\n",
      "Epoch 131/200\n",
      "learning rate scheduled to 0.000268046788405627\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1061.2539 - accuracy: 0.8506 - val_loss: 1060.5667 - val_accuracy: 0.8438\n",
      "Epoch 132/200\n",
      "learning rate scheduled to 0.000265366334351711\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1059.8719 - accuracy: 0.8519 - val_loss: 1059.2736 - val_accuracy: 0.8019\n",
      "Epoch 133/200\n",
      "learning rate scheduled to 0.00026271267619449646\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1058.5066 - accuracy: 0.8533 - val_loss: 1057.8271 - val_accuracy: 0.8525\n",
      "Epoch 134/200\n",
      "learning rate scheduled to 0.00026008555461885406\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1057.1593 - accuracy: 0.8527 - val_loss: 1056.4939 - val_accuracy: 0.8471\n",
      "Epoch 135/200\n",
      "learning rate scheduled to 0.00025748471030965445\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1055.8246 - accuracy: 0.8537 - val_loss: 1055.1718 - val_accuracy: 0.8444\n",
      "Epoch 136/200\n",
      "learning rate scheduled to 0.0002549098551389761\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1054.5040 - accuracy: 0.8531 - val_loss: 1053.8458 - val_accuracy: 0.8531\n",
      "Epoch 137/200\n",
      "learning rate scheduled to 0.00025236075860448183\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1053.1976 - accuracy: 0.8550 - val_loss: 1052.5500 - val_accuracy: 0.8526\n",
      "Epoch 138/200\n",
      "learning rate scheduled to 0.0002498371613910422\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1051.9077 - accuracy: 0.8542 - val_loss: 1051.2808 - val_accuracy: 0.8414\n",
      "Epoch 139/200\n",
      "learning rate scheduled to 0.0002473388041835278\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1050.6322 - accuracy: 0.8551 - val_loss: 1049.9951 - val_accuracy: 0.8534\n",
      "Epoch 140/200\n",
      "learning rate scheduled to 0.0002448654276668094\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1049.3719 - accuracy: 0.8539 - val_loss: 1048.7483 - val_accuracy: 0.8500\n",
      "Epoch 141/200\n",
      "learning rate scheduled to 0.00024241677252575755\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1048.1259 - accuracy: 0.8541 - val_loss: 1047.5226 - val_accuracy: 0.8443\n",
      "Epoch 142/200\n",
      "learning rate scheduled to 0.00023999260825803502\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1046.8923 - accuracy: 0.8553 - val_loss: 1046.2808 - val_accuracy: 0.8526\n",
      "Epoch 143/200\n",
      "learning rate scheduled to 0.00023759267554851249\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1045.6752 - accuracy: 0.8552 - val_loss: 1045.0979 - val_accuracy: 0.8377\n",
      "Epoch 144/200\n",
      "learning rate scheduled to 0.0002352167438948527\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1044.4696 - accuracy: 0.8553 - val_loss: 1043.8635 - val_accuracy: 0.8566\n",
      "Epoch 145/200\n",
      "learning rate scheduled to 0.00023286458279471843\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1043.2773 - accuracy: 0.8560 - val_loss: 1042.6926 - val_accuracy: 0.8461\n",
      "Epoch 146/200\n",
      "learning rate scheduled to 0.00023053593293298035\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1042.0984 - accuracy: 0.8562 - val_loss: 1041.5103 - val_accuracy: 0.8563\n",
      "Epoch 147/200\n",
      "learning rate scheduled to 0.0002282305782136973\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1040.9335 - accuracy: 0.8560 - val_loss: 1040.3508 - val_accuracy: 0.8587\n",
      "Epoch 148/200\n",
      "learning rate scheduled to 0.00022594827372813598\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1039.7791 - accuracy: 0.8561 - val_loss: 1039.2238 - val_accuracy: 0.8438\n",
      "Epoch 149/200\n",
      "learning rate scheduled to 0.00022368878897395915\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1038.6377 - accuracy: 0.8562 - val_loss: 1038.0745 - val_accuracy: 0.8540\n",
      "Epoch 150/200\n",
      "learning rate scheduled to 0.00022145190785522573\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1037.5084 - accuracy: 0.8564 - val_loss: 1037.0017 - val_accuracy: 0.8244\n",
      "Epoch 151/200\n",
      "learning rate scheduled to 0.00021923738546320237\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1036.3940 - accuracy: 0.8564 - val_loss: 1035.8406 - val_accuracy: 0.8538\n",
      "Epoch 152/200\n",
      "learning rate scheduled to 0.00021704500570194797\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1035.2878 - accuracy: 0.8582 - val_loss: 1034.7795 - val_accuracy: 0.8336\n",
      "Epoch 153/200\n",
      "learning rate scheduled to 0.00021487455247552135\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1034.1986 - accuracy: 0.8585 - val_loss: 1033.6752 - val_accuracy: 0.8450\n",
      "Epoch 154/200\n",
      "learning rate scheduled to 0.00021272580968798138\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1033.1208 - accuracy: 0.8589 - val_loss: 1032.6250 - val_accuracy: 0.8319\n",
      "Epoch 155/200\n",
      "learning rate scheduled to 0.00021059854683699086\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1032.0563 - accuracy: 0.8583 - val_loss: 1031.5516 - val_accuracy: 0.8415\n",
      "Epoch 156/200\n",
      "learning rate scheduled to 0.0002084925622330047\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1031.0018 - accuracy: 0.8577 - val_loss: 1030.4819 - val_accuracy: 0.8548\n",
      "Epoch 157/200\n",
      "learning rate scheduled to 0.0002064076397800818\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1029.9581 - accuracy: 0.8598 - val_loss: 1029.4443 - val_accuracy: 0.8558\n",
      "Epoch 158/200\n",
      "learning rate scheduled to 0.000204343563382281\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1028.9277 - accuracy: 0.8596 - val_loss: 1028.4395 - val_accuracy: 0.8423\n",
      "Epoch 159/200\n",
      "learning rate scheduled to 0.0002023001313500572\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1027.9088 - accuracy: 0.8572 - val_loss: 1027.4178 - val_accuracy: 0.8483\n",
      "Epoch 160/200\n",
      "learning rate scheduled to 0.0002002771275874693\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1026.8987 - accuracy: 0.8588 - val_loss: 1026.4012 - val_accuracy: 0.8568\n",
      "Epoch 161/200\n",
      "learning rate scheduled to 0.0001982743504049722\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1025.8995 - accuracy: 0.8597 - val_loss: 1025.4055 - val_accuracy: 0.8556\n",
      "Epoch 162/200\n",
      "learning rate scheduled to 0.00019629161251941696\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1024.9137 - accuracy: 0.8584 - val_loss: 1024.4316 - val_accuracy: 0.8535\n",
      "Epoch 163/200\n",
      "learning rate scheduled to 0.0001943286978348624\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1023.9340 - accuracy: 0.8603 - val_loss: 1023.4388 - val_accuracy: 0.8651\n",
      "Epoch 164/200\n",
      "learning rate scheduled to 0.00019238540466176346\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1022.9664 - accuracy: 0.8603 - val_loss: 1022.5065 - val_accuracy: 0.8476\n",
      "Epoch 165/200\n",
      "learning rate scheduled to 0.00019046154571697116\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1022.0112 - accuracy: 0.8608 - val_loss: 1021.5336 - val_accuracy: 0.8570\n",
      "Epoch 166/200\n",
      "learning rate scheduled to 0.0001885569337173365\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1021.0648 - accuracy: 0.8602 - val_loss: 1020.5953 - val_accuracy: 0.8607\n",
      "Epoch 167/200\n",
      "learning rate scheduled to 0.00018667136697331444\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1020.1299 - accuracy: 0.8612 - val_loss: 1019.6843 - val_accuracy: 0.8501\n",
      "Epoch 168/200\n",
      "learning rate scheduled to 0.00018480465820175596\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1019.2085 - accuracy: 0.8602 - val_loss: 1018.7473 - val_accuracy: 0.8601\n",
      "Epoch 169/200\n",
      "learning rate scheduled to 0.000182956605713116\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1018.2930 - accuracy: 0.8618 - val_loss: 1017.8440 - val_accuracy: 0.8583\n",
      "Epoch 170/200\n",
      "learning rate scheduled to 0.00018112703663064166\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1017.3890 - accuracy: 0.8624 - val_loss: 1017.0034 - val_accuracy: 0.8251\n",
      "Epoch 171/200\n",
      "learning rate scheduled to 0.00017931576367118395\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1016.4966 - accuracy: 0.8605 - val_loss: 1016.0577 - val_accuracy: 0.8573\n",
      "Epoch 172/200\n",
      "learning rate scheduled to 0.0001775225995515939\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1015.6141 - accuracy: 0.8621 - val_loss: 1015.1648 - val_accuracy: 0.8648\n",
      "Epoch 173/200\n",
      "learning rate scheduled to 0.00017574737139511854\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1014.7390 - accuracy: 0.8616 - val_loss: 1014.3000 - val_accuracy: 0.8646\n",
      "Epoch 174/200\n",
      "learning rate scheduled to 0.00017398989191860892\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1013.8730 - accuracy: 0.8635 - val_loss: 1013.4462 - val_accuracy: 0.8633\n",
      "Epoch 175/200\n",
      "learning rate scheduled to 0.00017224998824531213\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1013.0164 - accuracy: 0.8635 - val_loss: 1012.6014 - val_accuracy: 0.8570\n",
      "Epoch 176/200\n",
      "learning rate scheduled to 0.00017052748749847522\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1012.1703 - accuracy: 0.8633 - val_loss: 1011.7854 - val_accuracy: 0.8431\n",
      "Epoch 177/200\n",
      "learning rate scheduled to 0.00016882221680134535\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1011.3341 - accuracy: 0.8631 - val_loss: 1010.9191 - val_accuracy: 0.8597\n",
      "Epoch 178/200\n",
      "learning rate scheduled to 0.00016713398887077346\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1010.5040 - accuracy: 0.8637 - val_loss: 1010.1647 - val_accuracy: 0.8229\n",
      "Epoch 179/200\n",
      "learning rate scheduled to 0.00016546264523640276\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1009.6825 - accuracy: 0.8639 - val_loss: 1009.2731 - val_accuracy: 0.8651\n",
      "Epoch 180/200\n",
      "learning rate scheduled to 0.00016380801302148028\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1008.8704 - accuracy: 0.8623 - val_loss: 1008.4714 - val_accuracy: 0.8579\n",
      "Epoch 181/200\n",
      "learning rate scheduled to 0.00016216993375564926\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 1008.0667 - accuracy: 0.8635 - val_loss: 1007.6616 - val_accuracy: 0.8666\n",
      "Epoch 182/200\n",
      "learning rate scheduled to 0.00016054823456215672\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1007.2699 - accuracy: 0.8651 - val_loss: 1006.8674 - val_accuracy: 0.8680\n",
      "Epoch 183/200\n",
      "learning rate scheduled to 0.00015894275697064586\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1006.4849 - accuracy: 0.8640 - val_loss: 1006.1127 - val_accuracy: 0.8503\n",
      "Epoch 184/200\n",
      "learning rate scheduled to 0.00015735332810436374\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1005.7095 - accuracy: 0.8639 - val_loss: 1005.3287 - val_accuracy: 0.8590\n",
      "Epoch 185/200\n",
      "learning rate scheduled to 0.00015577978949295356\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1004.9407 - accuracy: 0.8647 - val_loss: 1004.6424 - val_accuracy: 0.8169\n",
      "Epoch 186/200\n",
      "learning rate scheduled to 0.00015422199707245453\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1004.1827 - accuracy: 0.8644 - val_loss: 1003.8021 - val_accuracy: 0.8650\n",
      "Epoch 187/200\n",
      "learning rate scheduled to 0.00015267977796611375\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1003.4314 - accuracy: 0.8651 - val_loss: 1003.0555 - val_accuracy: 0.8656\n",
      "Epoch 188/200\n",
      "learning rate scheduled to 0.00015115297370357439\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 1002.6904 - accuracy: 0.8642 - val_loss: 1002.3366 - val_accuracy: 0.8546\n",
      "Epoch 189/200\n",
      "learning rate scheduled to 0.00014964144022087568\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1001.9560 - accuracy: 0.8641 - val_loss: 1001.5817 - val_accuracy: 0.8683\n",
      "Epoch 190/200\n",
      "learning rate scheduled to 0.00014814501904766075\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1001.2267 - accuracy: 0.8657 - val_loss: 1000.8625 - val_accuracy: 0.8663\n",
      "Epoch 191/200\n",
      "learning rate scheduled to 0.00014666356611996888\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 1000.5098 - accuracy: 0.8657 - val_loss: 1000.1672 - val_accuracy: 0.8555\n",
      "Epoch 192/200\n",
      "learning rate scheduled to 0.00014519693737383933\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 999.7970 - accuracy: 0.8659 - val_loss: 999.4362 - val_accuracy: 0.8681\n",
      "Epoch 193/200\n",
      "learning rate scheduled to 0.0001437449743389152\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 999.0930 - accuracy: 0.8666 - val_loss: 998.7509 - val_accuracy: 0.8621\n",
      "Epoch 194/200\n",
      "learning rate scheduled to 0.00014230751854483968\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 998.3973 - accuracy: 0.8667 - val_loss: 998.0515 - val_accuracy: 0.8675\n",
      "Epoch 195/200\n",
      "learning rate scheduled to 0.00014088444033404812\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 997.7075 - accuracy: 0.8669 - val_loss: 997.3656 - val_accuracy: 0.8641\n",
      "Epoch 196/200\n",
      "learning rate scheduled to 0.0001394755956425797\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 997.0245 - accuracy: 0.8666 - val_loss: 996.6989 - val_accuracy: 0.8563\n",
      "Epoch 197/200\n",
      "learning rate scheduled to 0.00013808084040647372\n",
      "610/610 [==============================] - 41s 62ms/step - loss: 996.3481 - accuracy: 0.8672 - val_loss: 996.0480 - val_accuracy: 0.8442\n",
      "Epoch 198/200\n",
      "learning rate scheduled to 0.00013670003056176939\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 995.6803 - accuracy: 0.8672 - val_loss: 995.3484 - val_accuracy: 0.8656\n",
      "Epoch 199/200\n",
      "learning rate scheduled to 0.000135333036450902\n",
      "610/610 [==============================] - 42s 63ms/step - loss: 995.0185 - accuracy: 0.8656 - val_loss: 994.7195 - val_accuracy: 0.8496\n",
      "Epoch 200/200\n",
      "learning rate scheduled to 0.00013397969960351474\n",
      "610/610 [==============================] - 42s 62ms/step - loss: 994.3596 - accuracy: 0.8686 - val_loss: 994.0291 - val_accuracy: 0.8730\n"
     ]
    }
   ],
   "source": [
    "history_original_siamese_model_2 = original_siamese_model_2.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                         model_checkpoint_callback, WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 7s 20ms/step - loss: 994.0361 - accuracy: 0.8654\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model_2.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Third Run - 150k Pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_150k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_150k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 105, 105, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 96, 96, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_3 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/20aij5dm\" target=\"_blank\">fearless-music-7</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 150k\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "original_siamese_model_3.compile(loss=config.loss_function,\n",
    "                                 optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_150k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "learning rate scheduled to 0.0009900000470224768\n",
      "1014/1014 [==============================] - 75s 64ms/step - loss: 1505.4133 - accuracy: 0.5258 - val_loss: 1499.3666 - val_accuracy: 0.5306\n",
      "Epoch 2/200\n",
      "learning rate scheduled to 0.000980100086890161\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1493.4252 - accuracy: 0.5599 - val_loss: 1487.4844 - val_accuracy: 0.5873\n",
      "Epoch 3/200\n",
      "learning rate scheduled to 0.0009702991275116801\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1481.6519 - accuracy: 0.5929 - val_loss: 1475.8176 - val_accuracy: 0.6028\n",
      "Epoch 4/200\n",
      "learning rate scheduled to 0.0009605961316265165\n",
      "1014/1014 [==============================] - 70s 63ms/step - loss: 1470.0846 - accuracy: 0.6192 - val_loss: 1464.3485 - val_accuracy: 0.6372\n",
      "Epoch 5/200\n",
      "learning rate scheduled to 0.0009509901772253215\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1458.7119 - accuracy: 0.6494 - val_loss: 1453.0735 - val_accuracy: 0.6653\n",
      "Epoch 6/200\n",
      "learning rate scheduled to 0.0009414802846731617\n",
      "1014/1014 [==============================] - 70s 63ms/step - loss: 1447.5256 - accuracy: 0.6886 - val_loss: 1441.9757 - val_accuracy: 0.7038\n",
      "Epoch 7/200\n",
      "learning rate scheduled to 0.0009320654743351042\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1436.5402 - accuracy: 0.7130 - val_loss: 1431.1011 - val_accuracy: 0.7315\n",
      "Epoch 8/200\n",
      "learning rate scheduled to 0.0009227448242017999\n",
      "1014/1014 [==============================] - 70s 63ms/step - loss: 1425.7623 - accuracy: 0.7277 - val_loss: 1420.4210 - val_accuracy: 0.7287\n",
      "Epoch 9/200\n",
      "learning rate scheduled to 0.0009135173546383158\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1415.1968 - accuracy: 0.7074 - val_loss: 1410.0913 - val_accuracy: 0.5153\n",
      "Epoch 10/200\n",
      "learning rate scheduled to 0.0009043822012608871\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1404.9222 - accuracy: 0.5899 - val_loss: 1399.7590 - val_accuracy: 0.6144\n",
      "Epoch 11/200\n",
      "learning rate scheduled to 0.0008953383844345808\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1394.6560 - accuracy: 0.6716 - val_loss: 1389.5535 - val_accuracy: 0.7061\n",
      "Epoch 12/200\n",
      "learning rate scheduled to 0.000886384982150048\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1384.5620 - accuracy: 0.7194 - val_loss: 1379.5623 - val_accuracy: 0.7304\n",
      "Epoch 13/200\n",
      "learning rate scheduled to 0.0008775211300235242\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1374.6671 - accuracy: 0.7367 - val_loss: 1369.7549 - val_accuracy: 0.7508\n",
      "Epoch 14/200\n",
      "learning rate scheduled to 0.0008687459060456604\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1364.9496 - accuracy: 0.7493 - val_loss: 1360.1299 - val_accuracy: 0.7624\n",
      "Epoch 15/200\n",
      "learning rate scheduled to 0.0008600584458326921\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1355.4086 - accuracy: 0.7470 - val_loss: 1350.6554 - val_accuracy: 0.7694\n",
      "Epoch 16/200\n",
      "learning rate scheduled to 0.0008514578850008547\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1346.0193 - accuracy: 0.7592 - val_loss: 1341.3500 - val_accuracy: 0.7774\n",
      "Epoch 17/200\n",
      "learning rate scheduled to 0.0008429433015407995\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1336.7831 - accuracy: 0.7762 - val_loss: 1332.2163 - val_accuracy: 0.7724\n",
      "Epoch 18/200\n",
      "learning rate scheduled to 0.0008345138886943459\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1327.7146 - accuracy: 0.7784 - val_loss: 1323.2357 - val_accuracy: 0.7709\n",
      "Epoch 19/200\n",
      "learning rate scheduled to 0.0008261687244521454\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1318.7961 - accuracy: 0.7839 - val_loss: 1314.3765 - val_accuracy: 0.7806\n",
      "Epoch 20/200\n",
      "learning rate scheduled to 0.0008179070596816018\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1310.0261 - accuracy: 0.7876 - val_loss: 1305.6996 - val_accuracy: 0.7777\n",
      "Epoch 21/200\n",
      "learning rate scheduled to 0.0008097279723733664\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1301.4045 - accuracy: 0.7911 - val_loss: 1297.1312 - val_accuracy: 0.7956\n",
      "Epoch 22/200\n",
      "learning rate scheduled to 0.000801630713394843\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1292.9265 - accuracy: 0.7915 - val_loss: 1288.7339 - val_accuracy: 0.7852\n",
      "Epoch 23/200\n",
      "learning rate scheduled to 0.0007936144183622674\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1284.5865 - accuracy: 0.7943 - val_loss: 1280.4547 - val_accuracy: 0.7957\n",
      "Epoch 24/200\n",
      "learning rate scheduled to 0.0007856782805174589\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1276.3843 - accuracy: 0.7973 - val_loss: 1272.3304 - val_accuracy: 0.7818\n",
      "Epoch 25/200\n",
      "learning rate scheduled to 0.0007778214931022376\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1268.3134 - accuracy: 0.7993 - val_loss: 1264.3097 - val_accuracy: 0.7983\n",
      "Epoch 26/200\n",
      "learning rate scheduled to 0.0007700433069840073\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1260.3750 - accuracy: 0.8030 - val_loss: 1256.4343 - val_accuracy: 0.8084\n",
      "Epoch 27/200\n",
      "learning rate scheduled to 0.0007623428577790037\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1252.5632 - accuracy: 0.8060 - val_loss: 1248.6909 - val_accuracy: 0.8033\n",
      "Epoch 28/200\n",
      "learning rate scheduled to 0.0007547194539802149\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1244.8805 - accuracy: 0.8063 - val_loss: 1241.0601 - val_accuracy: 0.8137\n",
      "Epoch 29/200\n",
      "learning rate scheduled to 0.0007471722312038764\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1237.3188 - accuracy: 0.8095 - val_loss: 1233.5634 - val_accuracy: 0.8148\n",
      "Epoch 30/200\n",
      "learning rate scheduled to 0.0007397004979429766\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1229.8794 - accuracy: 0.8118 - val_loss: 1226.1904 - val_accuracy: 0.8090\n",
      "Epoch 31/200\n",
      "learning rate scheduled to 0.0007323035050649196\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1222.5575 - accuracy: 0.8135 - val_loss: 1218.9277 - val_accuracy: 0.8156\n",
      "Epoch 32/200\n",
      "learning rate scheduled to 0.000724980445811525\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1215.3523 - accuracy: 0.8147 - val_loss: 1211.7690 - val_accuracy: 0.8229\n",
      "Epoch 33/200\n",
      "learning rate scheduled to 0.0007177306286757812\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1208.2604 - accuracy: 0.8178 - val_loss: 1204.7435 - val_accuracy: 0.8173\n",
      "Epoch 34/200\n",
      "learning rate scheduled to 0.0007105533045250923\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 1201.2836 - accuracy: 0.8182 - val_loss: 1197.8151 - val_accuracy: 0.8233\n",
      "Epoch 35/200\n",
      "learning rate scheduled to 0.0007034477818524464\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1194.4120 - accuracy: 0.8214 - val_loss: 1190.9988 - val_accuracy: 0.8272\n",
      "Epoch 36/200\n",
      "learning rate scheduled to 0.000696413311525248\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1187.6482 - accuracy: 0.8235 - val_loss: 1184.3174 - val_accuracy: 0.8077\n",
      "Epoch 37/200\n",
      "learning rate scheduled to 0.0006894492020364851\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1180.9902 - accuracy: 0.8247 - val_loss: 1177.6879 - val_accuracy: 0.8239\n",
      "Epoch 38/200\n",
      "learning rate scheduled to 0.0006825547042535618\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1174.4360 - accuracy: 0.8261 - val_loss: 1171.1760 - val_accuracy: 0.8330\n",
      "Epoch 39/200\n",
      "learning rate scheduled to 0.0006757291842950508\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1167.9865 - accuracy: 0.8265 - val_loss: 1164.7708 - val_accuracy: 0.8348\n",
      "Epoch 40/200\n",
      "learning rate scheduled to 0.0006689718930283561\n",
      "1014/1014 [==============================] - 70s 63ms/step - loss: 1161.6306 - accuracy: 0.8292 - val_loss: 1158.4866 - val_accuracy: 0.8192\n",
      "Epoch 41/200\n",
      "learning rate scheduled to 0.0006622821965720505\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1155.3746 - accuracy: 0.8303 - val_loss: 1152.2736 - val_accuracy: 0.8277\n",
      "Epoch 42/200\n",
      "learning rate scheduled to 0.0006556594034191221\n",
      "1014/1014 [==============================] - 70s 63ms/step - loss: 1149.2148 - accuracy: 0.8308 - val_loss: 1146.1521 - val_accuracy: 0.8367\n",
      "Epoch 43/200\n",
      "learning rate scheduled to 0.0006491028220625594\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 1143.1497 - accuracy: 0.8319 - val_loss: 1140.1293 - val_accuracy: 0.8363\n",
      "Epoch 44/200\n",
      "learning rate scheduled to 0.0006426118186209351\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1137.1771 - accuracy: 0.8334 - val_loss: 1134.2045 - val_accuracy: 0.8404\n",
      "Epoch 45/200\n",
      "learning rate scheduled to 0.0006361857015872375\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 1131.2914 - accuracy: 0.8353 - val_loss: 1128.3668 - val_accuracy: 0.8372\n",
      "Epoch 46/200\n",
      "learning rate scheduled to 0.0006298238370800391\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1125.4987 - accuracy: 0.8353 - val_loss: 1122.6213 - val_accuracy: 0.8338\n",
      "Epoch 47/200\n",
      "learning rate scheduled to 0.0006235255912179127\n",
      "1014/1014 [==============================] - 70s 63ms/step - loss: 1119.7894 - accuracy: 0.8377 - val_loss: 1116.9595 - val_accuracy: 0.8342\n",
      "Epoch 48/200\n",
      "learning rate scheduled to 0.000617290330119431\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1114.1694 - accuracy: 0.8379 - val_loss: 1111.3768 - val_accuracy: 0.8376\n",
      "Epoch 49/200\n",
      "learning rate scheduled to 0.0006111174199031666\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1108.6316 - accuracy: 0.8389 - val_loss: 1105.8827 - val_accuracy: 0.8381\n",
      "Epoch 50/200\n",
      "learning rate scheduled to 0.0006050062266876921\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1103.1743 - accuracy: 0.8400 - val_loss: 1100.4696 - val_accuracy: 0.8381\n",
      "Epoch 51/200\n",
      "learning rate scheduled to 0.0005989561742171646\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 1097.8022 - accuracy: 0.8408 - val_loss: 1095.1373 - val_accuracy: 0.8388\n",
      "Epoch 52/200\n",
      "learning rate scheduled to 0.0005929666286101564\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1092.5087 - accuracy: 0.8410 - val_loss: 1089.8779 - val_accuracy: 0.8436\n",
      "Epoch 53/200\n",
      "learning rate scheduled to 0.0005870369559852406\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1087.2914 - accuracy: 0.8438 - val_loss: 1084.7006 - val_accuracy: 0.8401\n",
      "Epoch 54/200\n",
      "learning rate scheduled to 0.000581166580086574\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 1082.1528 - accuracy: 0.8433 - val_loss: 1079.5991 - val_accuracy: 0.8437\n",
      "Epoch 55/200\n",
      "learning rate scheduled to 0.0005753549246583134\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1077.0869 - accuracy: 0.8444 - val_loss: 1074.5789 - val_accuracy: 0.8445\n",
      "Epoch 56/200\n",
      "learning rate scheduled to 0.0005696013558190316\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1072.0959 - accuracy: 0.8462 - val_loss: 1069.6155 - val_accuracy: 0.8493\n",
      "Epoch 57/200\n",
      "learning rate scheduled to 0.0005639053549384699\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1067.1781 - accuracy: 0.8471 - val_loss: 1064.7441 - val_accuracy: 0.8419\n",
      "Epoch 58/200\n",
      "learning rate scheduled to 0.0005582662881352008\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1062.3335 - accuracy: 0.8474 - val_loss: 1059.9283 - val_accuracy: 0.8472\n",
      "Epoch 59/200\n",
      "learning rate scheduled to 0.0005526836367789656\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1057.5551 - accuracy: 0.8488 - val_loss: 1055.1823 - val_accuracy: 0.8499\n",
      "Epoch 60/200\n",
      "learning rate scheduled to 0.0005471568246139213\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1052.8489 - accuracy: 0.8483 - val_loss: 1050.5128 - val_accuracy: 0.8487\n",
      "Epoch 61/200\n",
      "learning rate scheduled to 0.000541685275384225\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1048.2083 - accuracy: 0.8517 - val_loss: 1045.9136 - val_accuracy: 0.8432\n",
      "Epoch 62/200\n",
      "learning rate scheduled to 0.0005362684128340334\n",
      "1014/1014 [==============================] - 69s 62ms/step - loss: 1043.6349 - accuracy: 0.8526 - val_loss: 1041.3605 - val_accuracy: 0.8537\n",
      "Epoch 63/200\n",
      "learning rate scheduled to 0.0005309057183330878\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1039.1281 - accuracy: 0.8528 - val_loss: 1036.8893 - val_accuracy: 0.8552\n",
      "Epoch 64/200\n",
      "learning rate scheduled to 0.0005255966732511297\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1034.6858 - accuracy: 0.8533 - val_loss: 1032.4746 - val_accuracy: 0.8560\n",
      "Epoch 65/200\n",
      "learning rate scheduled to 0.0005203407013323158\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1030.3057 - accuracy: 0.8538 - val_loss: 1028.1185 - val_accuracy: 0.8599\n",
      "Epoch 66/200\n",
      "learning rate scheduled to 0.0005151372839463875\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1025.9873 - accuracy: 0.8548 - val_loss: 1023.8334 - val_accuracy: 0.8602\n",
      "Epoch 67/200\n",
      "learning rate scheduled to 0.0005099859024630859\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1021.7286 - accuracy: 0.8568 - val_loss: 1019.6110 - val_accuracy: 0.8590\n",
      "Epoch 68/200\n",
      "learning rate scheduled to 0.0005048860382521525\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1017.5333 - accuracy: 0.8556 - val_loss: 1015.4458 - val_accuracy: 0.8581\n",
      "Epoch 69/200\n",
      "learning rate scheduled to 0.0004998371726833284\n",
      "1014/1014 [==============================] - 70s 63ms/step - loss: 1013.3961 - accuracy: 0.8567 - val_loss: 1011.3383 - val_accuracy: 0.8582\n",
      "Epoch 70/200\n",
      "learning rate scheduled to 0.0004948387871263549\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1009.3159 - accuracy: 0.8577 - val_loss: 1007.2834 - val_accuracy: 0.8612\n",
      "Epoch 71/200\n",
      "learning rate scheduled to 0.0004898904205765575\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1005.2930 - accuracy: 0.8576 - val_loss: 1003.2931 - val_accuracy: 0.8582\n",
      "Epoch 72/200\n",
      "learning rate scheduled to 0.00048499149677809326\n",
      "1014/1014 [==============================] - 69s 63ms/step - loss: 1001.3240 - accuracy: 0.8595 - val_loss: 999.3496 - val_accuracy: 0.8641\n",
      "Epoch 73/200\n",
      "learning rate scheduled to 0.00048014158353907986\n",
      " 357/1014 [=========>....................] - ETA: 34s - loss: 998.6770 - accuracy: 0.8563"
     ]
    }
   ],
   "source": [
    "history_original_siamese_model_3 = original_siamese_model_3.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                             model_checkpoint_callback, WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, accuracy = original_siamese_model_3.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fourth Run - 30k Pairs gray"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_30k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_30k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 105, 105, 1)]     0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 96, 96, 64)        6464      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,947,648\n",
      "Trainable params: 38,947,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_4 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1972vkrr\" target=\"_blank\">easy-eon-8</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 30k - Grayscale\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "original_siamese_model_4.compile(loss=config.loss_function,\n",
    "                                 optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_150k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "learning rate scheduled to 0.0009900000470224768\n",
      "  6/203 [..............................] - ETA: 8s - loss: 1510.6082 - accuracy: 0.4905WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0220s vs `on_train_batch_end` time: 0.0229s). Check your callbacks.\n",
      "203/203 [==============================] - 12s 55ms/step - loss: 1509.4403 - accuracy: 0.5015 - val_loss: 1508.2151 - val_accuracy: 0.5015\n",
      "Epoch 2/200\n",
      "learning rate scheduled to 0.000980100086890161\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1507.0282 - accuracy: 0.5073 - val_loss: 1505.8185 - val_accuracy: 0.5021\n",
      "Epoch 3/200\n",
      "learning rate scheduled to 0.0009702991275116801\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1504.6437 - accuracy: 0.5033 - val_loss: 1503.4476 - val_accuracy: 0.4926\n",
      "Epoch 4/200\n",
      "learning rate scheduled to 0.0009605961316265165\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1502.2870 - accuracy: 0.5165 - val_loss: 1501.1044 - val_accuracy: 0.5160\n",
      "Epoch 5/200\n",
      "learning rate scheduled to 0.0009509901772253215\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1499.9572 - accuracy: 0.5246 - val_loss: 1498.7887 - val_accuracy: 0.5206\n",
      "Epoch 6/200\n",
      "learning rate scheduled to 0.0009414802846731617\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1497.6536 - accuracy: 0.5256 - val_loss: 1496.4982 - val_accuracy: 0.5207\n",
      "Epoch 7/200\n",
      "learning rate scheduled to 0.0009320654743351042\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1495.3767 - accuracy: 0.5234 - val_loss: 1494.2349 - val_accuracy: 0.5221\n",
      "Epoch 8/200\n",
      "learning rate scheduled to 0.0009227448242017999\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1493.1257 - accuracy: 0.5302 - val_loss: 1491.9957 - val_accuracy: 0.5241\n",
      "Epoch 9/200\n",
      "learning rate scheduled to 0.0009135173546383158\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1490.8995 - accuracy: 0.5215 - val_loss: 1489.7816 - val_accuracy: 0.5262\n",
      "Epoch 10/200\n",
      "learning rate scheduled to 0.0009043822012608871\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1488.6998 - accuracy: 0.5208 - val_loss: 1487.5941 - val_accuracy: 0.5273\n",
      "Epoch 11/200\n",
      "learning rate scheduled to 0.0008953383844345808\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1486.5243 - accuracy: 0.5319 - val_loss: 1485.4326 - val_accuracy: 0.5515\n",
      "Epoch 12/200\n",
      "learning rate scheduled to 0.000886384982150048\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1484.3737 - accuracy: 0.5488 - val_loss: 1483.2953 - val_accuracy: 0.5485\n",
      "Epoch 13/200\n",
      "learning rate scheduled to 0.0008775211300235242\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1482.2482 - accuracy: 0.5617 - val_loss: 1481.1794 - val_accuracy: 0.5654\n",
      "Epoch 14/200\n",
      "learning rate scheduled to 0.0008687459060456604\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1480.1456 - accuracy: 0.5658 - val_loss: 1479.0908 - val_accuracy: 0.5732\n",
      "Epoch 15/200\n",
      "learning rate scheduled to 0.0008600584458326921\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1478.0690 - accuracy: 0.5710 - val_loss: 1477.0232 - val_accuracy: 0.5843\n",
      "Epoch 16/200\n",
      "learning rate scheduled to 0.0008514578850008547\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1476.0139 - accuracy: 0.5787 - val_loss: 1474.9812 - val_accuracy: 0.5948\n",
      "Epoch 17/200\n",
      "learning rate scheduled to 0.0008429433015407995\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1473.9847 - accuracy: 0.5814 - val_loss: 1472.9683 - val_accuracy: 0.5886\n",
      "Epoch 18/200\n",
      "learning rate scheduled to 0.0008345138886943459\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1471.9773 - accuracy: 0.5859 - val_loss: 1470.9718 - val_accuracy: 0.5843\n",
      "Epoch 19/200\n",
      "learning rate scheduled to 0.0008261687244521454\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1469.9944 - accuracy: 0.5882 - val_loss: 1468.9968 - val_accuracy: 0.5858\n",
      "Epoch 20/200\n",
      "learning rate scheduled to 0.0008179070596816018\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1468.0322 - accuracy: 0.5879 - val_loss: 1467.0482 - val_accuracy: 0.5875\n",
      "Epoch 21/200\n",
      "learning rate scheduled to 0.0008097279723733664\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1466.0930 - accuracy: 0.5921 - val_loss: 1465.1215 - val_accuracy: 0.5920\n",
      "Epoch 22/200\n",
      "learning rate scheduled to 0.000801630713394843\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1464.1755 - accuracy: 0.5935 - val_loss: 1463.2141 - val_accuracy: 0.5990\n",
      "Epoch 23/200\n",
      "learning rate scheduled to 0.0007936144183622674\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1462.2808 - accuracy: 0.5947 - val_loss: 1461.3291 - val_accuracy: 0.5928\n",
      "Epoch 24/200\n",
      "learning rate scheduled to 0.0007856782805174589\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1460.4041 - accuracy: 0.6020 - val_loss: 1459.4640 - val_accuracy: 0.6037\n",
      "Epoch 25/200\n",
      "learning rate scheduled to 0.0007778214931022376\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1458.5511 - accuracy: 0.6038 - val_loss: 1457.6206 - val_accuracy: 0.5963\n",
      "Epoch 26/200\n",
      "learning rate scheduled to 0.0007700433069840073\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1456.7183 - accuracy: 0.6043 - val_loss: 1455.8013 - val_accuracy: 0.6039\n",
      "Epoch 27/200\n",
      "learning rate scheduled to 0.0007623428577790037\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1454.9061 - accuracy: 0.6104 - val_loss: 1453.9946 - val_accuracy: 0.6171\n",
      "Epoch 28/200\n",
      "learning rate scheduled to 0.0007547194539802149\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1453.1136 - accuracy: 0.6137 - val_loss: 1452.2104 - val_accuracy: 0.6254\n",
      "Epoch 29/200\n",
      "learning rate scheduled to 0.0007471722312038764\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1451.3372 - accuracy: 0.6209 - val_loss: 1450.4498 - val_accuracy: 0.6167\n",
      "Epoch 30/200\n",
      "learning rate scheduled to 0.0007397004979429766\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1449.5854 - accuracy: 0.6248 - val_loss: 1448.7058 - val_accuracy: 0.6269\n",
      "Epoch 31/200\n",
      "learning rate scheduled to 0.0007323035050649196\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1447.8516 - accuracy: 0.6313 - val_loss: 1446.9811 - val_accuracy: 0.6393\n",
      "Epoch 32/200\n",
      "learning rate scheduled to 0.000724980445811525\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1446.1365 - accuracy: 0.6344 - val_loss: 1445.2760 - val_accuracy: 0.6286\n",
      "Epoch 33/200\n",
      "learning rate scheduled to 0.0007177306286757812\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1444.4414 - accuracy: 0.6363 - val_loss: 1443.5919 - val_accuracy: 0.6365\n",
      "Epoch 34/200\n",
      "learning rate scheduled to 0.0007105533045250923\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1442.7643 - accuracy: 0.6402 - val_loss: 1441.9241 - val_accuracy: 0.6410\n",
      "Epoch 35/200\n",
      "learning rate scheduled to 0.0007034477818524464\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1441.1075 - accuracy: 0.6400 - val_loss: 1440.2775 - val_accuracy: 0.6369\n",
      "Epoch 36/200\n",
      "learning rate scheduled to 0.000696413311525248\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1439.4672 - accuracy: 0.6430 - val_loss: 1438.6465 - val_accuracy: 0.6423\n",
      "Epoch 37/200\n",
      "learning rate scheduled to 0.0006894492020364851\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1437.8459 - accuracy: 0.6474 - val_loss: 1437.0399 - val_accuracy: 0.6352\n",
      "Epoch 38/200\n",
      "learning rate scheduled to 0.0006825547042535618\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1436.2434 - accuracy: 0.6476 - val_loss: 1435.4417 - val_accuracy: 0.6444\n",
      "Epoch 39/200\n",
      "learning rate scheduled to 0.0006757291842950508\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1434.6582 - accuracy: 0.6483 - val_loss: 1433.8972 - val_accuracy: 0.6297\n",
      "Epoch 40/200\n",
      "learning rate scheduled to 0.0006689718930283561\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1433.0955 - accuracy: 0.6501 - val_loss: 1432.3058 - val_accuracy: 0.6520\n",
      "Epoch 41/200\n",
      "learning rate scheduled to 0.0006622821965720505\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1431.5420 - accuracy: 0.6542 - val_loss: 1430.7660 - val_accuracy: 0.6514\n",
      "Epoch 42/200\n",
      "learning rate scheduled to 0.0006556594034191221\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1430.0093 - accuracy: 0.6545 - val_loss: 1429.2399 - val_accuracy: 0.6621\n",
      "Epoch 43/200\n",
      "learning rate scheduled to 0.0006491028220625594\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1428.4930 - accuracy: 0.6591 - val_loss: 1427.7347 - val_accuracy: 0.6563\n",
      "Epoch 44/200\n",
      "learning rate scheduled to 0.0006426118186209351\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1426.9932 - accuracy: 0.6597 - val_loss: 1426.2444 - val_accuracy: 0.6531\n",
      "Epoch 45/200\n",
      "learning rate scheduled to 0.0006361857015872375\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1425.5115 - accuracy: 0.6606 - val_loss: 1424.7723 - val_accuracy: 0.6597\n",
      "Epoch 46/200\n",
      "learning rate scheduled to 0.0006298238370800391\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1424.0460 - accuracy: 0.6601 - val_loss: 1423.3079 - val_accuracy: 0.6599\n",
      "Epoch 47/200\n",
      "learning rate scheduled to 0.0006235255912179127\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1422.5961 - accuracy: 0.6653 - val_loss: 1421.8700 - val_accuracy: 0.6597\n",
      "Epoch 48/200\n",
      "learning rate scheduled to 0.000617290330119431\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1421.1615 - accuracy: 0.6634 - val_loss: 1420.4432 - val_accuracy: 0.6623\n",
      "Epoch 49/200\n",
      "learning rate scheduled to 0.0006111174199031666\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1419.7446 - accuracy: 0.6647 - val_loss: 1419.0321 - val_accuracy: 0.6712\n",
      "Epoch 50/200\n",
      "learning rate scheduled to 0.0006050062266876921\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1418.3424 - accuracy: 0.6675 - val_loss: 1417.6973 - val_accuracy: 0.6314\n",
      "Epoch 51/200\n",
      "learning rate scheduled to 0.0005989561742171646\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1416.9570 - accuracy: 0.6659 - val_loss: 1416.2668 - val_accuracy: 0.6638\n",
      "Epoch 52/200\n",
      "learning rate scheduled to 0.0005929666286101564\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1415.5850 - accuracy: 0.6667 - val_loss: 1414.9261 - val_accuracy: 0.6488\n",
      "Epoch 53/200\n",
      "learning rate scheduled to 0.0005870369559852406\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1414.2285 - accuracy: 0.6675 - val_loss: 1413.5513 - val_accuracy: 0.6691\n",
      "Epoch 54/200\n",
      "learning rate scheduled to 0.000581166580086574\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1412.8865 - accuracy: 0.6688 - val_loss: 1412.2461 - val_accuracy: 0.6495\n",
      "Epoch 55/200\n",
      "learning rate scheduled to 0.0005753549246583134\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1411.5610 - accuracy: 0.6682 - val_loss: 1410.8932 - val_accuracy: 0.6663\n",
      "Epoch 56/200\n",
      "learning rate scheduled to 0.0005696013558190316\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1410.2473 - accuracy: 0.6709 - val_loss: 1409.5908 - val_accuracy: 0.6595\n",
      "Epoch 57/200\n",
      "learning rate scheduled to 0.0005639053549384699\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1408.9492 - accuracy: 0.6681 - val_loss: 1408.3102 - val_accuracy: 0.6614\n",
      "Epoch 58/200\n",
      "learning rate scheduled to 0.0005582662881352008\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1407.6654 - accuracy: 0.6712 - val_loss: 1407.0249 - val_accuracy: 0.6738\n",
      "Epoch 59/200\n",
      "learning rate scheduled to 0.0005526836367789656\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1406.3962 - accuracy: 0.6699 - val_loss: 1405.7565 - val_accuracy: 0.6689\n",
      "Epoch 60/200\n",
      "learning rate scheduled to 0.0005471568246139213\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1405.1412 - accuracy: 0.6702 - val_loss: 1404.5189 - val_accuracy: 0.6618\n",
      "Epoch 61/200\n",
      "learning rate scheduled to 0.000541685275384225\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1403.8988 - accuracy: 0.6732 - val_loss: 1403.3021 - val_accuracy: 0.6521\n",
      "Epoch 62/200\n",
      "learning rate scheduled to 0.0005362684128340334\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1402.6691 - accuracy: 0.6749 - val_loss: 1402.1143 - val_accuracy: 0.6097\n",
      "Epoch 63/200\n",
      "learning rate scheduled to 0.0005309057183330878\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1401.4563 - accuracy: 0.6687 - val_loss: 1400.8491 - val_accuracy: 0.6691\n",
      "Epoch 64/200\n",
      "learning rate scheduled to 0.0005255966732511297\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1400.2512 - accuracy: 0.6763 - val_loss: 1399.7190 - val_accuracy: 0.6390\n",
      "Epoch 65/200\n",
      "learning rate scheduled to 0.0005203407013323158\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1399.0669 - accuracy: 0.6698 - val_loss: 1398.5291 - val_accuracy: 0.6103\n",
      "Epoch 66/200\n",
      "learning rate scheduled to 0.0005151372839463875\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1397.8867 - accuracy: 0.6740 - val_loss: 1397.3376 - val_accuracy: 0.6521\n",
      "Epoch 67/200\n",
      "learning rate scheduled to 0.0005099859024630859\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1396.7260 - accuracy: 0.6712 - val_loss: 1396.1422 - val_accuracy: 0.6718\n",
      "Epoch 68/200\n",
      "learning rate scheduled to 0.0005048860382521525\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1395.5723 - accuracy: 0.6765 - val_loss: 1395.0714 - val_accuracy: 0.5905\n",
      "Epoch 69/200\n",
      "learning rate scheduled to 0.0004998371726833284\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1394.4355 - accuracy: 0.6753 - val_loss: 1393.8724 - val_accuracy: 0.6704\n",
      "Epoch 70/200\n",
      "learning rate scheduled to 0.0004948387871263549\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1393.3058 - accuracy: 0.6785 - val_loss: 1392.7407 - val_accuracy: 0.6848\n",
      "Epoch 71/200\n",
      "learning rate scheduled to 0.0004898904205765575\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1392.1957 - accuracy: 0.6760 - val_loss: 1391.6606 - val_accuracy: 0.6525\n",
      "Epoch 72/200\n",
      "learning rate scheduled to 0.00048499149677809326\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1391.0951 - accuracy: 0.6742 - val_loss: 1390.5460 - val_accuracy: 0.6702\n",
      "Epoch 73/200\n",
      "learning rate scheduled to 0.00048014158353907986\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1390.0029 - accuracy: 0.6791 - val_loss: 1389.4504 - val_accuracy: 0.6812\n",
      "Epoch 74/200\n",
      "learning rate scheduled to 0.00047534016222925855\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1388.9249 - accuracy: 0.6782 - val_loss: 1388.4384 - val_accuracy: 0.6261\n",
      "Epoch 75/200\n",
      "learning rate scheduled to 0.0004705867718439549\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1387.8583 - accuracy: 0.6804 - val_loss: 1387.3459 - val_accuracy: 0.6659\n",
      "Epoch 76/200\n",
      "learning rate scheduled to 0.0004658808937529102\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1386.8043 - accuracy: 0.6773 - val_loss: 1386.3445 - val_accuracy: 0.6397\n",
      "Epoch 77/200\n",
      "learning rate scheduled to 0.0004612220957642421\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1385.7582 - accuracy: 0.6822 - val_loss: 1385.3202 - val_accuracy: 0.6286\n",
      "Epoch 78/200\n",
      "learning rate scheduled to 0.00045660988806048406\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1384.7280 - accuracy: 0.6777 - val_loss: 1384.2395 - val_accuracy: 0.6659\n",
      "Epoch 79/200\n",
      "learning rate scheduled to 0.0004520437808241695\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1383.7065 - accuracy: 0.6792 - val_loss: 1383.2064 - val_accuracy: 0.6650\n",
      "Epoch 80/200\n",
      "learning rate scheduled to 0.0004475233418634161\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1382.6965 - accuracy: 0.6774 - val_loss: 1382.2761 - val_accuracy: 0.5903\n",
      "Epoch 81/200\n",
      "learning rate scheduled to 0.0004430481101735495\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1381.7013 - accuracy: 0.6745 - val_loss: 1381.2622 - val_accuracy: 0.6386\n",
      "Epoch 82/200\n",
      "learning rate scheduled to 0.0004386176247498952\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1380.7094 - accuracy: 0.6802 - val_loss: 1380.2235 - val_accuracy: 0.6712\n",
      "Epoch 83/200\n",
      "learning rate scheduled to 0.00043423145340057087\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1379.7289 - accuracy: 0.6816 - val_loss: 1379.2933 - val_accuracy: 0.6472\n",
      "Epoch 84/200\n",
      "learning rate scheduled to 0.0004298891351209022\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1378.7628 - accuracy: 0.6776 - val_loss: 1378.2764 - val_accuracy: 0.6733\n",
      "Epoch 85/200\n",
      "learning rate scheduled to 0.00042559023771900686\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1377.8020 - accuracy: 0.6827 - val_loss: 1377.3372 - val_accuracy: 0.6748\n",
      "Epoch 86/200\n",
      "learning rate scheduled to 0.0004213343290030025\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1376.8552 - accuracy: 0.6813 - val_loss: 1376.4315 - val_accuracy: 0.6533\n",
      "Epoch 87/200\n",
      "learning rate scheduled to 0.00041712097678100687\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1375.9205 - accuracy: 0.6807 - val_loss: 1375.5498 - val_accuracy: 0.6259\n",
      "Epoch 88/200\n",
      "learning rate scheduled to 0.00041294977767392994\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1374.9927 - accuracy: 0.6811 - val_loss: 1374.5419 - val_accuracy: 0.6618\n",
      "Epoch 89/200\n",
      "learning rate scheduled to 0.0004088202706770971\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1374.0754 - accuracy: 0.6807 - val_loss: 1373.6178 - val_accuracy: 0.6746\n",
      "Epoch 90/200\n",
      "learning rate scheduled to 0.00040473208122421056\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1373.1680 - accuracy: 0.6827 - val_loss: 1372.7444 - val_accuracy: 0.6561\n",
      "Epoch 91/200\n",
      "learning rate scheduled to 0.0004006847483105957\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1372.2693 - accuracy: 0.6815 - val_loss: 1371.8822 - val_accuracy: 0.6220\n",
      "Epoch 92/200\n",
      "learning rate scheduled to 0.0003966778973699547\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1371.3813 - accuracy: 0.6835 - val_loss: 1370.9305 - val_accuracy: 0.6874\n",
      "Epoch 93/200\n",
      "learning rate scheduled to 0.0003927111250231974\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1370.5018 - accuracy: 0.6859 - val_loss: 1370.0966 - val_accuracy: 0.6484\n",
      "Epoch 94/200\n",
      "learning rate scheduled to 0.0003887840278912336\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1369.6331 - accuracy: 0.6835 - val_loss: 1369.1943 - val_accuracy: 0.6917\n",
      "Epoch 95/200\n",
      "learning rate scheduled to 0.000384896173782181\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1368.7740 - accuracy: 0.6812 - val_loss: 1368.3643 - val_accuracy: 0.6678\n",
      "Epoch 96/200\n",
      "learning rate scheduled to 0.00038104721694253384\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1367.9215 - accuracy: 0.6846 - val_loss: 1367.5090 - val_accuracy: 0.6646\n",
      "Epoch 97/200\n",
      "learning rate scheduled to 0.000377236753993202\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1367.0798 - accuracy: 0.6848 - val_loss: 1366.7057 - val_accuracy: 0.6416\n",
      "Epoch 98/200\n",
      "learning rate scheduled to 0.0003734643815550953\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1366.2479 - accuracy: 0.6847 - val_loss: 1365.8292 - val_accuracy: 0.6802\n",
      "Epoch 99/200\n",
      "learning rate scheduled to 0.0003697297250619158\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1365.4215 - accuracy: 0.6872 - val_loss: 1365.0481 - val_accuracy: 0.6591\n",
      "Epoch 100/200\n",
      "learning rate scheduled to 0.0003660324387601577\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1364.6066 - accuracy: 0.6849 - val_loss: 1364.2988 - val_accuracy: 0.5739\n",
      "Epoch 101/200\n",
      "learning rate scheduled to 0.00036237211927073074\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1363.8068 - accuracy: 0.6778 - val_loss: 1363.3947 - val_accuracy: 0.6838\n",
      "Epoch 102/200\n",
      "learning rate scheduled to 0.0003587483920273371\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1363.0020 - accuracy: 0.6837 - val_loss: 1362.5969 - val_accuracy: 0.6863\n",
      "Epoch 103/200\n",
      "learning rate scheduled to 0.0003551609112764709\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1362.2111 - accuracy: 0.6878 - val_loss: 1361.8169 - val_accuracy: 0.6934\n",
      "Epoch 104/200\n",
      "learning rate scheduled to 0.0003516093024518341\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1361.4301 - accuracy: 0.6872 - val_loss: 1361.0297 - val_accuracy: 0.6917\n",
      "Epoch 105/200\n",
      "learning rate scheduled to 0.0003480932197999209\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1360.6569 - accuracy: 0.6867 - val_loss: 1360.2600 - val_accuracy: 0.6917\n",
      "Epoch 106/200\n",
      "learning rate scheduled to 0.0003446122887544334\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1359.8911 - accuracy: 0.6890 - val_loss: 1359.5043 - val_accuracy: 0.6948\n",
      "Epoch 107/200\n",
      "learning rate scheduled to 0.00034116616356186566\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1359.1338 - accuracy: 0.6874 - val_loss: 1358.8555 - val_accuracy: 0.5754\n",
      "Epoch 108/200\n",
      "learning rate scheduled to 0.00033775449846871195\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1358.3850 - accuracy: 0.6875 - val_loss: 1358.0333 - val_accuracy: 0.6736\n",
      "Epoch 109/200\n",
      "learning rate scheduled to 0.0003343769477214664\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1357.6451 - accuracy: 0.6858 - val_loss: 1357.3302 - val_accuracy: 0.6571\n",
      "Epoch 110/200\n",
      "learning rate scheduled to 0.0003310331655666232\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1356.9111 - accuracy: 0.6876 - val_loss: 1356.5498 - val_accuracy: 0.6855\n",
      "Epoch 111/200\n",
      "learning rate scheduled to 0.00032772283506346864\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1356.1852 - accuracy: 0.6877 - val_loss: 1355.9739 - val_accuracy: 0.5997\n",
      "Epoch 112/200\n",
      "learning rate scheduled to 0.00032444561045849693\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1355.4706 - accuracy: 0.6919 - val_loss: 1355.2352 - val_accuracy: 0.5364\n",
      "Epoch 113/200\n",
      "learning rate scheduled to 0.00032120114599820226\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1354.7782 - accuracy: 0.6658 - val_loss: 1354.4050 - val_accuracy: 0.6829\n",
      "Epoch 114/200\n",
      "learning rate scheduled to 0.00031798912474187093\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1354.0563 - accuracy: 0.6908 - val_loss: 1353.7091 - val_accuracy: 0.6831\n",
      "Epoch 115/200\n",
      "learning rate scheduled to 0.0003148092297487892\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1353.3585 - accuracy: 0.6911 - val_loss: 1353.0134 - val_accuracy: 0.6914\n",
      "Epoch 116/200\n",
      "learning rate scheduled to 0.0003116611440782435\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1352.6670 - accuracy: 0.6942 - val_loss: 1352.3274 - val_accuracy: 0.6887\n",
      "Epoch 117/200\n",
      "learning rate scheduled to 0.000308544521976728\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1351.9882 - accuracy: 0.6900 - val_loss: 1351.6736 - val_accuracy: 0.6702\n",
      "Epoch 118/200\n",
      "learning rate scheduled to 0.0003054590753163211\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1351.3151 - accuracy: 0.6887 - val_loss: 1350.9966 - val_accuracy: 0.6727\n",
      "Epoch 119/200\n",
      "learning rate scheduled to 0.0003024044871563092\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1350.6497 - accuracy: 0.6894 - val_loss: 1350.3218 - val_accuracy: 0.6819\n",
      "Epoch 120/200\n",
      "learning rate scheduled to 0.00029938044055597855\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1349.9883 - accuracy: 0.6903 - val_loss: 1349.6533 - val_accuracy: 0.6983\n",
      "Epoch 121/200\n",
      "learning rate scheduled to 0.0002963866473874077\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1349.3357 - accuracy: 0.6896 - val_loss: 1349.0171 - val_accuracy: 0.6891\n",
      "Epoch 122/200\n",
      "learning rate scheduled to 0.000293422790709883\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1348.6898 - accuracy: 0.6914 - val_loss: 1348.4771 - val_accuracy: 0.6216\n",
      "Epoch 123/200\n",
      "learning rate scheduled to 0.00029048855358269063\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1348.0526 - accuracy: 0.6890 - val_loss: 1347.7946 - val_accuracy: 0.6516\n",
      "Epoch 124/200\n",
      "learning rate scheduled to 0.0002875836766907014\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1347.4198 - accuracy: 0.6894 - val_loss: 1347.1193 - val_accuracy: 0.6721\n",
      "Epoch 125/200\n",
      "learning rate scheduled to 0.0002847078430932015\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1346.7904 - accuracy: 0.6901 - val_loss: 1346.5345 - val_accuracy: 0.6574\n",
      "Epoch 126/200\n",
      "learning rate scheduled to 0.0002818607646622695\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1346.1726 - accuracy: 0.6894 - val_loss: 1345.8525 - val_accuracy: 0.6987\n",
      "Epoch 127/200\n",
      "learning rate scheduled to 0.00027904215326998385\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1345.5575 - accuracy: 0.6928 - val_loss: 1345.2643 - val_accuracy: 0.6812\n",
      "Epoch 128/200\n",
      "learning rate scheduled to 0.00027625172078842296\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1344.9504 - accuracy: 0.6938 - val_loss: 1344.6700 - val_accuracy: 0.6782\n",
      "Epoch 129/200\n",
      "learning rate scheduled to 0.0002734892079024576\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1344.3535 - accuracy: 0.6902 - val_loss: 1344.0530 - val_accuracy: 0.6878\n",
      "Epoch 130/200\n",
      "learning rate scheduled to 0.0002707543264841661\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1343.7582 - accuracy: 0.6922 - val_loss: 1343.6006 - val_accuracy: 0.6092\n",
      "Epoch 131/200\n",
      "learning rate scheduled to 0.000268046788405627\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1343.1754 - accuracy: 0.6878 - val_loss: 1343.0172 - val_accuracy: 0.5206\n",
      "Epoch 132/200\n",
      "learning rate scheduled to 0.000265366334351711\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1342.6758 - accuracy: 0.5856 - val_loss: 1342.3307 - val_accuracy: 0.6695\n",
      "Epoch 133/200\n",
      "learning rate scheduled to 0.00026271267619449646\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1342.0173 - accuracy: 0.6929 - val_loss: 1341.7295 - val_accuracy: 0.6902\n",
      "Epoch 134/200\n",
      "learning rate scheduled to 0.00026008555461885406\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1341.4459 - accuracy: 0.6927 - val_loss: 1341.2146 - val_accuracy: 0.6386\n",
      "Epoch 135/200\n",
      "learning rate scheduled to 0.00025748471030965445\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1340.8816 - accuracy: 0.6947 - val_loss: 1340.5874 - val_accuracy: 0.6982\n",
      "Epoch 136/200\n",
      "learning rate scheduled to 0.0002549098551389761\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1340.3251 - accuracy: 0.6906 - val_loss: 1340.0387 - val_accuracy: 0.6906\n",
      "Epoch 137/200\n",
      "learning rate scheduled to 0.00025236075860448183\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1339.7708 - accuracy: 0.6949 - val_loss: 1339.4993 - val_accuracy: 0.6833\n",
      "Epoch 138/200\n",
      "learning rate scheduled to 0.0002498371613910422\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1339.2234 - accuracy: 0.6943 - val_loss: 1338.9600 - val_accuracy: 0.6814\n",
      "Epoch 139/200\n",
      "learning rate scheduled to 0.0002473388041835278\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1338.6820 - accuracy: 0.6925 - val_loss: 1338.4279 - val_accuracy: 0.6827\n",
      "Epoch 140/200\n",
      "learning rate scheduled to 0.0002448654276668094\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1338.1473 - accuracy: 0.6946 - val_loss: 1337.8855 - val_accuracy: 0.6833\n",
      "Epoch 141/200\n",
      "learning rate scheduled to 0.00024241677252575755\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1337.6183 - accuracy: 0.6932 - val_loss: 1337.3556 - val_accuracy: 0.6925\n",
      "Epoch 142/200\n",
      "learning rate scheduled to 0.00023999260825803502\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1337.0948 - accuracy: 0.6930 - val_loss: 1336.8395 - val_accuracy: 0.6906\n",
      "Epoch 143/200\n",
      "learning rate scheduled to 0.00023759267554851249\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1336.5789 - accuracy: 0.6929 - val_loss: 1336.3173 - val_accuracy: 0.6859\n",
      "Epoch 144/200\n",
      "learning rate scheduled to 0.0002352167438948527\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1336.0624 - accuracy: 0.6970 - val_loss: 1335.8035 - val_accuracy: 0.6940\n",
      "Epoch 145/200\n",
      "learning rate scheduled to 0.00023286458279471843\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1335.5562 - accuracy: 0.6964 - val_loss: 1335.2974 - val_accuracy: 0.7012\n",
      "Epoch 146/200\n",
      "learning rate scheduled to 0.00023053593293298035\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1335.0557 - accuracy: 0.6934 - val_loss: 1334.7961 - val_accuracy: 0.7010\n",
      "Epoch 147/200\n",
      "learning rate scheduled to 0.0002282305782136973\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1334.5569 - accuracy: 0.6964 - val_loss: 1334.3107 - val_accuracy: 0.6982\n",
      "Epoch 148/200\n",
      "learning rate scheduled to 0.00022594827372813598\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1334.0635 - accuracy: 0.6948 - val_loss: 1333.9050 - val_accuracy: 0.6344\n",
      "Epoch 149/200\n",
      "learning rate scheduled to 0.00022368878897395915\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1333.5806 - accuracy: 0.6908 - val_loss: 1333.3330 - val_accuracy: 0.6980\n",
      "Epoch 150/200\n",
      "learning rate scheduled to 0.00022145190785522573\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1333.0942 - accuracy: 0.6950 - val_loss: 1332.8542 - val_accuracy: 0.6900\n",
      "Epoch 151/200\n",
      "learning rate scheduled to 0.00021923738546320237\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1332.6141 - accuracy: 0.6974 - val_loss: 1332.4182 - val_accuracy: 0.6631\n",
      "Epoch 152/200\n",
      "learning rate scheduled to 0.00021704500570194797\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1332.1437 - accuracy: 0.6972 - val_loss: 1331.9066 - val_accuracy: 0.6874\n",
      "Epoch 153/200\n",
      "learning rate scheduled to 0.00021487455247552135\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1331.6769 - accuracy: 0.6974 - val_loss: 1331.4821 - val_accuracy: 0.6702\n",
      "Epoch 154/200\n",
      "learning rate scheduled to 0.00021272580968798138\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1331.2152 - accuracy: 0.6964 - val_loss: 1330.9918 - val_accuracy: 0.6944\n",
      "Epoch 155/200\n",
      "learning rate scheduled to 0.00021059854683699086\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1330.7594 - accuracy: 0.6936 - val_loss: 1330.5311 - val_accuracy: 0.6982\n",
      "Epoch 156/200\n",
      "learning rate scheduled to 0.0002084925622330047\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1330.3064 - accuracy: 0.6951 - val_loss: 1330.1074 - val_accuracy: 0.6750\n",
      "Epoch 157/200\n",
      "learning rate scheduled to 0.0002064076397800818\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1329.8588 - accuracy: 0.6967 - val_loss: 1329.7164 - val_accuracy: 0.6167\n",
      "Epoch 158/200\n",
      "learning rate scheduled to 0.000204343563382281\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1329.4185 - accuracy: 0.6927 - val_loss: 1329.3256 - val_accuracy: 0.5437\n",
      "Epoch 159/200\n",
      "learning rate scheduled to 0.0002023001313500572\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1329.0001 - accuracy: 0.6718 - val_loss: 1328.7578 - val_accuracy: 0.6965\n",
      "Epoch 160/200\n",
      "learning rate scheduled to 0.0002002771275874693\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1328.5421 - accuracy: 0.6999 - val_loss: 1328.3357 - val_accuracy: 0.6872\n",
      "Epoch 161/200\n",
      "learning rate scheduled to 0.0001982743504049722\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1328.1106 - accuracy: 0.6987 - val_loss: 1327.9596 - val_accuracy: 0.6331\n",
      "Epoch 162/200\n",
      "learning rate scheduled to 0.00019629161251941696\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1327.6884 - accuracy: 0.6968 - val_loss: 1327.4810 - val_accuracy: 0.6899\n",
      "Epoch 163/200\n",
      "learning rate scheduled to 0.0001943286978348624\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1327.2654 - accuracy: 0.6964 - val_loss: 1327.0522 - val_accuracy: 0.6980\n",
      "Epoch 164/200\n",
      "learning rate scheduled to 0.00019238540466176346\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1326.8483 - accuracy: 0.6954 - val_loss: 1326.6434 - val_accuracy: 0.6914\n",
      "Epoch 165/200\n",
      "learning rate scheduled to 0.00019046154571697116\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1326.4358 - accuracy: 0.6974 - val_loss: 1326.2461 - val_accuracy: 0.6793\n",
      "Epoch 166/200\n",
      "learning rate scheduled to 0.0001885569337173365\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1326.0284 - accuracy: 0.6948 - val_loss: 1325.8148 - val_accuracy: 0.7098\n",
      "Epoch 167/200\n",
      "learning rate scheduled to 0.00018667136697331444\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1325.6211 - accuracy: 0.6995 - val_loss: 1325.4451 - val_accuracy: 0.6785\n",
      "Epoch 168/200\n",
      "learning rate scheduled to 0.00018480465820175596\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1325.2219 - accuracy: 0.6982 - val_loss: 1325.0383 - val_accuracy: 0.6846\n",
      "Epoch 169/200\n",
      "learning rate scheduled to 0.000182956605713116\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1324.8270 - accuracy: 0.6972 - val_loss: 1324.6503 - val_accuracy: 0.6712\n",
      "Epoch 170/200\n",
      "learning rate scheduled to 0.00018112703663064166\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1324.4379 - accuracy: 0.6962 - val_loss: 1324.2643 - val_accuracy: 0.6802\n",
      "Epoch 171/200\n",
      "learning rate scheduled to 0.00017931576367118395\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1324.0494 - accuracy: 0.6988 - val_loss: 1323.8518 - val_accuracy: 0.7006\n",
      "Epoch 172/200\n",
      "learning rate scheduled to 0.0001775225995515939\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1323.6650 - accuracy: 0.6973 - val_loss: 1323.4758 - val_accuracy: 0.6989\n",
      "Epoch 173/200\n",
      "learning rate scheduled to 0.00017574737139511854\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1323.2853 - accuracy: 0.6998 - val_loss: 1323.0920 - val_accuracy: 0.7055\n",
      "Epoch 174/200\n",
      "learning rate scheduled to 0.00017398989191860892\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1322.9124 - accuracy: 0.6986 - val_loss: 1322.7759 - val_accuracy: 0.6552\n",
      "Epoch 175/200\n",
      "learning rate scheduled to 0.00017224998824531213\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1322.5443 - accuracy: 0.6944 - val_loss: 1322.3671 - val_accuracy: 0.6880\n",
      "Epoch 176/200\n",
      "learning rate scheduled to 0.00017052748749847522\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1322.1733 - accuracy: 0.6969 - val_loss: 1321.9845 - val_accuracy: 0.6961\n",
      "Epoch 177/200\n",
      "learning rate scheduled to 0.00016882221680134535\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1321.8083 - accuracy: 0.6997 - val_loss: 1321.6392 - val_accuracy: 0.6883\n",
      "Epoch 178/200\n",
      "learning rate scheduled to 0.00016713398887077346\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1321.4509 - accuracy: 0.6963 - val_loss: 1321.2651 - val_accuracy: 0.6917\n",
      "Epoch 179/200\n",
      "learning rate scheduled to 0.00016546264523640276\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1321.0862 - accuracy: 0.7031 - val_loss: 1320.9282 - val_accuracy: 0.6906\n",
      "Epoch 180/200\n",
      "learning rate scheduled to 0.00016380801302148028\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1320.7360 - accuracy: 0.6974 - val_loss: 1320.5632 - val_accuracy: 0.7032\n",
      "Epoch 181/200\n",
      "learning rate scheduled to 0.00016216993375564926\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1320.3833 - accuracy: 0.7001 - val_loss: 1320.2109 - val_accuracy: 0.6948\n",
      "Epoch 182/200\n",
      "learning rate scheduled to 0.00016054823456215672\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1320.0374 - accuracy: 0.6996 - val_loss: 1319.8682 - val_accuracy: 0.7049\n",
      "Epoch 183/200\n",
      "learning rate scheduled to 0.00015894275697064586\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1319.6941 - accuracy: 0.6997 - val_loss: 1319.5293 - val_accuracy: 0.6906\n",
      "Epoch 184/200\n",
      "learning rate scheduled to 0.00015735332810436374\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1319.3564 - accuracy: 0.7008 - val_loss: 1319.1714 - val_accuracy: 0.7157\n",
      "Epoch 185/200\n",
      "learning rate scheduled to 0.00015577978949295356\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1319.0184 - accuracy: 0.7018 - val_loss: 1318.8563 - val_accuracy: 0.6914\n",
      "Epoch 186/200\n",
      "learning rate scheduled to 0.00015422199707245453\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1318.6907 - accuracy: 0.6987 - val_loss: 1318.5211 - val_accuracy: 0.6993\n",
      "Epoch 187/200\n",
      "learning rate scheduled to 0.00015267977796611375\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1318.3610 - accuracy: 0.6991 - val_loss: 1318.2012 - val_accuracy: 0.7029\n",
      "Epoch 188/200\n",
      "learning rate scheduled to 0.00015115297370357439\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1318.0366 - accuracy: 0.7005 - val_loss: 1317.8999 - val_accuracy: 0.6774\n",
      "Epoch 189/200\n",
      "learning rate scheduled to 0.00014964144022087568\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1317.7140 - accuracy: 0.7016 - val_loss: 1317.5558 - val_accuracy: 0.6974\n",
      "Epoch 190/200\n",
      "learning rate scheduled to 0.00014814501904766075\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1317.3983 - accuracy: 0.6965 - val_loss: 1317.2343 - val_accuracy: 0.7034\n",
      "Epoch 191/200\n",
      "learning rate scheduled to 0.00014666356611996888\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1317.0825 - accuracy: 0.7013 - val_loss: 1316.9268 - val_accuracy: 0.7059\n",
      "Epoch 192/200\n",
      "learning rate scheduled to 0.00014519693737383933\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1316.7694 - accuracy: 0.7028 - val_loss: 1316.6338 - val_accuracy: 0.6861\n",
      "Epoch 193/200\n",
      "learning rate scheduled to 0.0001437449743389152\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1316.4625 - accuracy: 0.6995 - val_loss: 1316.3761 - val_accuracy: 0.6640\n",
      "Epoch 194/200\n",
      "learning rate scheduled to 0.00014230751854483968\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1316.1606 - accuracy: 0.6972 - val_loss: 1316.0061 - val_accuracy: 0.6914\n",
      "Epoch 195/200\n",
      "learning rate scheduled to 0.00014088444033404812\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1315.8536 - accuracy: 0.7022 - val_loss: 1315.7009 - val_accuracy: 0.7053\n",
      "Epoch 196/200\n",
      "learning rate scheduled to 0.0001394755956425797\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1315.5541 - accuracy: 0.7031 - val_loss: 1315.4050 - val_accuracy: 0.7010\n",
      "Epoch 197/200\n",
      "learning rate scheduled to 0.00013808084040647372\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1315.2576 - accuracy: 0.7015 - val_loss: 1315.1155 - val_accuracy: 0.6970\n",
      "Epoch 198/200\n",
      "learning rate scheduled to 0.00013670003056176939\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1314.9650 - accuracy: 0.6997 - val_loss: 1314.8246 - val_accuracy: 0.6949\n",
      "Epoch 199/200\n",
      "learning rate scheduled to 0.000135333036450902\n",
      "203/203 [==============================] - 11s 52ms/step - loss: 1314.6730 - accuracy: 0.7024 - val_loss: 1314.5323 - val_accuracy: 0.6985\n",
      "Epoch 200/200\n",
      "learning rate scheduled to 0.00013397969960351474\n",
      "203/203 [==============================] - 11s 53ms/step - loss: 1314.3854 - accuracy: 0.7013 - val_loss: 1314.2648 - val_accuracy: 0.6778\n"
     ]
    }
   ],
   "source": [
    "history_original_siamese_model_4 = original_siamese_model_4.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                             model_checkpoint_callback, WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 2s 18ms/step - loss: 1314.2748 - accuracy: 0.6682\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model_4.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fifth Run - 90K Pairs gray"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_90k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_90k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_5 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 90k - Grayscale\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_siamese_model_5.compile(loss=config.loss_function,\n",
    "                                 optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, accuracy = original_siamese_model_5.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Paper: Siamese Neural Networks for One-shot Image Recognition\n",
    "http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Images sizes: 40x40 / 75x75 / 105x105 / 120x120 / 150x150\n",
    "Images in RGB / Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow_addons as tfa\n",
    "import math\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from helper_functions import plot_training\n",
    "from helper_functions import create_tf_data_datasets_contrastive\n",
    "from helper_functions import create_tf_data_testset_contrastive\n",
    "from helper_functions import get_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Original Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## First Run - 30k Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_30k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_30k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def get_original_model(height, width, channels):\n",
    "\n",
    "    input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "    x = keras.layers.Conv2D(64, (10,10), activation=\"relu\",\n",
    "                            kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv1')(input)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(128, (7,7), activation=\"relu\",\n",
    "                            kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                            kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv2')(x)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(128, (4,4), activation=\"relu\",\n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv3')(x)\n",
    "    x = keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(256, (4,4), activation=\"relu\",\n",
    "                        kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                        kernel_regularizer=keras.regularizers.l2(2e-4), name='Conv4')(x)\n",
    "\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    output = keras.layers.Dense(4096, activation=\"sigmoid\", kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.2),\n",
    "                           bias_initializer=keras.initializers.RandomNormal(mean=0, stddev=0.01),\n",
    "                           kernel_regularizer=keras.regularizers.l2(1e-3), name='Dense1')(x)\n",
    "\n",
    "    model = keras.models.Model(input, output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "original_model = get_original_model(height,width,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 105, 105, 3)]     0         \n_________________________________________________________________\nConv1 (Conv2D)               (None, 96, 96, 64)        19264     \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 48, 48, 64)        0         \n_________________________________________________________________\nConv2 (Conv2D)               (None, 42, 42, 128)       401536    \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 21, 21, 128)       0         \n_________________________________________________________________\nConv3 (Conv2D)               (None, 18, 18, 128)       262272    \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 9, 9, 128)         0         \n_________________________________________________________________\nConv4 (Conv2D)               (None, 6, 6, 256)         524544    \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 9216)              0         \n_________________________________________________________________\nDense1 (Dense)               (None, 4096)              37752832  \n=================================================================\nTotal params: 38,960,448\nTrainable params: 38,960,448\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 30k\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "original_siamese_model.compile(loss=config.loss_function,\n",
    "                               optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_30k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/200\nlearning rate scheduled to 0.0009900000470224768\n  6/166 [>.............................] - ETA: 9s - loss: 1510.7972 - accuracy: 0.4883WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0265s vs `on_train_batch_end` time: 0.0313s). Check your callbacks.\n166/166 [==============================] - 14s 73ms/step - loss: 1509.8461 - accuracy: 0.5195 - val_loss: 1508.8473 - val_accuracy: 0.5477\nEpoch 2/200\nlearning rate scheduled to 0.000980100086890161\n166/166 [==============================] - 13s 72ms/step - loss: 1507.8730 - accuracy: 0.5517 - val_loss: 1506.8850 - val_accuracy: 0.5713\nEpoch 3/200\nlearning rate scheduled to 0.0009702991275116801\n166/166 [==============================] - 13s 72ms/step - loss: 1505.9220 - accuracy: 0.5641 - val_loss: 1504.9448 - val_accuracy: 0.5760\nEpoch 4/200\nlearning rate scheduled to 0.0009605961316265165\n166/166 [==============================] - 13s 72ms/step - loss: 1503.9926 - accuracy: 0.5570 - val_loss: 1503.0267 - val_accuracy: 0.5584\nEpoch 5/200\nlearning rate scheduled to 0.0009509901772253215\n166/166 [==============================] - 13s 72ms/step - loss: 1502.0851 - accuracy: 0.5658 - val_loss: 1501.1294 - val_accuracy: 0.5626\nEpoch 6/200\nlearning rate scheduled to 0.0009414802846731617\n166/166 [==============================] - 13s 72ms/step - loss: 1500.1985 - accuracy: 0.5495 - val_loss: 1499.2531 - val_accuracy: 0.5622\nEpoch 7/200\nlearning rate scheduled to 0.0009320654743351042\n166/166 [==============================] - 13s 72ms/step - loss: 1498.3329 - accuracy: 0.5498 - val_loss: 1497.3988 - val_accuracy: 0.5624\nEpoch 8/200\nlearning rate scheduled to 0.0009227448242017999\n166/166 [==============================] - 13s 73ms/step - loss: 1496.4883 - accuracy: 0.5533 - val_loss: 1495.5640 - val_accuracy: 0.5464\nEpoch 9/200\nlearning rate scheduled to 0.0009135173546383158\n166/166 [==============================] - 13s 71ms/step - loss: 1494.6643 - accuracy: 0.5528 - val_loss: 1493.7501 - val_accuracy: 0.5502\nEpoch 10/200\nlearning rate scheduled to 0.0009043822012608871\n166/166 [==============================] - 13s 72ms/step - loss: 1492.8597 - accuracy: 0.5554 - val_loss: 1491.9556 - val_accuracy: 0.5545\nEpoch 11/200\nlearning rate scheduled to 0.0008953383844345808\n166/166 [==============================] - 13s 72ms/step - loss: 1491.0754 - accuracy: 0.5547 - val_loss: 1490.1818 - val_accuracy: 0.5584\nEpoch 12/200\nlearning rate scheduled to 0.000886384982150048\n166/166 [==============================] - 13s 72ms/step - loss: 1489.3113 - accuracy: 0.5561 - val_loss: 1488.4277 - val_accuracy: 0.5515\nEpoch 13/200\nlearning rate scheduled to 0.0008775211300235242\n166/166 [==============================] - 13s 73ms/step - loss: 1487.5659 - accuracy: 0.5630 - val_loss: 1486.6920 - val_accuracy: 0.5588\nEpoch 14/200\nlearning rate scheduled to 0.0008687459060456604\n166/166 [==============================] - 13s 72ms/step - loss: 1485.8402 - accuracy: 0.5646 - val_loss: 1484.9774 - val_accuracy: 0.5607\nEpoch 15/200\nlearning rate scheduled to 0.0008600584458326921\n166/166 [==============================] - 13s 72ms/step - loss: 1484.1348 - accuracy: 0.5679 - val_loss: 1483.2804 - val_accuracy: 0.5683\nEpoch 16/200\nlearning rate scheduled to 0.0008514578850008547\n166/166 [==============================] - 13s 72ms/step - loss: 1482.4471 - accuracy: 0.5776 - val_loss: 1481.6041 - val_accuracy: 0.5833\nEpoch 17/200\nlearning rate scheduled to 0.0008429433015407995\n166/166 [==============================] - 13s 72ms/step - loss: 1480.7802 - accuracy: 0.5809 - val_loss: 1479.9431 - val_accuracy: 0.5924\nEpoch 18/200\nlearning rate scheduled to 0.0008345138886943459\n166/166 [==============================] - 13s 72ms/step - loss: 1479.1293 - accuracy: 0.5868 - val_loss: 1478.3011 - val_accuracy: 0.5958\nEpoch 19/200\nlearning rate scheduled to 0.0008261687244521454\n166/166 [==============================] - 13s 72ms/step - loss: 1477.4977 - accuracy: 0.5922 - val_loss: 1476.6799 - val_accuracy: 0.5952\nEpoch 20/200\nlearning rate scheduled to 0.0008179070596816018\n166/166 [==============================] - 13s 72ms/step - loss: 1475.8861 - accuracy: 0.5899 - val_loss: 1475.0784 - val_accuracy: 0.5873\nEpoch 21/200\nlearning rate scheduled to 0.0008097279723733664\n166/166 [==============================] - 13s 72ms/step - loss: 1474.2894 - accuracy: 0.5968 - val_loss: 1473.4921 - val_accuracy: 0.6029\nEpoch 22/200\nlearning rate scheduled to 0.000801630713394843\n166/166 [==============================] - 13s 72ms/step - loss: 1472.7108 - accuracy: 0.6010 - val_loss: 1471.9152 - val_accuracy: 0.6082\nEpoch 23/200\nlearning rate scheduled to 0.0007936144183622674\n166/166 [==============================] - 13s 73ms/step - loss: 1471.1504 - accuracy: 0.6053 - val_loss: 1470.3665 - val_accuracy: 0.6078\nEpoch 24/200\nlearning rate scheduled to 0.0007856782805174589\n166/166 [==============================] - 13s 72ms/step - loss: 1469.6035 - accuracy: 0.6153 - val_loss: 1468.8282 - val_accuracy: 0.6186\nEpoch 25/200\nlearning rate scheduled to 0.0007778214931022376\n166/166 [==============================] - 13s 72ms/step - loss: 1468.0759 - accuracy: 0.6203 - val_loss: 1467.3094 - val_accuracy: 0.6288\nEpoch 26/200\nlearning rate scheduled to 0.0007700433069840073\n166/166 [==============================] - 13s 73ms/step - loss: 1466.5646 - accuracy: 0.6254 - val_loss: 1465.8110 - val_accuracy: 0.6192\nEpoch 27/200\nlearning rate scheduled to 0.0007623428577790037\n166/166 [==============================] - 13s 72ms/step - loss: 1465.0709 - accuracy: 0.6268 - val_loss: 1464.3197 - val_accuracy: 0.6376\nEpoch 28/200\nlearning rate scheduled to 0.0007547194539802149\n166/166 [==============================] - 13s 72ms/step - loss: 1463.5918 - accuracy: 0.6337 - val_loss: 1462.8486 - val_accuracy: 0.6456\nEpoch 29/200\nlearning rate scheduled to 0.0007471722312038764\n166/166 [==============================] - 13s 72ms/step - loss: 1462.1288 - accuracy: 0.6404 - val_loss: 1461.3939 - val_accuracy: 0.6467\nEpoch 30/200\nlearning rate scheduled to 0.0007397004979429766\n166/166 [==============================] - 13s 72ms/step - loss: 1460.6816 - accuracy: 0.6467 - val_loss: 1459.9513 - val_accuracy: 0.6544\nEpoch 31/200\nlearning rate scheduled to 0.0007323035050649196\n166/166 [==============================] - 13s 72ms/step - loss: 1459.2504 - accuracy: 0.6517 - val_loss: 1458.5309 - val_accuracy: 0.6552\nEpoch 32/200\nlearning rate scheduled to 0.000724980445811525\n166/166 [==============================] - 13s 72ms/step - loss: 1457.8333 - accuracy: 0.6589 - val_loss: 1457.1237 - val_accuracy: 0.6621\nEpoch 33/200\nlearning rate scheduled to 0.0007177306286757812\n166/166 [==============================] - 13s 72ms/step - loss: 1456.4297 - accuracy: 0.6648 - val_loss: 1455.7256 - val_accuracy: 0.6699\nEpoch 34/200\nlearning rate scheduled to 0.0007105533045250923\n166/166 [==============================] - 13s 72ms/step - loss: 1455.0441 - accuracy: 0.6740 - val_loss: 1454.3439 - val_accuracy: 0.6876\nEpoch 35/200\nlearning rate scheduled to 0.0007034477818524464\n166/166 [==============================] - 13s 72ms/step - loss: 1453.6680 - accuracy: 0.6859 - val_loss: 1452.9767 - val_accuracy: 0.6844\nEpoch 36/200\nlearning rate scheduled to 0.000696413311525248\n166/166 [==============================] - 13s 72ms/step - loss: 1452.3135 - accuracy: 0.6890 - val_loss: 1451.6378 - val_accuracy: 0.6863\nEpoch 37/200\nlearning rate scheduled to 0.0006894492020364851\n166/166 [==============================] - 13s 72ms/step - loss: 1450.9730 - accuracy: 0.6921 - val_loss: 1450.2922 - val_accuracy: 0.7014\nEpoch 38/200\nlearning rate scheduled to 0.0006825547042535618\n166/166 [==============================] - 13s 71ms/step - loss: 1449.6477 - accuracy: 0.7002 - val_loss: 1448.9771 - val_accuracy: 0.6989\nEpoch 39/200\nlearning rate scheduled to 0.0006757291842950508\n166/166 [==============================] - 13s 72ms/step - loss: 1448.3364 - accuracy: 0.7056 - val_loss: 1447.6836 - val_accuracy: 0.7108\nEpoch 40/200\nlearning rate scheduled to 0.0006689718930283561\n166/166 [==============================] - 13s 72ms/step - loss: 1447.0380 - accuracy: 0.7104 - val_loss: 1446.3938 - val_accuracy: 0.7053\nEpoch 41/200\nlearning rate scheduled to 0.0006622821965720505\n166/166 [==============================] - 13s 72ms/step - loss: 1445.7533 - accuracy: 0.7161 - val_loss: 1445.1145 - val_accuracy: 0.7155\nEpoch 42/200\nlearning rate scheduled to 0.0006556594034191221\n166/166 [==============================] - 13s 72ms/step - loss: 1444.4882 - accuracy: 0.7174 - val_loss: 1443.8448 - val_accuracy: 0.7257\nEpoch 43/200\nlearning rate scheduled to 0.0006491028220625594\n166/166 [==============================] - 13s 72ms/step - loss: 1443.2343 - accuracy: 0.7187 - val_loss: 1442.6505 - val_accuracy: 0.6897\nEpoch 44/200\nlearning rate scheduled to 0.0006426118186209351\n166/166 [==============================] - 13s 72ms/step - loss: 1441.9928 - accuracy: 0.7215 - val_loss: 1441.3646 - val_accuracy: 0.7362\nEpoch 45/200\nlearning rate scheduled to 0.0006361857015872375\n166/166 [==============================] - 13s 72ms/step - loss: 1440.7684 - accuracy: 0.7241 - val_loss: 1440.1431 - val_accuracy: 0.7342\nEpoch 46/200\nlearning rate scheduled to 0.0006298238370800391\n166/166 [==============================] - 13s 72ms/step - loss: 1439.5547 - accuracy: 0.7274 - val_loss: 1438.9360 - val_accuracy: 0.7400\nEpoch 47/200\nlearning rate scheduled to 0.0006235255912179127\n166/166 [==============================] - 13s 72ms/step - loss: 1438.3555 - accuracy: 0.7256 - val_loss: 1437.7771 - val_accuracy: 0.7172\nEpoch 48/200\nlearning rate scheduled to 0.000617290330119431\n166/166 [==============================] - 13s 72ms/step - loss: 1437.1694 - accuracy: 0.7277 - val_loss: 1436.5664 - val_accuracy: 0.7327\nEpoch 49/200\nlearning rate scheduled to 0.0006111174199031666\n166/166 [==============================] - 13s 71ms/step - loss: 1435.9958 - accuracy: 0.7281 - val_loss: 1435.4087 - val_accuracy: 0.7308\nEpoch 50/200\nlearning rate scheduled to 0.0006050062266876921\n166/166 [==============================] - 13s 71ms/step - loss: 1434.8298 - accuracy: 0.7327 - val_loss: 1434.2590 - val_accuracy: 0.7342\nEpoch 51/200\nlearning rate scheduled to 0.0005989561742171646\n166/166 [==============================] - 13s 72ms/step - loss: 1433.6853 - accuracy: 0.7318 - val_loss: 1433.0975 - val_accuracy: 0.7396\nEpoch 52/200\nlearning rate scheduled to 0.0005929666286101564\n166/166 [==============================] - 13s 72ms/step - loss: 1432.5488 - accuracy: 0.7328 - val_loss: 1431.9653 - val_accuracy: 0.7447\nEpoch 53/200\nlearning rate scheduled to 0.0005870369559852406\n166/166 [==============================] - 13s 72ms/step - loss: 1431.4260 - accuracy: 0.7350 - val_loss: 1430.8665 - val_accuracy: 0.7344\nEpoch 54/200\nlearning rate scheduled to 0.000581166580086574\n166/166 [==============================] - 13s 72ms/step - loss: 1430.3159 - accuracy: 0.7318 - val_loss: 1429.7407 - val_accuracy: 0.7538\nEpoch 55/200\nlearning rate scheduled to 0.0005753549246583134\n166/166 [==============================] - 13s 72ms/step - loss: 1429.2163 - accuracy: 0.7354 - val_loss: 1428.6726 - val_accuracy: 0.7347\nEpoch 56/200\nlearning rate scheduled to 0.0005696013558190316\n166/166 [==============================] - 13s 72ms/step - loss: 1428.1266 - accuracy: 0.7423 - val_loss: 1427.5834 - val_accuracy: 0.7464\nEpoch 57/200\nlearning rate scheduled to 0.0005639053549384699\n166/166 [==============================] - 13s 72ms/step - loss: 1427.0515 - accuracy: 0.7421 - val_loss: 1426.5179 - val_accuracy: 0.7425\nEpoch 58/200\nlearning rate scheduled to 0.0005582662881352008\n166/166 [==============================] - 13s 72ms/step - loss: 1425.9874 - accuracy: 0.7412 - val_loss: 1425.4668 - val_accuracy: 0.7234\nEpoch 59/200\nlearning rate scheduled to 0.0005526836367789656\n166/166 [==============================] - 13s 72ms/step - loss: 1424.9360 - accuracy: 0.7407 - val_loss: 1424.4032 - val_accuracy: 0.7410\nEpoch 60/200\nlearning rate scheduled to 0.0005471568246139213\n166/166 [==============================] - 13s 72ms/step - loss: 1423.8971 - accuracy: 0.7408 - val_loss: 1423.3633 - val_accuracy: 0.7459\nEpoch 61/200\nlearning rate scheduled to 0.000541685275384225\n166/166 [==============================] - 13s 72ms/step - loss: 1422.8647 - accuracy: 0.7434 - val_loss: 1422.3595 - val_accuracy: 0.7291\nEpoch 62/200\nlearning rate scheduled to 0.0005362684128340334\n166/166 [==============================] - 13s 72ms/step - loss: 1421.8483 - accuracy: 0.7408 - val_loss: 1421.3300 - val_accuracy: 0.7508\nEpoch 63/200\nlearning rate scheduled to 0.0005309057183330878\n166/166 [==============================] - 13s 72ms/step - loss: 1420.8390 - accuracy: 0.7424 - val_loss: 1420.3521 - val_accuracy: 0.7377\nEpoch 64/200\nlearning rate scheduled to 0.0005255966732511297\n166/166 [==============================] - 13s 73ms/step - loss: 1419.8370 - accuracy: 0.7483 - val_loss: 1419.3491 - val_accuracy: 0.7438\nEpoch 65/200\nlearning rate scheduled to 0.0005203407013323158\n166/166 [==============================] - 13s 72ms/step - loss: 1418.8553 - accuracy: 0.7457 - val_loss: 1418.3788 - val_accuracy: 0.7191\nEpoch 66/200\nlearning rate scheduled to 0.0005151372839463875\n166/166 [==============================] - 13s 71ms/step - loss: 1417.8779 - accuracy: 0.7455 - val_loss: 1417.3843 - val_accuracy: 0.7532\nEpoch 67/200\nlearning rate scheduled to 0.0005099859024630859\n166/166 [==============================] - 13s 72ms/step - loss: 1416.9117 - accuracy: 0.7475 - val_loss: 1416.4457 - val_accuracy: 0.7438\nEpoch 68/200\nlearning rate scheduled to 0.0005048860382521525\n166/166 [==============================] - 13s 72ms/step - loss: 1415.9556 - accuracy: 0.7497 - val_loss: 1415.4779 - val_accuracy: 0.7517\nEpoch 69/200\nlearning rate scheduled to 0.0004998371726833284\n166/166 [==============================] - 13s 72ms/step - loss: 1415.0114 - accuracy: 0.7495 - val_loss: 1414.5400 - val_accuracy: 0.7400\nEpoch 70/200\nlearning rate scheduled to 0.0004948387871263549\n166/166 [==============================] - 13s 72ms/step - loss: 1414.0732 - accuracy: 0.7536 - val_loss: 1413.6356 - val_accuracy: 0.7251\nEpoch 71/200\nlearning rate scheduled to 0.0004898904205765575\n166/166 [==============================] - 13s 73ms/step - loss: 1413.1494 - accuracy: 0.7513 - val_loss: 1412.6749 - val_accuracy: 0.7611\nEpoch 72/200\nlearning rate scheduled to 0.00048499149677809326\n166/166 [==============================] - 13s 72ms/step - loss: 1412.2343 - accuracy: 0.7544 - val_loss: 1411.7728 - val_accuracy: 0.7607\nEpoch 73/200\nlearning rate scheduled to 0.00048014158353907986\n166/166 [==============================] - 13s 72ms/step - loss: 1411.3280 - accuracy: 0.7558 - val_loss: 1410.8782 - val_accuracy: 0.7492\nEpoch 74/200\nlearning rate scheduled to 0.00047534016222925855\n166/166 [==============================] - 13s 71ms/step - loss: 1410.4314 - accuracy: 0.7547 - val_loss: 1409.9823 - val_accuracy: 0.7577\nEpoch 75/200\nlearning rate scheduled to 0.0004705867718439549\n166/166 [==============================] - 13s 72ms/step - loss: 1409.5490 - accuracy: 0.7557 - val_loss: 1409.1305 - val_accuracy: 0.7315\nEpoch 76/200\nlearning rate scheduled to 0.0004658808937529102\n166/166 [==============================] - 13s 72ms/step - loss: 1408.6715 - accuracy: 0.7545 - val_loss: 1408.2053 - val_accuracy: 0.7790\nEpoch 77/200\nlearning rate scheduled to 0.0004612220957642421\n166/166 [==============================] - 13s 72ms/step - loss: 1407.8057 - accuracy: 0.7561 - val_loss: 1407.3727 - val_accuracy: 0.7583\nEpoch 78/200\nlearning rate scheduled to 0.00045660988806048406\n166/166 [==============================] - 13s 72ms/step - loss: 1406.9438 - accuracy: 0.7565 - val_loss: 1406.4998 - val_accuracy: 0.7683\nEpoch 79/200\nlearning rate scheduled to 0.0004520437808241695\n166/166 [==============================] - 13s 72ms/step - loss: 1406.0945 - accuracy: 0.7591 - val_loss: 1405.6794 - val_accuracy: 0.7623\nEpoch 80/200\nlearning rate scheduled to 0.0004475233418634161\n166/166 [==============================] - 13s 71ms/step - loss: 1405.2565 - accuracy: 0.7582 - val_loss: 1404.8359 - val_accuracy: 0.7643\nEpoch 81/200\nlearning rate scheduled to 0.0004430481101735495\n166/166 [==============================] - 13s 72ms/step - loss: 1404.4224 - accuracy: 0.7592 - val_loss: 1403.9932 - val_accuracy: 0.7730\nEpoch 82/200\nlearning rate scheduled to 0.0004386176247498952\n166/166 [==============================] - 13s 72ms/step - loss: 1403.6021 - accuracy: 0.7604 - val_loss: 1403.1704 - val_accuracy: 0.7753\nEpoch 83/200\nlearning rate scheduled to 0.00043423145340057087\n166/166 [==============================] - 13s 72ms/step - loss: 1402.7836 - accuracy: 0.7619 - val_loss: 1402.3737 - val_accuracy: 0.7700\nEpoch 84/200\nlearning rate scheduled to 0.0004298891351209022\n166/166 [==============================] - 13s 72ms/step - loss: 1401.9827 - accuracy: 0.7611 - val_loss: 1401.5635 - val_accuracy: 0.7696\nEpoch 85/200\nlearning rate scheduled to 0.00042559023771900686\n166/166 [==============================] - 13s 72ms/step - loss: 1401.1790 - accuracy: 0.7664 - val_loss: 1400.7910 - val_accuracy: 0.7609\nEpoch 86/200\nlearning rate scheduled to 0.0004213343290030025\n166/166 [==============================] - 13s 71ms/step - loss: 1400.3965 - accuracy: 0.7624 - val_loss: 1399.9956 - val_accuracy: 0.7675\nEpoch 87/200\nlearning rate scheduled to 0.00041712097678100687\n166/166 [==============================] - 13s 72ms/step - loss: 1399.6107 - accuracy: 0.7648 - val_loss: 1399.2184 - val_accuracy: 0.7747\nEpoch 88/200\nlearning rate scheduled to 0.00041294977767392994\n166/166 [==============================] - 13s 72ms/step - loss: 1398.8433 - accuracy: 0.7670 - val_loss: 1398.4529 - val_accuracy: 0.7698\nEpoch 89/200\nlearning rate scheduled to 0.0004088202706770971\n166/166 [==============================] - 13s 72ms/step - loss: 1398.0789 - accuracy: 0.7658 - val_loss: 1397.7241 - val_accuracy: 0.7494\nEpoch 90/200\nlearning rate scheduled to 0.00040473208122421056\n166/166 [==============================] - 13s 72ms/step - loss: 1397.3246 - accuracy: 0.7650 - val_loss: 1396.9454 - val_accuracy: 0.7734\nEpoch 91/200\nlearning rate scheduled to 0.0004006847483105957\n166/166 [==============================] - 13s 72ms/step - loss: 1396.5729 - accuracy: 0.7716 - val_loss: 1396.1936 - val_accuracy: 0.7666\nEpoch 92/200\nlearning rate scheduled to 0.0003966778973699547\n166/166 [==============================] - 13s 72ms/step - loss: 1395.8354 - accuracy: 0.7674 - val_loss: 1395.4524 - val_accuracy: 0.7773\nEpoch 93/200\nlearning rate scheduled to 0.0003927111250231974\n166/166 [==============================] - 13s 72ms/step - loss: 1395.1027 - accuracy: 0.7700 - val_loss: 1394.7374 - val_accuracy: 0.7790\nEpoch 94/200\nlearning rate scheduled to 0.0003887840278912336\n166/166 [==============================] - 13s 72ms/step - loss: 1394.3784 - accuracy: 0.7726 - val_loss: 1394.0200 - val_accuracy: 0.7598\nEpoch 95/200\nlearning rate scheduled to 0.000384896173782181\n166/166 [==============================] - 13s 72ms/step - loss: 1393.6625 - accuracy: 0.7720 - val_loss: 1393.2960 - val_accuracy: 0.7711\nEpoch 96/200\nlearning rate scheduled to 0.00038104721694253384\n166/166 [==============================] - 13s 72ms/step - loss: 1392.9540 - accuracy: 0.7690 - val_loss: 1392.5890 - val_accuracy: 0.7787\nEpoch 97/200\nlearning rate scheduled to 0.000377236753993202\n166/166 [==============================] - 13s 72ms/step - loss: 1392.2524 - accuracy: 0.7715 - val_loss: 1391.8923 - val_accuracy: 0.7755\nEpoch 98/200\nlearning rate scheduled to 0.0003734643815550953\n166/166 [==============================] - 13s 72ms/step - loss: 1391.5518 - accuracy: 0.7760 - val_loss: 1391.2009 - val_accuracy: 0.7766\nEpoch 99/200\nlearning rate scheduled to 0.0003697297250619158\n166/166 [==============================] - 13s 72ms/step - loss: 1390.8663 - accuracy: 0.7759 - val_loss: 1390.5237 - val_accuracy: 0.7787\nEpoch 100/200\nlearning rate scheduled to 0.0003660324387601577\n166/166 [==============================] - 13s 72ms/step - loss: 1390.1864 - accuracy: 0.7729 - val_loss: 1389.8613 - val_accuracy: 0.7704\nEpoch 101/200\nlearning rate scheduled to 0.00036237211927073074\n166/166 [==============================] - 13s 72ms/step - loss: 1389.5144 - accuracy: 0.7772 - val_loss: 1389.1753 - val_accuracy: 0.7813\nEpoch 102/200\nlearning rate scheduled to 0.0003587483920273371\n166/166 [==============================] - 13s 72ms/step - loss: 1388.8490 - accuracy: 0.7781 - val_loss: 1388.5099 - val_accuracy: 0.7868\nEpoch 103/200\nlearning rate scheduled to 0.0003551609112764709\n166/166 [==============================] - 13s 72ms/step - loss: 1388.1910 - accuracy: 0.7759 - val_loss: 1387.8534 - val_accuracy: 0.7824\nEpoch 104/200\nlearning rate scheduled to 0.0003516093024518341\n166/166 [==============================] - 13s 72ms/step - loss: 1387.5383 - accuracy: 0.7773 - val_loss: 1387.2227 - val_accuracy: 0.7724\nEpoch 105/200\nlearning rate scheduled to 0.0003480932197999209\n166/166 [==============================] - 13s 72ms/step - loss: 1386.8956 - accuracy: 0.7777 - val_loss: 1386.5726 - val_accuracy: 0.7700\nEpoch 106/200\nlearning rate scheduled to 0.0003446122887544334\n166/166 [==============================] - 13s 72ms/step - loss: 1386.2585 - accuracy: 0.7760 - val_loss: 1385.9261 - val_accuracy: 0.7930\nEpoch 107/200\nlearning rate scheduled to 0.00034116616356186566\n166/166 [==============================] - 13s 72ms/step - loss: 1385.6237 - accuracy: 0.7788 - val_loss: 1385.3113 - val_accuracy: 0.7807\nEpoch 108/200\nlearning rate scheduled to 0.00033775449846871195\n166/166 [==============================] - 13s 71ms/step - loss: 1384.9978 - accuracy: 0.7786 - val_loss: 1384.6826 - val_accuracy: 0.7885\nEpoch 109/200\nlearning rate scheduled to 0.0003343769477214664\n166/166 [==============================] - 13s 71ms/step - loss: 1384.3831 - accuracy: 0.7789 - val_loss: 1384.0945 - val_accuracy: 0.7551\nEpoch 110/200\nlearning rate scheduled to 0.0003310331655666232\n166/166 [==============================] - 13s 72ms/step - loss: 1383.7725 - accuracy: 0.7778 - val_loss: 1383.4542 - val_accuracy: 0.7949\nEpoch 111/200\nlearning rate scheduled to 0.00032772283506346864\n166/166 [==============================] - 13s 72ms/step - loss: 1383.1628 - accuracy: 0.7804 - val_loss: 1382.8774 - val_accuracy: 0.7717\nEpoch 112/200\nlearning rate scheduled to 0.00032444561045849693\n166/166 [==============================] - 13s 72ms/step - loss: 1382.5648 - accuracy: 0.7824 - val_loss: 1382.2670 - val_accuracy: 0.7798\nEpoch 113/200\nlearning rate scheduled to 0.00032120114599820226\n166/166 [==============================] - 13s 72ms/step - loss: 1381.9724 - accuracy: 0.7813 - val_loss: 1381.6614 - val_accuracy: 0.7924\nEpoch 114/200\nlearning rate scheduled to 0.00031798912474187093\n166/166 [==============================] - 13s 72ms/step - loss: 1381.3816 - accuracy: 0.7845 - val_loss: 1381.0846 - val_accuracy: 0.7881\nEpoch 115/200\nlearning rate scheduled to 0.0003148092297487892\n166/166 [==============================] - 13s 72ms/step - loss: 1380.8009 - accuracy: 0.7824 - val_loss: 1380.5101 - val_accuracy: 0.7773\nEpoch 116/200\nlearning rate scheduled to 0.0003116611440782435\n166/166 [==============================] - 13s 72ms/step - loss: 1380.2246 - accuracy: 0.7855 - val_loss: 1379.9594 - val_accuracy: 0.7643\nEpoch 117/200\nlearning rate scheduled to 0.000308544521976728\n166/166 [==============================] - 13s 72ms/step - loss: 1379.6591 - accuracy: 0.7822 - val_loss: 1379.3721 - val_accuracy: 0.7787\nEpoch 118/200\nlearning rate scheduled to 0.0003054590753163211\n166/166 [==============================] - 13s 72ms/step - loss: 1379.0948 - accuracy: 0.7832 - val_loss: 1378.8143 - val_accuracy: 0.7749\nEpoch 119/200\nlearning rate scheduled to 0.0003024044871563092\n166/166 [==============================] - 13s 72ms/step - loss: 1378.5391 - accuracy: 0.7840 - val_loss: 1378.2576 - val_accuracy: 0.7870\nEpoch 120/200\nlearning rate scheduled to 0.00029938044055597855\n166/166 [==============================] - 13s 72ms/step - loss: 1377.9873 - accuracy: 0.7844 - val_loss: 1377.7101 - val_accuracy: 0.7847\nEpoch 121/200\nlearning rate scheduled to 0.0002963866473874077\n166/166 [==============================] - 13s 71ms/step - loss: 1377.4423 - accuracy: 0.7863 - val_loss: 1377.1692 - val_accuracy: 0.7941\nEpoch 122/200\nlearning rate scheduled to 0.000293422790709883\n166/166 [==============================] - 13s 72ms/step - loss: 1376.9028 - accuracy: 0.7850 - val_loss: 1376.6221 - val_accuracy: 0.7981\nEpoch 123/200\nlearning rate scheduled to 0.00029048855358269063\n166/166 [==============================] - 13s 72ms/step - loss: 1376.3711 - accuracy: 0.7831 - val_loss: 1376.0962 - val_accuracy: 0.7881\nEpoch 124/200\nlearning rate scheduled to 0.0002875836766907014\n166/166 [==============================] - 13s 71ms/step - loss: 1375.8386 - accuracy: 0.7868 - val_loss: 1375.5647 - val_accuracy: 0.7909\nEpoch 125/200\nlearning rate scheduled to 0.0002847078430932015\n166/166 [==============================] - 13s 72ms/step - loss: 1375.3165 - accuracy: 0.7852 - val_loss: 1375.0527 - val_accuracy: 0.7881\nEpoch 126/200\nlearning rate scheduled to 0.0002818607646622695\n166/166 [==============================] - 13s 72ms/step - loss: 1374.7979 - accuracy: 0.7867 - val_loss: 1374.5496 - val_accuracy: 0.7706\nEpoch 127/200\nlearning rate scheduled to 0.00027904215326998385\n166/166 [==============================] - 13s 72ms/step - loss: 1374.2859 - accuracy: 0.7880 - val_loss: 1374.0280 - val_accuracy: 0.7854\nEpoch 128/200\nlearning rate scheduled to 0.00027625172078842296\n166/166 [==============================] - 13s 72ms/step - loss: 1373.7782 - accuracy: 0.7884 - val_loss: 1373.5383 - val_accuracy: 0.7868\nEpoch 129/200\nlearning rate scheduled to 0.0002734892079024576\n166/166 [==============================] - 13s 72ms/step - loss: 1373.2811 - accuracy: 0.7864 - val_loss: 1373.0177 - val_accuracy: 0.7920\nEpoch 130/200\nlearning rate scheduled to 0.0002707543264841661\n166/166 [==============================] - 13s 72ms/step - loss: 1372.7800 - accuracy: 0.7866 - val_loss: 1372.5345 - val_accuracy: 0.7892\nEpoch 131/200\nlearning rate scheduled to 0.000268046788405627\n166/166 [==============================] - 13s 72ms/step - loss: 1372.2899 - accuracy: 0.7878 - val_loss: 1372.0372 - val_accuracy: 0.7956\nEpoch 132/200\nlearning rate scheduled to 0.000265366334351711\n166/166 [==============================] - 13s 72ms/step - loss: 1371.8047 - accuracy: 0.7885 - val_loss: 1371.5647 - val_accuracy: 0.7864\nEpoch 133/200\nlearning rate scheduled to 0.00026271267619449646\n166/166 [==============================] - 13s 71ms/step - loss: 1371.3218 - accuracy: 0.7905 - val_loss: 1371.0784 - val_accuracy: 0.7968\nEpoch 134/200\nlearning rate scheduled to 0.00026008555461885406\n166/166 [==============================] - 13s 72ms/step - loss: 1370.8469 - accuracy: 0.7900 - val_loss: 1370.6042 - val_accuracy: 0.7958\nEpoch 135/200\nlearning rate scheduled to 0.00025748471030965445\n166/166 [==============================] - 13s 71ms/step - loss: 1370.3776 - accuracy: 0.7878 - val_loss: 1370.1443 - val_accuracy: 0.7903\nEpoch 136/200\nlearning rate scheduled to 0.0002549098551389761\n166/166 [==============================] - 13s 72ms/step - loss: 1369.9099 - accuracy: 0.7882 - val_loss: 1369.6733 - val_accuracy: 0.7830\nEpoch 137/200\nlearning rate scheduled to 0.00025236075860448183\n166/166 [==============================] - 13s 72ms/step - loss: 1369.4493 - accuracy: 0.7901 - val_loss: 1369.2080 - val_accuracy: 0.8002\nEpoch 138/200\nlearning rate scheduled to 0.0002498371613910422\n166/166 [==============================] - 13s 71ms/step - loss: 1368.9890 - accuracy: 0.7908 - val_loss: 1368.7582 - val_accuracy: 0.7924\nEpoch 139/200\nlearning rate scheduled to 0.0002473388041835278\n166/166 [==============================] - 13s 72ms/step - loss: 1368.5350 - accuracy: 0.7908 - val_loss: 1368.3187 - val_accuracy: 0.7875\nEpoch 140/200\nlearning rate scheduled to 0.0002448654276668094\n166/166 [==============================] - 13s 72ms/step - loss: 1368.0896 - accuracy: 0.7902 - val_loss: 1367.8645 - val_accuracy: 0.7917\nEpoch 141/200\nlearning rate scheduled to 0.00024241677252575755\n166/166 [==============================] - 13s 72ms/step - loss: 1367.6486 - accuracy: 0.7897 - val_loss: 1367.4257 - val_accuracy: 0.7866\nEpoch 142/200\nlearning rate scheduled to 0.00023999260825803502\n166/166 [==============================] - 13s 71ms/step - loss: 1367.2098 - accuracy: 0.7911 - val_loss: 1366.9937 - val_accuracy: 0.7917\nEpoch 143/200\nlearning rate scheduled to 0.00023759267554851249\n166/166 [==============================] - 13s 72ms/step - loss: 1366.7748 - accuracy: 0.7892 - val_loss: 1366.5511 - val_accuracy: 0.7975\nEpoch 144/200\nlearning rate scheduled to 0.0002352167438948527\n166/166 [==============================] - 13s 72ms/step - loss: 1366.3483 - accuracy: 0.7881 - val_loss: 1366.1333 - val_accuracy: 0.7883\nEpoch 145/200\nlearning rate scheduled to 0.00023286458279471843\n166/166 [==============================] - 13s 72ms/step - loss: 1365.9229 - accuracy: 0.7907 - val_loss: 1365.7125 - val_accuracy: 0.7864\nEpoch 146/200\nlearning rate scheduled to 0.00023053593293298035\n166/166 [==============================] - 13s 72ms/step - loss: 1365.5027 - accuracy: 0.7911 - val_loss: 1365.3065 - val_accuracy: 0.7796\nEpoch 147/200\nlearning rate scheduled to 0.0002282305782136973\n166/166 [==============================] - 13s 72ms/step - loss: 1365.0847 - accuracy: 0.7921 - val_loss: 1364.8807 - val_accuracy: 0.7883\nEpoch 148/200\nlearning rate scheduled to 0.00022594827372813598\n166/166 [==============================] - 13s 72ms/step - loss: 1364.6733 - accuracy: 0.7952 - val_loss: 1364.4805 - val_accuracy: 0.7902\nEpoch 149/200\nlearning rate scheduled to 0.00022368878897395915\n166/166 [==============================] - 13s 72ms/step - loss: 1364.2687 - accuracy: 0.7893 - val_loss: 1364.0598 - val_accuracy: 0.7930\nEpoch 150/200\nlearning rate scheduled to 0.00022145190785522573\n166/166 [==============================] - 13s 72ms/step - loss: 1363.8630 - accuracy: 0.7902 - val_loss: 1363.6476 - val_accuracy: 0.8015\nEpoch 151/200\nlearning rate scheduled to 0.00021923738546320237\n166/166 [==============================] - 13s 72ms/step - loss: 1363.4647 - accuracy: 0.7909 - val_loss: 1363.2593 - val_accuracy: 0.7915\nEpoch 152/200\nlearning rate scheduled to 0.00021704500570194797\n166/166 [==============================] - 13s 71ms/step - loss: 1363.0684 - accuracy: 0.7928 - val_loss: 1362.8740 - val_accuracy: 0.7968\nEpoch 153/200\nlearning rate scheduled to 0.00021487455247552135\n166/166 [==============================] - 13s 72ms/step - loss: 1362.6759 - accuracy: 0.7921 - val_loss: 1362.4761 - val_accuracy: 0.7962\nEpoch 154/200\nlearning rate scheduled to 0.00021272580968798138\n166/166 [==============================] - 13s 71ms/step - loss: 1362.2875 - accuracy: 0.7936 - val_loss: 1362.0979 - val_accuracy: 0.7892\nEpoch 155/200\nlearning rate scheduled to 0.00021059854683699086\n166/166 [==============================] - 13s 73ms/step - loss: 1361.9049 - accuracy: 0.7942 - val_loss: 1361.7045 - val_accuracy: 0.7996\nEpoch 156/200\nlearning rate scheduled to 0.0002084925622330047\n166/166 [==============================] - 13s 72ms/step - loss: 1361.5251 - accuracy: 0.7929 - val_loss: 1361.3285 - val_accuracy: 0.7985\nEpoch 157/200\nlearning rate scheduled to 0.0002064076397800818\n166/166 [==============================] - 13s 72ms/step - loss: 1361.1531 - accuracy: 0.7937 - val_loss: 1360.9878 - val_accuracy: 0.7696\nEpoch 158/200\nlearning rate scheduled to 0.000204343563382281\n166/166 [==============================] - 13s 72ms/step - loss: 1360.7814 - accuracy: 0.7926 - val_loss: 1360.5905 - val_accuracy: 0.7960\nEpoch 159/200\nlearning rate scheduled to 0.0002023001313500572\n166/166 [==============================] - 13s 72ms/step - loss: 1360.4125 - accuracy: 0.7934 - val_loss: 1360.2452 - val_accuracy: 0.7847\nEpoch 160/200\nlearning rate scheduled to 0.0002002771275874693\n166/166 [==============================] - 13s 72ms/step - loss: 1360.0487 - accuracy: 0.7962 - val_loss: 1359.9019 - val_accuracy: 0.7687\nEpoch 161/200\nlearning rate scheduled to 0.0001982743504049722\n166/166 [==============================] - 13s 72ms/step - loss: 1359.6895 - accuracy: 0.7955 - val_loss: 1359.4985 - val_accuracy: 0.8056\nEpoch 162/200\nlearning rate scheduled to 0.00019629161251941696\n166/166 [==============================] - 13s 72ms/step - loss: 1359.3357 - accuracy: 0.7936 - val_loss: 1359.1512 - val_accuracy: 0.8002\nEpoch 163/200\nlearning rate scheduled to 0.0001943286978348624\n166/166 [==============================] - 13s 72ms/step - loss: 1358.9774 - accuracy: 0.7960 - val_loss: 1358.7944 - val_accuracy: 0.7945\nEpoch 164/200\nlearning rate scheduled to 0.00019238540466176346\n166/166 [==============================] - 13s 71ms/step - loss: 1358.6318 - accuracy: 0.7918 - val_loss: 1358.4515 - val_accuracy: 0.7986\nEpoch 165/200\nlearning rate scheduled to 0.00019046154571697116\n166/166 [==============================] - 13s 72ms/step - loss: 1358.2833 - accuracy: 0.7938 - val_loss: 1358.1013 - val_accuracy: 0.8028\nEpoch 166/200\nlearning rate scheduled to 0.0001885569337173365\n166/166 [==============================] - 13s 72ms/step - loss: 1357.9418 - accuracy: 0.7959 - val_loss: 1357.7667 - val_accuracy: 0.7898\nEpoch 167/200\nlearning rate scheduled to 0.00018667136697331444\n166/166 [==============================] - 13s 72ms/step - loss: 1357.6040 - accuracy: 0.7933 - val_loss: 1357.4373 - val_accuracy: 0.7934\nEpoch 168/200\nlearning rate scheduled to 0.00018480465820175596\n166/166 [==============================] - 13s 72ms/step - loss: 1357.2712 - accuracy: 0.7936 - val_loss: 1357.0946 - val_accuracy: 0.7992\nEpoch 169/200\nlearning rate scheduled to 0.000182956605713116\n166/166 [==============================] - 13s 72ms/step - loss: 1356.9393 - accuracy: 0.7936 - val_loss: 1356.7743 - val_accuracy: 0.7919\nEpoch 170/200\nlearning rate scheduled to 0.00018112703663064166\n166/166 [==============================] - 13s 72ms/step - loss: 1356.6100 - accuracy: 0.7963 - val_loss: 1356.4377 - val_accuracy: 0.8028\nEpoch 171/200\nlearning rate scheduled to 0.00017931576367118395\n166/166 [==============================] - 13s 72ms/step - loss: 1356.2865 - accuracy: 0.7922 - val_loss: 1356.1178 - val_accuracy: 0.7975\nEpoch 172/200\nlearning rate scheduled to 0.0001775225995515939\n166/166 [==============================] - 13s 71ms/step - loss: 1355.9623 - accuracy: 0.7959 - val_loss: 1355.8020 - val_accuracy: 0.7960\nEpoch 173/200\nlearning rate scheduled to 0.00017574737139511854\n166/166 [==============================] - 13s 72ms/step - loss: 1355.6454 - accuracy: 0.7962 - val_loss: 1355.4958 - val_accuracy: 0.7896\nEpoch 174/200\nlearning rate scheduled to 0.00017398989191860892\n166/166 [==============================] - 13s 72ms/step - loss: 1355.3329 - accuracy: 0.7943 - val_loss: 1355.1744 - val_accuracy: 0.7977\nEpoch 175/200\nlearning rate scheduled to 0.00017224998824531213\n166/166 [==============================] - 13s 72ms/step - loss: 1355.0194 - accuracy: 0.7967 - val_loss: 1354.8586 - val_accuracy: 0.8047\nEpoch 176/200\nlearning rate scheduled to 0.00017052748749847522\n166/166 [==============================] - 13s 72ms/step - loss: 1354.7129 - accuracy: 0.7950 - val_loss: 1354.5573 - val_accuracy: 0.7977\nEpoch 177/200\nlearning rate scheduled to 0.00016882221680134535\n166/166 [==============================] - 13s 72ms/step - loss: 1354.4061 - accuracy: 0.7947 - val_loss: 1354.2593 - val_accuracy: 0.7924\nEpoch 178/200\nlearning rate scheduled to 0.00016713398887077346\n166/166 [==============================] - 13s 72ms/step - loss: 1354.1034 - accuracy: 0.7946 - val_loss: 1353.9506 - val_accuracy: 0.7939\nEpoch 179/200\nlearning rate scheduled to 0.00016546264523640276\n166/166 [==============================] - 13s 72ms/step - loss: 1353.8018 - accuracy: 0.7989 - val_loss: 1353.6405 - val_accuracy: 0.7998\nEpoch 180/200\nlearning rate scheduled to 0.00016380801302148028\n166/166 [==============================] - 13s 72ms/step - loss: 1353.5048 - accuracy: 0.7971 - val_loss: 1353.3633 - val_accuracy: 0.7888\nEpoch 181/200\nlearning rate scheduled to 0.00016216993375564926\n166/166 [==============================] - 13s 72ms/step - loss: 1353.2123 - accuracy: 0.7964 - val_loss: 1353.0725 - val_accuracy: 0.7817\nEpoch 182/200\nlearning rate scheduled to 0.00016054823456215672\n166/166 [==============================] - 13s 72ms/step - loss: 1352.9241 - accuracy: 0.7940 - val_loss: 1352.7775 - val_accuracy: 0.7977\nEpoch 183/200\nlearning rate scheduled to 0.00015894275697064586\n166/166 [==============================] - 13s 72ms/step - loss: 1352.6340 - accuracy: 0.7958 - val_loss: 1352.4875 - val_accuracy: 0.7941\nEpoch 184/200\nlearning rate scheduled to 0.00015735332810436374\n166/166 [==============================] - 13s 72ms/step - loss: 1352.3513 - accuracy: 0.7947 - val_loss: 1352.2054 - val_accuracy: 0.7905\nEpoch 185/200\nlearning rate scheduled to 0.00015577978949295356\n166/166 [==============================] - 13s 72ms/step - loss: 1352.0699 - accuracy: 0.7968 - val_loss: 1351.9360 - val_accuracy: 0.7947\nEpoch 186/200\nlearning rate scheduled to 0.00015422199707245453\n166/166 [==============================] - 13s 73ms/step - loss: 1351.7932 - accuracy: 0.7956 - val_loss: 1351.6587 - val_accuracy: 0.7920\nEpoch 187/200\nlearning rate scheduled to 0.00015267977796611375\n166/166 [==============================] - 13s 71ms/step - loss: 1351.5176 - accuracy: 0.7953 - val_loss: 1351.3815 - val_accuracy: 0.7879\nEpoch 188/200\nlearning rate scheduled to 0.00015115297370357439\n166/166 [==============================] - 13s 72ms/step - loss: 1351.2461 - accuracy: 0.7964 - val_loss: 1351.1014 - val_accuracy: 0.8034\nEpoch 189/200\nlearning rate scheduled to 0.00014964144022087568\n166/166 [==============================] - 13s 71ms/step - loss: 1350.9766 - accuracy: 0.7956 - val_loss: 1350.8362 - val_accuracy: 0.8002\nEpoch 190/200\nlearning rate scheduled to 0.00014814501904766075\n166/166 [==============================] - 13s 72ms/step - loss: 1350.7079 - accuracy: 0.7954 - val_loss: 1350.5720 - val_accuracy: 0.7937\nEpoch 191/200\nlearning rate scheduled to 0.00014666356611996888\n166/166 [==============================] - 13s 72ms/step - loss: 1350.4470 - accuracy: 0.7931 - val_loss: 1350.3146 - val_accuracy: 0.7975\nEpoch 192/200\nlearning rate scheduled to 0.00014519693737383933\n166/166 [==============================] - 13s 72ms/step - loss: 1350.1842 - accuracy: 0.7954 - val_loss: 1350.0581 - val_accuracy: 0.7960\nEpoch 193/200\nlearning rate scheduled to 0.0001437449743389152\n166/166 [==============================] - 13s 72ms/step - loss: 1349.9260 - accuracy: 0.7943 - val_loss: 1349.8063 - val_accuracy: 0.7909\nEpoch 194/200\nlearning rate scheduled to 0.00014230751854483968\n166/166 [==============================] - 13s 72ms/step - loss: 1349.6687 - accuracy: 0.7982 - val_loss: 1349.5514 - val_accuracy: 0.7870\nEpoch 195/200\nlearning rate scheduled to 0.00014088444033404812\n166/166 [==============================] - 13s 71ms/step - loss: 1349.4161 - accuracy: 0.7967 - val_loss: 1349.2786 - val_accuracy: 0.8011\nEpoch 196/200\nlearning rate scheduled to 0.0001394755956425797\n166/166 [==============================] - 13s 72ms/step - loss: 1349.1630 - accuracy: 0.7984 - val_loss: 1349.0345 - val_accuracy: 0.7998\nEpoch 197/200\nlearning rate scheduled to 0.00013808084040647372\n166/166 [==============================] - 13s 72ms/step - loss: 1348.9170 - accuracy: 0.7957 - val_loss: 1348.7848 - val_accuracy: 0.8020\nEpoch 198/200\nlearning rate scheduled to 0.00013670003056176939\n166/166 [==============================] - 13s 72ms/step - loss: 1348.6669 - accuracy: 0.7979 - val_loss: 1348.5424 - val_accuracy: 0.7885\nEpoch 199/200\nlearning rate scheduled to 0.000135333036450902\n166/166 [==============================] - 13s 71ms/step - loss: 1348.4230 - accuracy: 0.7962 - val_loss: 1348.2994 - val_accuracy: 0.8005\nEpoch 200/200\nlearning rate scheduled to 0.00013397969960351474\n166/166 [==============================] - 13s 71ms/step - loss: 1348.1787 - accuracy: 0.7969 - val_loss: 1348.0508 - val_accuracy: 0.8047\n"
    }
   ],
   "source": [
    "history_original_siamese_model = original_siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                              model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "42/42 [==============================] - 2s 23ms/step - loss: 1348.0594 - accuracy: 0.7954\n"
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Second Run - 90k Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_90k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_90k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_7 (InputLayer)         [(None, 105, 105, 3)]     0         \n_________________________________________________________________\nConv1 (Conv2D)               (None, 96, 96, 64)        19264     \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 48, 48, 64)        0         \n_________________________________________________________________\nConv2 (Conv2D)               (None, 42, 42, 128)       401536    \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 21, 21, 128)       0         \n_________________________________________________________________\nConv3 (Conv2D)               (None, 18, 18, 128)       262272    \n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 9, 9, 128)         0         \n_________________________________________________________________\nConv4 (Conv2D)               (None, 6, 6, 256)         524544    \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 9216)              0         \n_________________________________________________________________\nDense1 (Dense)               (None, 4096)              37752832  \n=================================================================\nTotal params: 38,960,448\nTrainable params: 38,960,448\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_2 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1te8jbgy\" target=\"_blank\">driven-snow-5</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 90k\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "original_siamese_model_2.compile(loss=config.loss_function,\n",
    "                               optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_90k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/200\nlearning rate scheduled to 0.0009900000470224768\n610/610 [==============================] - 45s 64ms/step - loss: 1507.0350 - accuracy: 0.5532 - val_loss: 1503.3882 - val_accuracy: 0.5779\nEpoch 2/200\nlearning rate scheduled to 0.000980100086890161\n610/610 [==============================] - 42s 63ms/step - loss: 1499.8073 - accuracy: 0.5696 - val_loss: 1496.2098 - val_accuracy: 0.5619\nEpoch 3/200\nlearning rate scheduled to 0.0009702991275116801\n610/610 [==============================] - 42s 62ms/step - loss: 1492.6814 - accuracy: 0.5683 - val_loss: 1489.1345 - val_accuracy: 0.5802\nEpoch 4/200\nlearning rate scheduled to 0.0009605961316265165\n610/610 [==============================] - 42s 63ms/step - loss: 1485.6611 - accuracy: 0.5803 - val_loss: 1482.1699 - val_accuracy: 0.5906\nEpoch 5/200\nlearning rate scheduled to 0.0009509901772253215\n610/610 [==============================] - 42s 63ms/step - loss: 1478.7444 - accuracy: 0.5917 - val_loss: 1475.3059 - val_accuracy: 0.5928\nEpoch 6/200\nlearning rate scheduled to 0.0009414802846731617\n610/610 [==============================] - 41s 62ms/step - loss: 1471.9305 - accuracy: 0.6039 - val_loss: 1468.5360 - val_accuracy: 0.6190\nEpoch 7/200\nlearning rate scheduled to 0.0009320654743351042\n610/610 [==============================] - 41s 62ms/step - loss: 1465.2065 - accuracy: 0.6267 - val_loss: 1461.8644 - val_accuracy: 0.6321\nEpoch 8/200\nlearning rate scheduled to 0.0009227448242017999\n610/610 [==============================] - 42s 62ms/step - loss: 1458.5757 - accuracy: 0.6484 - val_loss: 1455.2772 - val_accuracy: 0.6582\nEpoch 9/200\nlearning rate scheduled to 0.0009135173546383158\n610/610 [==============================] - 41s 62ms/step - loss: 1452.0315 - accuracy: 0.6749 - val_loss: 1448.7897 - val_accuracy: 0.6790\nEpoch 10/200\nlearning rate scheduled to 0.0009043822012608871\n610/610 [==============================] - 42s 63ms/step - loss: 1445.5803 - accuracy: 0.7005 - val_loss: 1442.4174 - val_accuracy: 0.6789\nEpoch 11/200\nlearning rate scheduled to 0.0008953383844345808\n610/610 [==============================] - 42s 63ms/step - loss: 1439.2347 - accuracy: 0.7116 - val_loss: 1436.0679 - val_accuracy: 0.7206\nEpoch 12/200\nlearning rate scheduled to 0.000886384982150048\n610/610 [==============================] - 42s 63ms/step - loss: 1432.9813 - accuracy: 0.7225 - val_loss: 1429.9125 - val_accuracy: 0.7014\nEpoch 13/200\nlearning rate scheduled to 0.0008775211300235242\n610/610 [==============================] - 42s 63ms/step - loss: 1426.8206 - accuracy: 0.7287 - val_loss: 1423.9325 - val_accuracy: 0.5117\nEpoch 14/200\nlearning rate scheduled to 0.0008687459060456604\n610/610 [==============================] - 42s 63ms/step - loss: 1420.7604 - accuracy: 0.7171 - val_loss: 1417.7642 - val_accuracy: 0.7122\nEpoch 15/200\nlearning rate scheduled to 0.0008600584458326921\n610/610 [==============================] - 42s 63ms/step - loss: 1414.7615 - accuracy: 0.7413 - val_loss: 1411.9445 - val_accuracy: 0.5789\nEpoch 16/200\nlearning rate scheduled to 0.0008514578850008547\n610/610 [==============================] - 42s 63ms/step - loss: 1408.8625 - accuracy: 0.7464 - val_loss: 1405.9989 - val_accuracy: 0.7011\nEpoch 17/200\nlearning rate scheduled to 0.0008429433015407995\n610/610 [==============================] - 42s 63ms/step - loss: 1403.0518 - accuracy: 0.7462 - val_loss: 1400.1442 - val_accuracy: 0.7484\nEpoch 18/200\nlearning rate scheduled to 0.0008345138886943459\n610/610 [==============================] - 42s 63ms/step - loss: 1397.3131 - accuracy: 0.7526 - val_loss: 1394.4819 - val_accuracy: 0.7435\nEpoch 19/200\nlearning rate scheduled to 0.0008261687244521454\n610/610 [==============================] - 42s 63ms/step - loss: 1391.6644 - accuracy: 0.7538 - val_loss: 1388.8376 - val_accuracy: 0.7611\nEpoch 20/200\nlearning rate scheduled to 0.0008179070596816018\n610/610 [==============================] - 42s 63ms/step - loss: 1386.0854 - accuracy: 0.7590 - val_loss: 1383.4348 - val_accuracy: 0.6671\nEpoch 21/200\nlearning rate scheduled to 0.0008097279723733664\n610/610 [==============================] - 42s 63ms/step - loss: 1380.5967 - accuracy: 0.7584 - val_loss: 1377.8844 - val_accuracy: 0.7514\nEpoch 22/200\nlearning rate scheduled to 0.000801630713394843\n610/610 [==============================] - 42s 63ms/step - loss: 1375.1671 - accuracy: 0.7691 - val_loss: 1372.4764 - val_accuracy: 0.7705\nEpoch 23/200\nlearning rate scheduled to 0.0007936144183622674\n610/610 [==============================] - 42s 63ms/step - loss: 1369.8274 - accuracy: 0.7668 - val_loss: 1367.1617 - val_accuracy: 0.7666\nEpoch 24/200\nlearning rate scheduled to 0.0007856782805174589\n610/610 [==============================] - 42s 63ms/step - loss: 1364.5526 - accuracy: 0.7740 - val_loss: 1361.9307 - val_accuracy: 0.7708\nEpoch 25/200\nlearning rate scheduled to 0.0007778214931022376\n610/610 [==============================] - 42s 63ms/step - loss: 1359.3542 - accuracy: 0.7776 - val_loss: 1356.8906 - val_accuracy: 0.6782\nEpoch 26/200\nlearning rate scheduled to 0.0007700433069840073\n610/610 [==============================] - 42s 63ms/step - loss: 1354.2271 - accuracy: 0.7795 - val_loss: 1351.6852 - val_accuracy: 0.7811\nEpoch 27/200\nlearning rate scheduled to 0.0007623428577790037\n610/610 [==============================] - 42s 63ms/step - loss: 1349.1678 - accuracy: 0.7838 - val_loss: 1346.6478 - val_accuracy: 0.7867\nEpoch 28/200\nlearning rate scheduled to 0.0007547194539802149\n610/610 [==============================] - 42s 63ms/step - loss: 1344.1853 - accuracy: 0.7830 - val_loss: 1341.7061 - val_accuracy: 0.7760\nEpoch 29/200\nlearning rate scheduled to 0.0007471722312038764\n610/610 [==============================] - 42s 63ms/step - loss: 1339.2679 - accuracy: 0.7841 - val_loss: 1336.8086 - val_accuracy: 0.7930\nEpoch 30/200\nlearning rate scheduled to 0.0007397004979429766\n610/610 [==============================] - 42s 63ms/step - loss: 1334.4143 - accuracy: 0.7884 - val_loss: 1332.3622 - val_accuracy: 0.4982\nEpoch 31/200\nlearning rate scheduled to 0.0007323035050649196\n610/610 [==============================] - 42s 63ms/step - loss: 1329.6329 - accuracy: 0.7857 - val_loss: 1327.2433 - val_accuracy: 0.7890\nEpoch 32/200\nlearning rate scheduled to 0.000724980445811525\n610/610 [==============================] - 42s 63ms/step - loss: 1324.9077 - accuracy: 0.7906 - val_loss: 1322.5947 - val_accuracy: 0.7646\nEpoch 33/200\nlearning rate scheduled to 0.0007177306286757812\n610/610 [==============================] - 42s 63ms/step - loss: 1320.2485 - accuracy: 0.7923 - val_loss: 1317.9796 - val_accuracy: 0.7568\nEpoch 34/200\nlearning rate scheduled to 0.0007105533045250923\n610/610 [==============================] - 42s 63ms/step - loss: 1315.6534 - accuracy: 0.7938 - val_loss: 1313.3619 - val_accuracy: 0.7985\nEpoch 35/200\nlearning rate scheduled to 0.0007034477818524464\n610/610 [==============================] - 41s 62ms/step - loss: 1311.1200 - accuracy: 0.7961 - val_loss: 1309.2632 - val_accuracy: 0.5048\nEpoch 36/200\nlearning rate scheduled to 0.000696413311525248\n610/610 [==============================] - 42s 63ms/step - loss: 1306.6477 - accuracy: 0.7966 - val_loss: 1304.4231 - val_accuracy: 0.7940\nEpoch 37/200\nlearning rate scheduled to 0.0006894492020364851\n610/610 [==============================] - 42s 63ms/step - loss: 1302.2341 - accuracy: 0.7983 - val_loss: 1300.0413 - val_accuracy: 0.7912\nEpoch 38/200\nlearning rate scheduled to 0.0006825547042535618\n610/610 [==============================] - 42s 63ms/step - loss: 1297.8794 - accuracy: 0.7992 - val_loss: 1295.7295 - val_accuracy: 0.7862\nEpoch 39/200\nlearning rate scheduled to 0.0006757291842950508\n610/610 [==============================] - 42s 63ms/step - loss: 1293.5837 - accuracy: 0.8019 - val_loss: 1291.4979 - val_accuracy: 0.7638\nEpoch 40/200\nlearning rate scheduled to 0.0006689718930283561\n610/610 [==============================] - 42s 63ms/step - loss: 1289.3441 - accuracy: 0.8030 - val_loss: 1287.2780 - val_accuracy: 0.7711\nEpoch 41/200\nlearning rate scheduled to 0.0006622821965720505\n610/610 [==============================] - 42s 62ms/step - loss: 1285.1630 - accuracy: 0.8022 - val_loss: 1283.1990 - val_accuracy: 0.7346\nEpoch 42/200\nlearning rate scheduled to 0.0006556594034191221\n610/610 [==============================] - 42s 63ms/step - loss: 1281.0383 - accuracy: 0.8036 - val_loss: 1278.9851 - val_accuracy: 0.8039\nEpoch 43/200\nlearning rate scheduled to 0.0006491028220625594\n610/610 [==============================] - 42s 63ms/step - loss: 1276.9648 - accuracy: 0.8040 - val_loss: 1275.1462 - val_accuracy: 0.6613\nEpoch 44/200\nlearning rate scheduled to 0.0006426118186209351\n610/610 [==============================] - 42s 63ms/step - loss: 1272.9481 - accuracy: 0.8042 - val_loss: 1271.0267 - val_accuracy: 0.7601\nEpoch 45/200\nlearning rate scheduled to 0.0006361857015872375\n610/610 [==============================] - 42s 62ms/step - loss: 1268.9812 - accuracy: 0.8060 - val_loss: 1266.9989 - val_accuracy: 0.8101\nEpoch 46/200\nlearning rate scheduled to 0.0006298238370800391\n610/610 [==============================] - 42s 63ms/step - loss: 1265.0660 - accuracy: 0.8077 - val_loss: 1263.2086 - val_accuracy: 0.7453\nEpoch 47/200\nlearning rate scheduled to 0.0006235255912179127\n610/610 [==============================] - 42s 63ms/step - loss: 1261.2062 - accuracy: 0.8076 - val_loss: 1259.3153 - val_accuracy: 0.7862\nEpoch 48/200\nlearning rate scheduled to 0.000617290330119431\n610/610 [==============================] - 42s 63ms/step - loss: 1257.3931 - accuracy: 0.8064 - val_loss: 1255.7924 - val_accuracy: 0.6621\nEpoch 49/200\nlearning rate scheduled to 0.0006111174199031666\n610/610 [==============================] - 41s 62ms/step - loss: 1253.6295 - accuracy: 0.8077 - val_loss: 1252.0464 - val_accuracy: 0.6237\nEpoch 50/200\nlearning rate scheduled to 0.0006050062266876921\n610/610 [==============================] - 42s 63ms/step - loss: 1249.9150 - accuracy: 0.8088 - val_loss: 1248.0654 - val_accuracy: 0.8070\nEpoch 51/200\nlearning rate scheduled to 0.0005989561742171646\n610/610 [==============================] - 42s 63ms/step - loss: 1246.2469 - accuracy: 0.8105 - val_loss: 1244.4200 - val_accuracy: 0.8149\nEpoch 52/200\nlearning rate scheduled to 0.0005929666286101564\n610/610 [==============================] - 42s 63ms/step - loss: 1242.6287 - accuracy: 0.8113 - val_loss: 1240.8542 - val_accuracy: 0.7861\nEpoch 53/200\nlearning rate scheduled to 0.0005870369559852406\n610/610 [==============================] - 42s 62ms/step - loss: 1239.0557 - accuracy: 0.8118 - val_loss: 1237.2771 - val_accuracy: 0.8110\nEpoch 54/200\nlearning rate scheduled to 0.000581166580086574\n610/610 [==============================] - 42s 63ms/step - loss: 1235.5272 - accuracy: 0.8122 - val_loss: 1233.7715 - val_accuracy: 0.8144\nEpoch 55/200\nlearning rate scheduled to 0.0005753549246583134\n610/610 [==============================] - 41s 62ms/step - loss: 1232.0459 - accuracy: 0.8137 - val_loss: 1230.3989 - val_accuracy: 0.7556\nEpoch 56/200\nlearning rate scheduled to 0.0005696013558190316\n610/610 [==============================] - 42s 63ms/step - loss: 1228.6112 - accuracy: 0.8122 - val_loss: 1226.8976 - val_accuracy: 0.8139\nEpoch 57/200\nlearning rate scheduled to 0.0005639053549384699\n610/610 [==============================] - 42s 63ms/step - loss: 1225.2148 - accuracy: 0.8154 - val_loss: 1223.6423 - val_accuracy: 0.7378\nEpoch 58/200\nlearning rate scheduled to 0.0005582662881352008\n610/610 [==============================] - 41s 62ms/step - loss: 1221.8671 - accuracy: 0.8142 - val_loss: 1220.2166 - val_accuracy: 0.8042\nEpoch 59/200\nlearning rate scheduled to 0.0005526836367789656\n610/610 [==============================] - 42s 63ms/step - loss: 1218.5553 - accuracy: 0.8171 - val_loss: 1216.9401 - val_accuracy: 0.7960\nEpoch 60/200\nlearning rate scheduled to 0.0005471568246139213\n610/610 [==============================] - 42s 63ms/step - loss: 1215.2919 - accuracy: 0.8160 - val_loss: 1214.3577 - val_accuracy: 0.5004\nEpoch 61/200\nlearning rate scheduled to 0.000541685275384225\n610/610 [==============================] - 42s 63ms/step - loss: 1212.0751 - accuracy: 0.8131 - val_loss: 1210.4591 - val_accuracy: 0.8192\nEpoch 62/200\nlearning rate scheduled to 0.0005362684128340334\n610/610 [==============================] - 42s 63ms/step - loss: 1208.8845 - accuracy: 0.8170 - val_loss: 1207.3274 - val_accuracy: 0.8000\nEpoch 63/200\nlearning rate scheduled to 0.0005309057183330878\n610/610 [==============================] - 42s 63ms/step - loss: 1205.7416 - accuracy: 0.8178 - val_loss: 1204.2291 - val_accuracy: 0.7894\nEpoch 64/200\nlearning rate scheduled to 0.0005255966732511297\n610/610 [==============================] - 42s 63ms/step - loss: 1202.6344 - accuracy: 0.8182 - val_loss: 1201.0917 - val_accuracy: 0.8188\nEpoch 65/200\nlearning rate scheduled to 0.0005203407013323158\n610/610 [==============================] - 41s 62ms/step - loss: 1199.5699 - accuracy: 0.8205 - val_loss: 1198.0327 - val_accuracy: 0.8232\nEpoch 66/200\nlearning rate scheduled to 0.0005151372839463875\n610/610 [==============================] - 42s 62ms/step - loss: 1196.5426 - accuracy: 0.8198 - val_loss: 1195.1063 - val_accuracy: 0.7798\nEpoch 67/200\nlearning rate scheduled to 0.0005099859024630859\n610/610 [==============================] - 42s 63ms/step - loss: 1193.5537 - accuracy: 0.8202 - val_loss: 1192.0641 - val_accuracy: 0.8219\nEpoch 68/200\nlearning rate scheduled to 0.0005048860382521525\n610/610 [==============================] - 42s 62ms/step - loss: 1190.6021 - accuracy: 0.8213 - val_loss: 1189.1271 - val_accuracy: 0.8257\nEpoch 69/200\nlearning rate scheduled to 0.0004998371726833284\n610/610 [==============================] - 42s 63ms/step - loss: 1187.6862 - accuracy: 0.8217 - val_loss: 1186.3461 - val_accuracy: 0.7636\nEpoch 70/200\nlearning rate scheduled to 0.0004948387871263549\n610/610 [==============================] - 42s 62ms/step - loss: 1184.8069 - accuracy: 0.8232 - val_loss: 1183.3694 - val_accuracy: 0.8249\nEpoch 71/200\nlearning rate scheduled to 0.0004898904205765575\n610/610 [==============================] - 42s 63ms/step - loss: 1181.9645 - accuracy: 0.8212 - val_loss: 1180.5573 - val_accuracy: 0.8160\nEpoch 72/200\nlearning rate scheduled to 0.00048499149677809326\n610/610 [==============================] - 42s 63ms/step - loss: 1179.1549 - accuracy: 0.8236 - val_loss: 1177.7479 - val_accuracy: 0.8287\nEpoch 73/200\nlearning rate scheduled to 0.00048014158353907986\n610/610 [==============================] - 41s 62ms/step - loss: 1176.3818 - accuracy: 0.8235 - val_loss: 1174.9929 - val_accuracy: 0.8285\nEpoch 74/200\nlearning rate scheduled to 0.00047534016222925855\n610/610 [==============================] - 42s 63ms/step - loss: 1173.6417 - accuracy: 0.8254 - val_loss: 1172.2737 - val_accuracy: 0.8278\nEpoch 75/200\nlearning rate scheduled to 0.0004705867718439549\n610/610 [==============================] - 42s 62ms/step - loss: 1170.9366 - accuracy: 0.8248 - val_loss: 1169.6934 - val_accuracy: 0.7668\nEpoch 76/200\nlearning rate scheduled to 0.0004658808937529102\n610/610 [==============================] - 42s 63ms/step - loss: 1168.2642 - accuracy: 0.8251 - val_loss: 1166.9368 - val_accuracy: 0.8224\nEpoch 77/200\nlearning rate scheduled to 0.0004612220957642421\n610/610 [==============================] - 42s 63ms/step - loss: 1165.6234 - accuracy: 0.8256 - val_loss: 1164.3176 - val_accuracy: 0.8191\nEpoch 78/200\nlearning rate scheduled to 0.00045660988806048406\n610/610 [==============================] - 42s 63ms/step - loss: 1163.0153 - accuracy: 0.8261 - val_loss: 1161.7191 - val_accuracy: 0.8261\nEpoch 79/200\nlearning rate scheduled to 0.0004520437808241695\n610/610 [==============================] - 42s 63ms/step - loss: 1160.4387 - accuracy: 0.8291 - val_loss: 1159.1459 - val_accuracy: 0.8338\nEpoch 80/200\nlearning rate scheduled to 0.0004475233418634161\n610/610 [==============================] - 42s 63ms/step - loss: 1157.8934 - accuracy: 0.8282 - val_loss: 1156.6318 - val_accuracy: 0.8237\nEpoch 81/200\nlearning rate scheduled to 0.0004430481101735495\n610/610 [==============================] - 42s 63ms/step - loss: 1155.3828 - accuracy: 0.8286 - val_loss: 1154.2388 - val_accuracy: 0.7659\nEpoch 82/200\nlearning rate scheduled to 0.0004386176247498952\n610/610 [==============================] - 42s 63ms/step - loss: 1152.8998 - accuracy: 0.8290 - val_loss: 1151.6813 - val_accuracy: 0.8149\nEpoch 83/200\nlearning rate scheduled to 0.00043423145340057087\n610/610 [==============================] - 42s 63ms/step - loss: 1150.4443 - accuracy: 0.8305 - val_loss: 1149.2217 - val_accuracy: 0.8292\nEpoch 84/200\nlearning rate scheduled to 0.0004298891351209022\n610/610 [==============================] - 42s 63ms/step - loss: 1148.0221 - accuracy: 0.8306 - val_loss: 1146.8552 - val_accuracy: 0.8079\nEpoch 85/200\nlearning rate scheduled to 0.00042559023771900686\n610/610 [==============================] - 42s 63ms/step - loss: 1145.6272 - accuracy: 0.8320 - val_loss: 1144.5083 - val_accuracy: 0.7868\nEpoch 86/200\nlearning rate scheduled to 0.0004213343290030025\n610/610 [==============================] - 42s 63ms/step - loss: 1143.2610 - accuracy: 0.8333 - val_loss: 1142.0750 - val_accuracy: 0.8379\nEpoch 87/200\nlearning rate scheduled to 0.00041712097678100687\n610/610 [==============================] - 42s 63ms/step - loss: 1140.9270 - accuracy: 0.8323 - val_loss: 1139.7670 - val_accuracy: 0.8286\nEpoch 88/200\nlearning rate scheduled to 0.00041294977767392994\n610/610 [==============================] - 42s 63ms/step - loss: 1138.6189 - accuracy: 0.8322 - val_loss: 1137.4767 - val_accuracy: 0.8256\nEpoch 89/200\nlearning rate scheduled to 0.0004088202706770971\n610/610 [==============================] - 42s 63ms/step - loss: 1136.3365 - accuracy: 0.8342 - val_loss: 1135.2081 - val_accuracy: 0.8252\nEpoch 90/200\nlearning rate scheduled to 0.00040473208122421056\n610/610 [==============================] - 42s 63ms/step - loss: 1134.0814 - accuracy: 0.8363 - val_loss: 1132.9755 - val_accuracy: 0.8236\nEpoch 91/200\nlearning rate scheduled to 0.0004006847483105957\n610/610 [==============================] - 42s 62ms/step - loss: 1131.8536 - accuracy: 0.8363 - val_loss: 1130.7465 - val_accuracy: 0.8337\nEpoch 92/200\nlearning rate scheduled to 0.0003966778973699547\n610/610 [==============================] - 41s 62ms/step - loss: 1129.6555 - accuracy: 0.8354 - val_loss: 1128.5698 - val_accuracy: 0.8252\nEpoch 93/200\nlearning rate scheduled to 0.0003927111250231974\n610/610 [==============================] - 42s 62ms/step - loss: 1127.4822 - accuracy: 0.8356 - val_loss: 1126.3971 - val_accuracy: 0.8343\nEpoch 94/200\nlearning rate scheduled to 0.0003887840278912336\n610/610 [==============================] - 42s 63ms/step - loss: 1125.3334 - accuracy: 0.8353 - val_loss: 1124.2821 - val_accuracy: 0.8236\nEpoch 95/200\nlearning rate scheduled to 0.000384896173782181\n610/610 [==============================] - 42s 63ms/step - loss: 1123.2112 - accuracy: 0.8371 - val_loss: 1122.1509 - val_accuracy: 0.8348\nEpoch 96/200\nlearning rate scheduled to 0.00038104721694253384\n610/610 [==============================] - 42s 63ms/step - loss: 1121.1149 - accuracy: 0.8369 - val_loss: 1120.1112 - val_accuracy: 0.8127\nEpoch 97/200\nlearning rate scheduled to 0.000377236753993202\n610/610 [==============================] - 42s 63ms/step - loss: 1119.0403 - accuracy: 0.8390 - val_loss: 1118.0361 - val_accuracy: 0.8176\nEpoch 98/200\nlearning rate scheduled to 0.0003734643815550953\n610/610 [==============================] - 42s 63ms/step - loss: 1116.9944 - accuracy: 0.8385 - val_loss: 1115.9784 - val_accuracy: 0.8341\nEpoch 99/200\nlearning rate scheduled to 0.0003697297250619158\n610/610 [==============================] - 42s 63ms/step - loss: 1114.9668 - accuracy: 0.8398 - val_loss: 1113.9742 - val_accuracy: 0.8292\nEpoch 100/200\nlearning rate scheduled to 0.0003660324387601577\n610/610 [==============================] - 42s 63ms/step - loss: 1112.9680 - accuracy: 0.8393 - val_loss: 1111.9747 - val_accuracy: 0.8375\nEpoch 101/200\nlearning rate scheduled to 0.00036237211927073074\n610/610 [==============================] - 42s 63ms/step - loss: 1110.9917 - accuracy: 0.8406 - val_loss: 1109.9973 - val_accuracy: 0.8464\nEpoch 102/200\nlearning rate scheduled to 0.0003587483920273371\n610/610 [==============================] - 42s 63ms/step - loss: 1109.0380 - accuracy: 0.8408 - val_loss: 1108.0706 - val_accuracy: 0.8364\nEpoch 103/200\nlearning rate scheduled to 0.0003551609112764709\n610/610 [==============================] - 42s 63ms/step - loss: 1107.1090 - accuracy: 0.8415 - val_loss: 1106.1476 - val_accuracy: 0.8423\nEpoch 104/200\nlearning rate scheduled to 0.0003516093024518341\n610/610 [==============================] - 42s 63ms/step - loss: 1105.2012 - accuracy: 0.8410 - val_loss: 1104.2662 - val_accuracy: 0.8316\nEpoch 105/200\nlearning rate scheduled to 0.0003480932197999209\n610/610 [==============================] - 42s 62ms/step - loss: 1103.3152 - accuracy: 0.8414 - val_loss: 1102.3723 - val_accuracy: 0.8418\nEpoch 106/200\nlearning rate scheduled to 0.0003446122887544334\n610/610 [==============================] - 42s 63ms/step - loss: 1101.4526 - accuracy: 0.8426 - val_loss: 1100.5234 - val_accuracy: 0.8413\nEpoch 107/200\nlearning rate scheduled to 0.00034116616356186566\n610/610 [==============================] - 42s 63ms/step - loss: 1099.6096 - accuracy: 0.8422 - val_loss: 1098.7008 - val_accuracy: 0.8373\nEpoch 108/200\nlearning rate scheduled to 0.00033775449846871195\n610/610 [==============================] - 42s 63ms/step - loss: 1097.7881 - accuracy: 0.8442 - val_loss: 1096.8916 - val_accuracy: 0.8382\nEpoch 109/200\nlearning rate scheduled to 0.0003343769477214664\n610/610 [==============================] - 42s 63ms/step - loss: 1095.9888 - accuracy: 0.8449 - val_loss: 1095.0864 - val_accuracy: 0.8476\nEpoch 110/200\nlearning rate scheduled to 0.0003310331655666232\n610/610 [==============================] - 41s 62ms/step - loss: 1094.2123 - accuracy: 0.8446 - val_loss: 1093.3250 - val_accuracy: 0.8441\nEpoch 111/200\nlearning rate scheduled to 0.00032772283506346864\n610/610 [==============================] - 42s 63ms/step - loss: 1092.4554 - accuracy: 0.8456 - val_loss: 1091.5901 - val_accuracy: 0.8375\nEpoch 112/200\nlearning rate scheduled to 0.00032444561045849693\n610/610 [==============================] - 42s 63ms/step - loss: 1090.7177 - accuracy: 0.8455 - val_loss: 1089.8679 - val_accuracy: 0.8338\nEpoch 113/200\nlearning rate scheduled to 0.00032120114599820226\n610/610 [==============================] - 42s 63ms/step - loss: 1089.0028 - accuracy: 0.8455 - val_loss: 1088.2227 - val_accuracy: 0.8072\nEpoch 114/200\nlearning rate scheduled to 0.00031798912474187093\n610/610 [==============================] - 42s 63ms/step - loss: 1087.3046 - accuracy: 0.8460 - val_loss: 1086.4847 - val_accuracy: 0.8344\nEpoch 115/200\nlearning rate scheduled to 0.0003148092297487892\n610/610 [==============================] - 42s 63ms/step - loss: 1085.6250 - accuracy: 0.8475 - val_loss: 1084.7831 - val_accuracy: 0.8529\nEpoch 116/200\nlearning rate scheduled to 0.0003116611440782435\n610/610 [==============================] - 42s 63ms/step - loss: 1083.9661 - accuracy: 0.8457 - val_loss: 1083.1378 - val_accuracy: 0.8496\nEpoch 117/200\nlearning rate scheduled to 0.000308544521976728\n610/610 [==============================] - 41s 62ms/step - loss: 1082.3269 - accuracy: 0.8474 - val_loss: 1081.5848 - val_accuracy: 0.8068\nEpoch 118/200\nlearning rate scheduled to 0.0003054590753163211\n610/610 [==============================] - 42s 63ms/step - loss: 1080.7068 - accuracy: 0.8471 - val_loss: 1079.9425 - val_accuracy: 0.8206\nEpoch 119/200\nlearning rate scheduled to 0.0003024044871563092\n610/610 [==============================] - 42s 63ms/step - loss: 1079.1039 - accuracy: 0.8488 - val_loss: 1078.3130 - val_accuracy: 0.8419\nEpoch 120/200\nlearning rate scheduled to 0.00029938044055597855\n610/610 [==============================] - 42s 63ms/step - loss: 1077.5222 - accuracy: 0.8490 - val_loss: 1076.7236 - val_accuracy: 0.8556\nEpoch 121/200\nlearning rate scheduled to 0.0002963866473874077\n610/610 [==============================] - 42s 63ms/step - loss: 1075.9584 - accuracy: 0.8479 - val_loss: 1075.1970 - val_accuracy: 0.8334\nEpoch 122/200\nlearning rate scheduled to 0.000293422790709883\n610/610 [==============================] - 42s 63ms/step - loss: 1074.4094 - accuracy: 0.8499 - val_loss: 1073.6345 - val_accuracy: 0.8543\nEpoch 123/200\nlearning rate scheduled to 0.00029048855358269063\n610/610 [==============================] - 42s 63ms/step - loss: 1072.8792 - accuracy: 0.8500 - val_loss: 1072.1644 - val_accuracy: 0.8206\nEpoch 124/200\nlearning rate scheduled to 0.0002875836766907014\n610/610 [==============================] - 42s 63ms/step - loss: 1071.3691 - accuracy: 0.8499 - val_loss: 1070.6438 - val_accuracy: 0.8343\nEpoch 125/200\nlearning rate scheduled to 0.0002847078430932015\n610/610 [==============================] - 41s 62ms/step - loss: 1069.8728 - accuracy: 0.8498 - val_loss: 1069.1395 - val_accuracy: 0.8418\nEpoch 126/200\nlearning rate scheduled to 0.0002818607646622695\n610/610 [==============================] - 42s 63ms/step - loss: 1068.3940 - accuracy: 0.8495 - val_loss: 1067.6678 - val_accuracy: 0.8428\nEpoch 127/200\nlearning rate scheduled to 0.00027904215326998385\n610/610 [==============================] - 42s 63ms/step - loss: 1066.9331 - accuracy: 0.8506 - val_loss: 1066.2031 - val_accuracy: 0.8496\nEpoch 128/200\nlearning rate scheduled to 0.00027625172078842296\n610/610 [==============================] - 42s 63ms/step - loss: 1065.4872 - accuracy: 0.8502 - val_loss: 1064.8131 - val_accuracy: 0.8252\nEpoch 129/200\nlearning rate scheduled to 0.0002734892079024576\n610/610 [==============================] - 42s 63ms/step - loss: 1064.0582 - accuracy: 0.8524 - val_loss: 1063.3521 - val_accuracy: 0.8464\nEpoch 130/200\nlearning rate scheduled to 0.0002707543264841661\n610/610 [==============================] - 41s 62ms/step - loss: 1062.6478 - accuracy: 0.8520 - val_loss: 1061.9620 - val_accuracy: 0.8401\nEpoch 131/200\nlearning rate scheduled to 0.000268046788405627\n610/610 [==============================] - 42s 63ms/step - loss: 1061.2539 - accuracy: 0.8506 - val_loss: 1060.5667 - val_accuracy: 0.8438\nEpoch 132/200\nlearning rate scheduled to 0.000265366334351711\n610/610 [==============================] - 42s 63ms/step - loss: 1059.8719 - accuracy: 0.8519 - val_loss: 1059.2736 - val_accuracy: 0.8019\nEpoch 133/200\nlearning rate scheduled to 0.00026271267619449646\n610/610 [==============================] - 41s 62ms/step - loss: 1058.5066 - accuracy: 0.8533 - val_loss: 1057.8271 - val_accuracy: 0.8525\nEpoch 134/200\nlearning rate scheduled to 0.00026008555461885406\n610/610 [==============================] - 42s 62ms/step - loss: 1057.1593 - accuracy: 0.8527 - val_loss: 1056.4939 - val_accuracy: 0.8471\nEpoch 135/200\nlearning rate scheduled to 0.00025748471030965445\n610/610 [==============================] - 42s 62ms/step - loss: 1055.8246 - accuracy: 0.8537 - val_loss: 1055.1718 - val_accuracy: 0.8444\nEpoch 136/200\nlearning rate scheduled to 0.0002549098551389761\n610/610 [==============================] - 41s 62ms/step - loss: 1054.5040 - accuracy: 0.8531 - val_loss: 1053.8458 - val_accuracy: 0.8531\nEpoch 137/200\nlearning rate scheduled to 0.00025236075860448183\n610/610 [==============================] - 42s 63ms/step - loss: 1053.1976 - accuracy: 0.8550 - val_loss: 1052.5500 - val_accuracy: 0.8526\nEpoch 138/200\nlearning rate scheduled to 0.0002498371613910422\n610/610 [==============================] - 42s 62ms/step - loss: 1051.9077 - accuracy: 0.8542 - val_loss: 1051.2808 - val_accuracy: 0.8414\nEpoch 139/200\nlearning rate scheduled to 0.0002473388041835278\n610/610 [==============================] - 42s 63ms/step - loss: 1050.6322 - accuracy: 0.8551 - val_loss: 1049.9951 - val_accuracy: 0.8534\nEpoch 140/200\nlearning rate scheduled to 0.0002448654276668094\n610/610 [==============================] - 42s 62ms/step - loss: 1049.3719 - accuracy: 0.8539 - val_loss: 1048.7483 - val_accuracy: 0.8500\nEpoch 141/200\nlearning rate scheduled to 0.00024241677252575755\n610/610 [==============================] - 42s 63ms/step - loss: 1048.1259 - accuracy: 0.8541 - val_loss: 1047.5226 - val_accuracy: 0.8443\nEpoch 142/200\nlearning rate scheduled to 0.00023999260825803502\n610/610 [==============================] - 42s 63ms/step - loss: 1046.8923 - accuracy: 0.8553 - val_loss: 1046.2808 - val_accuracy: 0.8526\nEpoch 143/200\nlearning rate scheduled to 0.00023759267554851249\n610/610 [==============================] - 42s 63ms/step - loss: 1045.6752 - accuracy: 0.8552 - val_loss: 1045.0979 - val_accuracy: 0.8377\nEpoch 144/200\nlearning rate scheduled to 0.0002352167438948527\n610/610 [==============================] - 42s 63ms/step - loss: 1044.4696 - accuracy: 0.8553 - val_loss: 1043.8635 - val_accuracy: 0.8566\nEpoch 145/200\nlearning rate scheduled to 0.00023286458279471843\n610/610 [==============================] - 41s 62ms/step - loss: 1043.2773 - accuracy: 0.8560 - val_loss: 1042.6926 - val_accuracy: 0.8461\nEpoch 146/200\nlearning rate scheduled to 0.00023053593293298035\n610/610 [==============================] - 42s 62ms/step - loss: 1042.0984 - accuracy: 0.8562 - val_loss: 1041.5103 - val_accuracy: 0.8563\nEpoch 147/200\nlearning rate scheduled to 0.0002282305782136973\n610/610 [==============================] - 42s 63ms/step - loss: 1040.9335 - accuracy: 0.8560 - val_loss: 1040.3508 - val_accuracy: 0.8587\nEpoch 148/200\nlearning rate scheduled to 0.00022594827372813598\n610/610 [==============================] - 42s 62ms/step - loss: 1039.7791 - accuracy: 0.8561 - val_loss: 1039.2238 - val_accuracy: 0.8438\nEpoch 149/200\nlearning rate scheduled to 0.00022368878897395915\n610/610 [==============================] - 42s 63ms/step - loss: 1038.6377 - accuracy: 0.8562 - val_loss: 1038.0745 - val_accuracy: 0.8540\nEpoch 150/200\nlearning rate scheduled to 0.00022145190785522573\n610/610 [==============================] - 42s 62ms/step - loss: 1037.5084 - accuracy: 0.8564 - val_loss: 1037.0017 - val_accuracy: 0.8244\nEpoch 151/200\nlearning rate scheduled to 0.00021923738546320237\n610/610 [==============================] - 42s 63ms/step - loss: 1036.3940 - accuracy: 0.8564 - val_loss: 1035.8406 - val_accuracy: 0.8538\nEpoch 152/200\nlearning rate scheduled to 0.00021704500570194797\n610/610 [==============================] - 42s 63ms/step - loss: 1035.2878 - accuracy: 0.8582 - val_loss: 1034.7795 - val_accuracy: 0.8336\nEpoch 153/200\nlearning rate scheduled to 0.00021487455247552135\n610/610 [==============================] - 42s 63ms/step - loss: 1034.1986 - accuracy: 0.8585 - val_loss: 1033.6752 - val_accuracy: 0.8450\nEpoch 154/200\nlearning rate scheduled to 0.00021272580968798138\n610/610 [==============================] - 42s 62ms/step - loss: 1033.1208 - accuracy: 0.8589 - val_loss: 1032.6250 - val_accuracy: 0.8319\nEpoch 155/200\nlearning rate scheduled to 0.00021059854683699086\n610/610 [==============================] - 42s 63ms/step - loss: 1032.0563 - accuracy: 0.8583 - val_loss: 1031.5516 - val_accuracy: 0.8415\nEpoch 156/200\nlearning rate scheduled to 0.0002084925622330047\n610/610 [==============================] - 42s 63ms/step - loss: 1031.0018 - accuracy: 0.8577 - val_loss: 1030.4819 - val_accuracy: 0.8548\nEpoch 157/200\nlearning rate scheduled to 0.0002064076397800818\n610/610 [==============================] - 42s 62ms/step - loss: 1029.9581 - accuracy: 0.8598 - val_loss: 1029.4443 - val_accuracy: 0.8558\nEpoch 158/200\nlearning rate scheduled to 0.000204343563382281\n610/610 [==============================] - 42s 63ms/step - loss: 1028.9277 - accuracy: 0.8596 - val_loss: 1028.4395 - val_accuracy: 0.8423\nEpoch 159/200\nlearning rate scheduled to 0.0002023001313500572\n610/610 [==============================] - 42s 63ms/step - loss: 1027.9088 - accuracy: 0.8572 - val_loss: 1027.4178 - val_accuracy: 0.8483\nEpoch 160/200\nlearning rate scheduled to 0.0002002771275874693\n610/610 [==============================] - 42s 62ms/step - loss: 1026.8987 - accuracy: 0.8588 - val_loss: 1026.4012 - val_accuracy: 0.8568\nEpoch 161/200\nlearning rate scheduled to 0.0001982743504049722\n610/610 [==============================] - 42s 63ms/step - loss: 1025.8995 - accuracy: 0.8597 - val_loss: 1025.4055 - val_accuracy: 0.8556\nEpoch 162/200\nlearning rate scheduled to 0.00019629161251941696\n610/610 [==============================] - 42s 63ms/step - loss: 1024.9137 - accuracy: 0.8584 - val_loss: 1024.4316 - val_accuracy: 0.8535\nEpoch 163/200\nlearning rate scheduled to 0.0001943286978348624\n610/610 [==============================] - 42s 63ms/step - loss: 1023.9340 - accuracy: 0.8603 - val_loss: 1023.4388 - val_accuracy: 0.8651\nEpoch 164/200\nlearning rate scheduled to 0.00019238540466176346\n610/610 [==============================] - 42s 63ms/step - loss: 1022.9664 - accuracy: 0.8603 - val_loss: 1022.5065 - val_accuracy: 0.8476\nEpoch 165/200\nlearning rate scheduled to 0.00019046154571697116\n610/610 [==============================] - 42s 63ms/step - loss: 1022.0112 - accuracy: 0.8608 - val_loss: 1021.5336 - val_accuracy: 0.8570\nEpoch 166/200\nlearning rate scheduled to 0.0001885569337173365\n610/610 [==============================] - 42s 62ms/step - loss: 1021.0648 - accuracy: 0.8602 - val_loss: 1020.5953 - val_accuracy: 0.8607\nEpoch 167/200\nlearning rate scheduled to 0.00018667136697331444\n610/610 [==============================] - 42s 62ms/step - loss: 1020.1299 - accuracy: 0.8612 - val_loss: 1019.6843 - val_accuracy: 0.8501\nEpoch 168/200\nlearning rate scheduled to 0.00018480465820175596\n610/610 [==============================] - 41s 62ms/step - loss: 1019.2085 - accuracy: 0.8602 - val_loss: 1018.7473 - val_accuracy: 0.8601\nEpoch 169/200\nlearning rate scheduled to 0.000182956605713116\n610/610 [==============================] - 42s 63ms/step - loss: 1018.2930 - accuracy: 0.8618 - val_loss: 1017.8440 - val_accuracy: 0.8583\nEpoch 170/200\nlearning rate scheduled to 0.00018112703663064166\n610/610 [==============================] - 42s 63ms/step - loss: 1017.3890 - accuracy: 0.8624 - val_loss: 1017.0034 - val_accuracy: 0.8251\nEpoch 171/200\nlearning rate scheduled to 0.00017931576367118395\n610/610 [==============================] - 42s 63ms/step - loss: 1016.4966 - accuracy: 0.8605 - val_loss: 1016.0577 - val_accuracy: 0.8573\nEpoch 172/200\nlearning rate scheduled to 0.0001775225995515939\n610/610 [==============================] - 42s 63ms/step - loss: 1015.6141 - accuracy: 0.8621 - val_loss: 1015.1648 - val_accuracy: 0.8648\nEpoch 173/200\nlearning rate scheduled to 0.00017574737139511854\n610/610 [==============================] - 42s 63ms/step - loss: 1014.7390 - accuracy: 0.8616 - val_loss: 1014.3000 - val_accuracy: 0.8646\nEpoch 174/200\nlearning rate scheduled to 0.00017398989191860892\n610/610 [==============================] - 42s 63ms/step - loss: 1013.8730 - accuracy: 0.8635 - val_loss: 1013.4462 - val_accuracy: 0.8633\nEpoch 175/200\nlearning rate scheduled to 0.00017224998824531213\n610/610 [==============================] - 42s 62ms/step - loss: 1013.0164 - accuracy: 0.8635 - val_loss: 1012.6014 - val_accuracy: 0.8570\nEpoch 176/200\nlearning rate scheduled to 0.00017052748749847522\n610/610 [==============================] - 42s 63ms/step - loss: 1012.1703 - accuracy: 0.8633 - val_loss: 1011.7854 - val_accuracy: 0.8431\nEpoch 177/200\nlearning rate scheduled to 0.00016882221680134535\n610/610 [==============================] - 42s 63ms/step - loss: 1011.3341 - accuracy: 0.8631 - val_loss: 1010.9191 - val_accuracy: 0.8597\nEpoch 178/200\nlearning rate scheduled to 0.00016713398887077346\n610/610 [==============================] - 41s 62ms/step - loss: 1010.5040 - accuracy: 0.8637 - val_loss: 1010.1647 - val_accuracy: 0.8229\nEpoch 179/200\nlearning rate scheduled to 0.00016546264523640276\n610/610 [==============================] - 42s 63ms/step - loss: 1009.6825 - accuracy: 0.8639 - val_loss: 1009.2731 - val_accuracy: 0.8651\nEpoch 180/200\nlearning rate scheduled to 0.00016380801302148028\n610/610 [==============================] - 42s 63ms/step - loss: 1008.8704 - accuracy: 0.8623 - val_loss: 1008.4714 - val_accuracy: 0.8579\nEpoch 181/200\nlearning rate scheduled to 0.00016216993375564926\n610/610 [==============================] - 42s 62ms/step - loss: 1008.0667 - accuracy: 0.8635 - val_loss: 1007.6616 - val_accuracy: 0.8666\nEpoch 182/200\nlearning rate scheduled to 0.00016054823456215672\n610/610 [==============================] - 41s 62ms/step - loss: 1007.2699 - accuracy: 0.8651 - val_loss: 1006.8674 - val_accuracy: 0.8680\nEpoch 183/200\nlearning rate scheduled to 0.00015894275697064586\n610/610 [==============================] - 42s 63ms/step - loss: 1006.4849 - accuracy: 0.8640 - val_loss: 1006.1127 - val_accuracy: 0.8503\nEpoch 184/200\nlearning rate scheduled to 0.00015735332810436374\n610/610 [==============================] - 42s 63ms/step - loss: 1005.7095 - accuracy: 0.8639 - val_loss: 1005.3287 - val_accuracy: 0.8590\nEpoch 185/200\nlearning rate scheduled to 0.00015577978949295356\n610/610 [==============================] - 42s 63ms/step - loss: 1004.9407 - accuracy: 0.8647 - val_loss: 1004.6424 - val_accuracy: 0.8169\nEpoch 186/200\nlearning rate scheduled to 0.00015422199707245453\n610/610 [==============================] - 42s 63ms/step - loss: 1004.1827 - accuracy: 0.8644 - val_loss: 1003.8021 - val_accuracy: 0.8650\nEpoch 187/200\nlearning rate scheduled to 0.00015267977796611375\n610/610 [==============================] - 42s 63ms/step - loss: 1003.4314 - accuracy: 0.8651 - val_loss: 1003.0555 - val_accuracy: 0.8656\nEpoch 188/200\nlearning rate scheduled to 0.00015115297370357439\n610/610 [==============================] - 41s 62ms/step - loss: 1002.6904 - accuracy: 0.8642 - val_loss: 1002.3366 - val_accuracy: 0.8546\nEpoch 189/200\nlearning rate scheduled to 0.00014964144022087568\n610/610 [==============================] - 42s 63ms/step - loss: 1001.9560 - accuracy: 0.8641 - val_loss: 1001.5817 - val_accuracy: 0.8683\nEpoch 190/200\nlearning rate scheduled to 0.00014814501904766075\n610/610 [==============================] - 42s 63ms/step - loss: 1001.2267 - accuracy: 0.8657 - val_loss: 1000.8625 - val_accuracy: 0.8663\nEpoch 191/200\nlearning rate scheduled to 0.00014666356611996888\n610/610 [==============================] - 42s 63ms/step - loss: 1000.5098 - accuracy: 0.8657 - val_loss: 1000.1672 - val_accuracy: 0.8555\nEpoch 192/200\nlearning rate scheduled to 0.00014519693737383933\n610/610 [==============================] - 42s 63ms/step - loss: 999.7970 - accuracy: 0.8659 - val_loss: 999.4362 - val_accuracy: 0.8681\nEpoch 193/200\nlearning rate scheduled to 0.0001437449743389152\n610/610 [==============================] - 42s 63ms/step - loss: 999.0930 - accuracy: 0.8666 - val_loss: 998.7509 - val_accuracy: 0.8621\nEpoch 194/200\nlearning rate scheduled to 0.00014230751854483968\n610/610 [==============================] - 41s 62ms/step - loss: 998.3973 - accuracy: 0.8667 - val_loss: 998.0515 - val_accuracy: 0.8675\nEpoch 195/200\nlearning rate scheduled to 0.00014088444033404812\n610/610 [==============================] - 42s 63ms/step - loss: 997.7075 - accuracy: 0.8669 - val_loss: 997.3656 - val_accuracy: 0.8641\nEpoch 196/200\nlearning rate scheduled to 0.0001394755956425797\n610/610 [==============================] - 42s 63ms/step - loss: 997.0245 - accuracy: 0.8666 - val_loss: 996.6989 - val_accuracy: 0.8563\nEpoch 197/200\nlearning rate scheduled to 0.00013808084040647372\n610/610 [==============================] - 41s 62ms/step - loss: 996.3481 - accuracy: 0.8672 - val_loss: 996.0480 - val_accuracy: 0.8442\nEpoch 198/200\nlearning rate scheduled to 0.00013670003056176939\n610/610 [==============================] - 42s 63ms/step - loss: 995.6803 - accuracy: 0.8672 - val_loss: 995.3484 - val_accuracy: 0.8656\nEpoch 199/200\nlearning rate scheduled to 0.000135333036450902\n610/610 [==============================] - 42s 63ms/step - loss: 995.0185 - accuracy: 0.8656 - val_loss: 994.7195 - val_accuracy: 0.8496\nEpoch 200/200\nlearning rate scheduled to 0.00013397969960351474\n610/610 [==============================] - 42s 62ms/step - loss: 994.3596 - accuracy: 0.8686 - val_loss: 994.0291 - val_accuracy: 0.8730\n"
    }
   ],
   "source": [
    "history_original_siamese_model_2 = original_siamese_model_2.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                         model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "153/153 [==============================] - 7s 20ms/step - loss: 994.0361 - accuracy: 0.8654\n"
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model_2.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Third Run - 150k Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_150k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_150k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 105, 105, 3)]     0         \n_________________________________________________________________\nConv1 (Conv2D)               (None, 96, 96, 64)        19264     \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 48, 48, 64)        0         \n_________________________________________________________________\nConv2 (Conv2D)               (None, 42, 42, 128)       401536    \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 21, 21, 128)       0         \n_________________________________________________________________\nConv3 (Conv2D)               (None, 18, 18, 128)       262272    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n_________________________________________________________________\nConv4 (Conv2D)               (None, 6, 6, 256)         524544    \n_________________________________________________________________\nflatten (Flatten)            (None, 9216)              0         \n_________________________________________________________________\nDense1 (Dense)               (None, 4096)              37752832  \n=================================================================\nTotal params: 38,960,448\nTrainable params: 38,960,448\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_3 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3flsibxx\" target=\"_blank\">toasty-plant-9</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 150k\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "original_siamese_model_3.compile(loss=config.loss_function,\n",
    "                                 optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_150k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/200\nlearning rate scheduled to 0.0009900000470224768\n   6/1014 [..............................] - ETA: 56s - loss: 1510.4865 - accuracy: 0.5651WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0250s vs `on_train_batch_end` time: 0.0266s). Check your callbacks.\n1014/1014 [==============================] - 78s 67ms/step - loss: 1504.4772 - accuracy: 0.5708 - val_loss: 1498.4337 - val_accuracy: 0.5637\nEpoch 2/200\nlearning rate scheduled to 0.000980100086890161\n1014/1014 [==============================] - 71s 64ms/step - loss: 1492.4955 - accuracy: 0.5783 - val_loss: 1486.5610 - val_accuracy: 0.5823\nEpoch 3/200\nlearning rate scheduled to 0.0009702991275116801\n1014/1014 [==============================] - 72s 65ms/step - loss: 1480.7300 - accuracy: 0.5948 - val_loss: 1474.8993 - val_accuracy: 0.6120\nEpoch 4/200\nlearning rate scheduled to 0.0009605961316265165\n1014/1014 [==============================] - 73s 66ms/step - loss: 1469.1669 - accuracy: 0.6262 - val_loss: 1463.4354 - val_accuracy: 0.6393\nEpoch 5/200\nlearning rate scheduled to 0.0009509901772253215\n1014/1014 [==============================] - 72s 65ms/step - loss: 1457.7999 - accuracy: 0.6574 - val_loss: 1452.1571 - val_accuracy: 0.6813\nEpoch 6/200\nlearning rate scheduled to 0.0009414802846731617\n1014/1014 [==============================] - 72s 65ms/step - loss: 1446.6146 - accuracy: 0.7006 - val_loss: 1441.0907 - val_accuracy: 0.7050\nEpoch 7/200\nlearning rate scheduled to 0.0009320654743351042\n1014/1014 [==============================] - 71s 65ms/step - loss: 1435.6454 - accuracy: 0.7197 - val_loss: 1430.2236 - val_accuracy: 0.7165\nEpoch 8/200\nlearning rate scheduled to 0.0009227448242017999\n1014/1014 [==============================] - 71s 65ms/step - loss: 1424.8723 - accuracy: 0.7316 - val_loss: 1419.5322 - val_accuracy: 0.7320\nEpoch 9/200\nlearning rate scheduled to 0.0009135173546383158\n1014/1014 [==============================] - 72s 66ms/step - loss: 1414.2921 - accuracy: 0.7409 - val_loss: 1409.0358 - val_accuracy: 0.7516\nEpoch 10/200\nlearning rate scheduled to 0.0009043822012608871\n1014/1014 [==============================] - 71s 64ms/step - loss: 1403.9670 - accuracy: 0.6635 - val_loss: 1398.9020 - val_accuracy: 0.5894\nEpoch 11/200\nlearning rate scheduled to 0.0008953383844345808\n1014/1014 [==============================] - 71s 64ms/step - loss: 1393.8195 - accuracy: 0.6391 - val_loss: 1388.7133 - val_accuracy: 0.6977\nEpoch 12/200\nlearning rate scheduled to 0.000886384982150048\n1014/1014 [==============================] - 71s 64ms/step - loss: 1383.7073 - accuracy: 0.7172 - val_loss: 1378.7113 - val_accuracy: 0.7345\nEpoch 13/200\nlearning rate scheduled to 0.0008775211300235242\n1014/1014 [==============================] - 71s 64ms/step - loss: 1373.8098 - accuracy: 0.7449 - val_loss: 1368.9026 - val_accuracy: 0.7535\nEpoch 14/200\nlearning rate scheduled to 0.0008687459060456604\n1014/1014 [==============================] - 70s 64ms/step - loss: 1364.0985 - accuracy: 0.7550 - val_loss: 1359.2681 - val_accuracy: 0.7731\nEpoch 15/200\nlearning rate scheduled to 0.0008600584458326921\n1014/1014 [==============================] - 69s 63ms/step - loss: 1354.5739 - accuracy: 0.7434 - val_loss: 1350.0337 - val_accuracy: 0.5164\nEpoch 16/200\nlearning rate scheduled to 0.0008514578850008547\n1014/1014 [==============================] - 70s 63ms/step - loss: 1345.3564 - accuracy: 0.5978 - val_loss: 1340.6899 - val_accuracy: 0.6348\nEpoch 17/200\nlearning rate scheduled to 0.0008429433015407995\n1014/1014 [==============================] - 69s 63ms/step - loss: 1336.0804 - accuracy: 0.6870 - val_loss: 1331.4780 - val_accuracy: 0.7151\nEpoch 18/200\nlearning rate scheduled to 0.0008345138886943459\n1014/1014 [==============================] - 74s 68ms/step - loss: 1326.9739 - accuracy: 0.7282 - val_loss: 1322.4725 - val_accuracy: 0.7401\nEpoch 19/200\nlearning rate scheduled to 0.0008261687244521454\n1014/1014 [==============================] - 75s 68ms/step - loss: 1318.0491 - accuracy: 0.7397 - val_loss: 1313.6178 - val_accuracy: 0.7480\nEpoch 20/200\nlearning rate scheduled to 0.0008179070596816018\n1014/1014 [==============================] - 74s 68ms/step - loss: 1309.2700 - accuracy: 0.7530 - val_loss: 1304.9371 - val_accuracy: 0.7447\nEpoch 21/200\nlearning rate scheduled to 0.0008097279723733664\n1014/1014 [==============================] - 74s 68ms/step - loss: 1300.6346 - accuracy: 0.7692 - val_loss: 1296.3755 - val_accuracy: 0.7631\nEpoch 22/200\nlearning rate scheduled to 0.000801630713394843\n1014/1014 [==============================] - 74s 68ms/step - loss: 1292.1501 - accuracy: 0.7815 - val_loss: 1287.9397 - val_accuracy: 0.7832\nEpoch 23/200\nlearning rate scheduled to 0.0007936144183622674\n1014/1014 [==============================] - 75s 68ms/step - loss: 1283.8434 - accuracy: 0.7588 - val_loss: 1279.7241 - val_accuracy: 0.7559\nEpoch 24/200\nlearning rate scheduled to 0.0007856782805174589\n1014/1014 [==============================] - 75s 68ms/step - loss: 1275.6472 - accuracy: 0.7666 - val_loss: 1271.5723 - val_accuracy: 0.7795\nEpoch 25/200\nlearning rate scheduled to 0.0007778214931022376\n1014/1014 [==============================] - 75s 68ms/step - loss: 1267.5723 - accuracy: 0.7761 - val_loss: 1263.6071 - val_accuracy: 0.7497\nEpoch 26/200\nlearning rate scheduled to 0.0007700433069840073\n1014/1014 [==============================] - 75s 68ms/step - loss: 1259.6334 - accuracy: 0.7819 - val_loss: 1255.6895 - val_accuracy: 0.7894\nEpoch 27/200\nlearning rate scheduled to 0.0007623428577790037\n1014/1014 [==============================] - 75s 68ms/step - loss: 1251.8252 - accuracy: 0.7836 - val_loss: 1247.9434 - val_accuracy: 0.7876\nEpoch 28/200\nlearning rate scheduled to 0.0007547194539802149\n1014/1014 [==============================] - 75s 68ms/step - loss: 1244.1373 - accuracy: 0.7905 - val_loss: 1240.3503 - val_accuracy: 0.7794\nEpoch 29/200\nlearning rate scheduled to 0.0007471722312038764\n1014/1014 [==============================] - 75s 68ms/step - loss: 1236.5779 - accuracy: 0.7943 - val_loss: 1232.8340 - val_accuracy: 0.7886\nEpoch 30/200\nlearning rate scheduled to 0.0007397004979429766\n1014/1014 [==============================] - 75s 68ms/step - loss: 1229.1399 - accuracy: 0.7979 - val_loss: 1225.4417 - val_accuracy: 0.8042\nEpoch 31/200\nlearning rate scheduled to 0.0007323035050649196\n1014/1014 [==============================] - 75s 68ms/step - loss: 1221.8214 - accuracy: 0.8007 - val_loss: 1218.1849 - val_accuracy: 0.8025\nEpoch 32/200\nlearning rate scheduled to 0.000724980445811525\n1014/1014 [==============================] - 75s 68ms/step - loss: 1214.6183 - accuracy: 0.8042 - val_loss: 1211.0380 - val_accuracy: 0.8097\nEpoch 33/200\nlearning rate scheduled to 0.0007177306286757812\n1014/1014 [==============================] - 75s 68ms/step - loss: 1207.5299 - accuracy: 0.8056 - val_loss: 1204.0083 - val_accuracy: 0.8110\nEpoch 34/200\nlearning rate scheduled to 0.0007105533045250923\n1014/1014 [==============================] - 74s 68ms/step - loss: 1200.5544 - accuracy: 0.8073 - val_loss: 1197.0896 - val_accuracy: 0.8080\nEpoch 35/200\nlearning rate scheduled to 0.0007034477818524464\n1014/1014 [==============================] - 75s 68ms/step - loss: 1193.6887 - accuracy: 0.8077 - val_loss: 1190.2687 - val_accuracy: 0.8182\nEpoch 36/200\nlearning rate scheduled to 0.000696413311525248\n1014/1014 [==============================] - 75s 68ms/step - loss: 1186.9286 - accuracy: 0.8113 - val_loss: 1183.5688 - val_accuracy: 0.8165\nEpoch 37/200\nlearning rate scheduled to 0.0006894492020364851\n1014/1014 [==============================] - 74s 68ms/step - loss: 1180.2737 - accuracy: 0.8134 - val_loss: 1176.9691 - val_accuracy: 0.8170\nEpoch 38/200\nlearning rate scheduled to 0.0006825547042535618\n1014/1014 [==============================] - 74s 68ms/step - loss: 1173.7222 - accuracy: 0.8163 - val_loss: 1170.4789 - val_accuracy: 0.8085\nEpoch 39/200\nlearning rate scheduled to 0.0006757291842950508\n1014/1014 [==============================] - 74s 68ms/step - loss: 1167.2745 - accuracy: 0.8177 - val_loss: 1164.0692 - val_accuracy: 0.8248\nEpoch 40/200\nlearning rate scheduled to 0.0006689718930283561\n1014/1014 [==============================] - 74s 68ms/step - loss: 1160.9229 - accuracy: 0.8215 - val_loss: 1157.7673 - val_accuracy: 0.8229\nEpoch 41/200\nlearning rate scheduled to 0.0006622821965720505\n1014/1014 [==============================] - 74s 68ms/step - loss: 1154.6674 - accuracy: 0.8240 - val_loss: 1151.5699 - val_accuracy: 0.8199\nEpoch 42/200\nlearning rate scheduled to 0.0006556594034191221\n1014/1014 [==============================] - 75s 68ms/step - loss: 1148.5111 - accuracy: 0.8269 - val_loss: 1145.4685 - val_accuracy: 0.8201\nEpoch 43/200\nlearning rate scheduled to 0.0006491028220625594\n1014/1014 [==============================] - 75s 68ms/step - loss: 1142.4465 - accuracy: 0.8300 - val_loss: 1139.4395 - val_accuracy: 0.8269\nEpoch 44/200\nlearning rate scheduled to 0.0006426118186209351\n1014/1014 [==============================] - 74s 68ms/step - loss: 1136.4751 - accuracy: 0.8332 - val_loss: 1133.5186 - val_accuracy: 0.8264\nEpoch 45/200\nlearning rate scheduled to 0.0006361857015872375\n1014/1014 [==============================] - 74s 68ms/step - loss: 1130.5952 - accuracy: 0.8348 - val_loss: 1127.6783 - val_accuracy: 0.8352\nEpoch 46/200\nlearning rate scheduled to 0.0006298238370800391\n1014/1014 [==============================] - 74s 68ms/step - loss: 1124.8022 - accuracy: 0.8371 - val_loss: 1121.9307 - val_accuracy: 0.8340\nEpoch 47/200\nlearning rate scheduled to 0.0006235255912179127\n1014/1014 [==============================] - 74s 68ms/step - loss: 1119.0985 - accuracy: 0.8388 - val_loss: 1116.2787 - val_accuracy: 0.8328\nEpoch 48/200\nlearning rate scheduled to 0.000617290330119431\n1014/1014 [==============================] - 75s 68ms/step - loss: 1113.4799 - accuracy: 0.8386 - val_loss: 1110.6912 - val_accuracy: 0.8399\nEpoch 49/200\nlearning rate scheduled to 0.0006111174199031666\n1014/1014 [==============================] - 74s 68ms/step - loss: 1107.9452 - accuracy: 0.8407 - val_loss: 1105.2023 - val_accuracy: 0.8376\nEpoch 50/200\nlearning rate scheduled to 0.0006050062266876921\n1014/1014 [==============================] - 74s 68ms/step - loss: 1102.4932 - accuracy: 0.8425 - val_loss: 1099.7854 - val_accuracy: 0.8450\nEpoch 51/200\nlearning rate scheduled to 0.0005989561742171646\n1014/1014 [==============================] - 74s 68ms/step - loss: 1097.1211 - accuracy: 0.8438 - val_loss: 1094.4706 - val_accuracy: 0.8323\nEpoch 52/200\nlearning rate scheduled to 0.0005929666286101564\n1014/1014 [==============================] - 74s 68ms/step - loss: 1091.8301 - accuracy: 0.8439 - val_loss: 1089.2024 - val_accuracy: 0.8485\nEpoch 53/200\nlearning rate scheduled to 0.0005870369559852406\n1014/1014 [==============================] - 74s 68ms/step - loss: 1086.6177 - accuracy: 0.8465 - val_loss: 1084.0308 - val_accuracy: 0.8454\nEpoch 54/200\nlearning rate scheduled to 0.000581166580086574\n1014/1014 [==============================] - 74s 68ms/step - loss: 1081.4801 - accuracy: 0.8465 - val_loss: 1078.9292 - val_accuracy: 0.8485\nEpoch 55/200\nlearning rate scheduled to 0.0005753549246583134\n1014/1014 [==============================] - 74s 68ms/step - loss: 1076.4193 - accuracy: 0.8477 - val_loss: 1073.9215 - val_accuracy: 0.8378\nEpoch 56/200\nlearning rate scheduled to 0.0005696013558190316\n1014/1014 [==============================] - 74s 68ms/step - loss: 1071.4309 - accuracy: 0.8494 - val_loss: 1068.9554 - val_accuracy: 0.8512\nEpoch 57/200\nlearning rate scheduled to 0.0005639053549384699\n1014/1014 [==============================] - 74s 68ms/step - loss: 1066.5167 - accuracy: 0.8507 - val_loss: 1064.0715 - val_accuracy: 0.8529\nEpoch 58/200\nlearning rate scheduled to 0.0005582662881352008\n1014/1014 [==============================] - 74s 68ms/step - loss: 1061.6744 - accuracy: 0.8500 - val_loss: 1059.2739 - val_accuracy: 0.8475\nEpoch 59/200\nlearning rate scheduled to 0.0005526836367789656\n1014/1014 [==============================] - 74s 68ms/step - loss: 1056.8994 - accuracy: 0.8518 - val_loss: 1054.5272 - val_accuracy: 0.8546\nEpoch 60/200\nlearning rate scheduled to 0.0005471568246139213\n1014/1014 [==============================] - 74s 68ms/step - loss: 1052.1951 - accuracy: 0.8527 - val_loss: 1049.8602 - val_accuracy: 0.8509\nEpoch 61/200\nlearning rate scheduled to 0.000541685275384225\n1014/1014 [==============================] - 75s 68ms/step - loss: 1047.5571 - accuracy: 0.8538 - val_loss: 1045.2549 - val_accuracy: 0.8552\nEpoch 62/200\nlearning rate scheduled to 0.0005362684128340334\n1014/1014 [==============================] - 75s 68ms/step - loss: 1042.9888 - accuracy: 0.8536 - val_loss: 1040.7212 - val_accuracy: 0.8519\nEpoch 63/200\nlearning rate scheduled to 0.0005309057183330878\n1014/1014 [==============================] - 75s 69ms/step - loss: 1038.4867 - accuracy: 0.8538 - val_loss: 1036.2417 - val_accuracy: 0.8592\nEpoch 64/200\nlearning rate scheduled to 0.0005255966732511297\n1014/1014 [==============================] - 72s 65ms/step - loss: 1034.0438 - accuracy: 0.8559 - val_loss: 1031.8309 - val_accuracy: 0.8609\nEpoch 65/200\nlearning rate scheduled to 0.0005203407013323158\n1014/1014 [==============================] - 71s 64ms/step - loss: 1029.6655 - accuracy: 0.8566 - val_loss: 1027.4863 - val_accuracy: 0.8603\nEpoch 66/200\nlearning rate scheduled to 0.0005151372839463875\n1014/1014 [==============================] - 71s 64ms/step - loss: 1025.3506 - accuracy: 0.8573 - val_loss: 1023.2032 - val_accuracy: 0.8595\nEpoch 67/200\nlearning rate scheduled to 0.0005099859024630859\n1014/1014 [==============================] - 70s 63ms/step - loss: 1021.0964 - accuracy: 0.8581 - val_loss: 1018.9791 - val_accuracy: 0.8616\nEpoch 68/200\nlearning rate scheduled to 0.0005048860382521525\n1014/1014 [==============================] - 71s 64ms/step - loss: 1016.9006 - accuracy: 0.8599 - val_loss: 1014.8170 - val_accuracy: 0.8605\nEpoch 69/200\nlearning rate scheduled to 0.0004998371726833284\n1014/1014 [==============================] - 70s 63ms/step - loss: 1012.7662 - accuracy: 0.8593 - val_loss: 1010.7083 - val_accuracy: 0.8643\nEpoch 70/200\nlearning rate scheduled to 0.0004948387871263549\n1014/1014 [==============================] - 70s 64ms/step - loss: 1008.6887 - accuracy: 0.8605 - val_loss: 1006.6597 - val_accuracy: 0.8623\nEpoch 71/200\nlearning rate scheduled to 0.0004898904205765575\n1014/1014 [==============================] - 70s 64ms/step - loss: 1004.6686 - accuracy: 0.8616 - val_loss: 1002.6678 - val_accuracy: 0.8641\nEpoch 72/200\nlearning rate scheduled to 0.00048499149677809326\n1014/1014 [==============================] - 70s 64ms/step - loss: 1000.7032 - accuracy: 0.8612 - val_loss: 998.7258 - val_accuracy: 0.8664\nEpoch 73/200\nlearning rate scheduled to 0.00048014158353907986\n1014/1014 [==============================] - 71s 64ms/step - loss: 996.7946 - accuracy: 0.8619 - val_loss: 994.8551 - val_accuracy: 0.8596\nEpoch 74/200\nlearning rate scheduled to 0.00047534016222925855\n1014/1014 [==============================] - 70s 64ms/step - loss: 992.9402 - accuracy: 0.8626 - val_loss: 991.0275 - val_accuracy: 0.8623\nEpoch 75/200\nlearning rate scheduled to 0.0004705867718439549\n1014/1014 [==============================] - 70s 64ms/step - loss: 989.1367 - accuracy: 0.8645 - val_loss: 987.2415 - val_accuracy: 0.8694\nEpoch 76/200\nlearning rate scheduled to 0.0004658808937529102\n1014/1014 [==============================] - 69s 63ms/step - loss: 985.3867 - accuracy: 0.8643 - val_loss: 983.5361 - val_accuracy: 0.8596\nEpoch 77/200\nlearning rate scheduled to 0.0004612220957642421\n1014/1014 [==============================] - 69s 62ms/step - loss: 981.6877 - accuracy: 0.8647 - val_loss: 979.8519 - val_accuracy: 0.8632\nEpoch 78/200\nlearning rate scheduled to 0.00045660988806048406\n1014/1014 [==============================] - 69s 62ms/step - loss: 978.0406 - accuracy: 0.8648 - val_loss: 976.2250 - val_accuracy: 0.8671\nEpoch 79/200\nlearning rate scheduled to 0.0004520437808241695\n1014/1014 [==============================] - 69s 62ms/step - loss: 974.4409 - accuracy: 0.8671 - val_loss: 972.6536 - val_accuracy: 0.8659\nEpoch 80/200\nlearning rate scheduled to 0.0004475233418634161\n1014/1014 [==============================] - 68s 62ms/step - loss: 970.8956 - accuracy: 0.8658 - val_loss: 969.1296 - val_accuracy: 0.8673\nEpoch 81/200\nlearning rate scheduled to 0.0004430481101735495\n1014/1014 [==============================] - 68s 62ms/step - loss: 967.3931 - accuracy: 0.8674 - val_loss: 965.6502 - val_accuracy: 0.8672\nEpoch 82/200\nlearning rate scheduled to 0.0004386176247498952\n1014/1014 [==============================] - 69s 62ms/step - loss: 963.9427 - accuracy: 0.8669 - val_loss: 962.2278 - val_accuracy: 0.8676\nEpoch 83/200\nlearning rate scheduled to 0.00043423145340057087\n1014/1014 [==============================] - 69s 62ms/step - loss: 960.5339 - accuracy: 0.8677 - val_loss: 958.8486 - val_accuracy: 0.8634\nEpoch 84/200\nlearning rate scheduled to 0.0004298891351209022\n1014/1014 [==============================] - 68s 62ms/step - loss: 957.1729 - accuracy: 0.8684 - val_loss: 955.5078 - val_accuracy: 0.8642\nEpoch 85/200\nlearning rate scheduled to 0.00042559023771900686\n1014/1014 [==============================] - 69s 62ms/step - loss: 953.8573 - accuracy: 0.8693 - val_loss: 952.2130 - val_accuracy: 0.8666\nEpoch 86/200\nlearning rate scheduled to 0.0004213343290030025\n1014/1014 [==============================] - 68s 62ms/step - loss: 950.5880 - accuracy: 0.8687 - val_loss: 948.9631 - val_accuracy: 0.8682\nEpoch 87/200\nlearning rate scheduled to 0.00041712097678100687\n1014/1014 [==============================] - 68s 61ms/step - loss: 947.3603 - accuracy: 0.8704 - val_loss: 945.7653 - val_accuracy: 0.8643\nEpoch 88/200\nlearning rate scheduled to 0.00041294977767392994\n1014/1014 [==============================] - 67s 61ms/step - loss: 944.1764 - accuracy: 0.8710 - val_loss: 942.5887 - val_accuracy: 0.8726\nEpoch 89/200\nlearning rate scheduled to 0.0004088202706770971\n1014/1014 [==============================] - 74s 68ms/step - loss: 941.0359 - accuracy: 0.8707 - val_loss: 939.4701 - val_accuracy: 0.8729\nEpoch 90/200\nlearning rate scheduled to 0.00040473208122421056\n1014/1014 [==============================] - 74s 68ms/step - loss: 937.9357 - accuracy: 0.8711 - val_loss: 936.3916 - val_accuracy: 0.8739\nEpoch 91/200\nlearning rate scheduled to 0.0004006847483105957\n1014/1014 [==============================] - 74s 68ms/step - loss: 934.8757 - accuracy: 0.8716 - val_loss: 933.3459 - val_accuracy: 0.8758\nEpoch 92/200\nlearning rate scheduled to 0.0003966778973699547\n1014/1014 [==============================] - 74s 68ms/step - loss: 931.8575 - accuracy: 0.8720 - val_loss: 930.3597 - val_accuracy: 0.8730\nEpoch 93/200\nlearning rate scheduled to 0.0003927111250231974\n1014/1014 [==============================] - 74s 68ms/step - loss: 928.8773 - accuracy: 0.8734 - val_loss: 927.4009 - val_accuracy: 0.8716\nEpoch 94/200\nlearning rate scheduled to 0.0003887840278912336\n1014/1014 [==============================] - 74s 68ms/step - loss: 925.9393 - accuracy: 0.8735 - val_loss: 924.4770 - val_accuracy: 0.8729\nEpoch 95/200\nlearning rate scheduled to 0.000384896173782181\n1014/1014 [==============================] - 74s 68ms/step - loss: 923.0392 - accuracy: 0.8740 - val_loss: 921.5980 - val_accuracy: 0.8732\nEpoch 96/200\nlearning rate scheduled to 0.00038104721694253384\n1014/1014 [==============================] - 74s 68ms/step - loss: 920.1770 - accuracy: 0.8733 - val_loss: 918.7554 - val_accuracy: 0.8730\nEpoch 97/200\nlearning rate scheduled to 0.000377236753993202\n1014/1014 [==============================] - 74s 68ms/step - loss: 917.3518 - accuracy: 0.8744 - val_loss: 915.9442 - val_accuracy: 0.8758\nEpoch 98/200\nlearning rate scheduled to 0.0003734643815550953\n1014/1014 [==============================] - 75s 68ms/step - loss: 914.5611 - accuracy: 0.8736 - val_loss: 913.1732 - val_accuracy: 0.8743\nEpoch 99/200\nlearning rate scheduled to 0.0003697297250619158\n1014/1014 [==============================] - 74s 68ms/step - loss: 911.8071 - accuracy: 0.8758 - val_loss: 910.4343 - val_accuracy: 0.8771\nEpoch 100/200\nlearning rate scheduled to 0.0003660324387601577\n1014/1014 [==============================] - 75s 68ms/step - loss: 909.0903 - accuracy: 0.8749 - val_loss: 907.7424 - val_accuracy: 0.8735\nEpoch 101/200\nlearning rate scheduled to 0.00036237211927073074\n1014/1014 [==============================] - 75s 68ms/step - loss: 906.4101 - accuracy: 0.8752 - val_loss: 905.0762 - val_accuracy: 0.8777\nEpoch 102/200\nlearning rate scheduled to 0.0003587483920273371\n1014/1014 [==============================] - 74s 68ms/step - loss: 903.7634 - accuracy: 0.8765 - val_loss: 902.4498 - val_accuracy: 0.8748\nEpoch 103/200\nlearning rate scheduled to 0.0003551609112764709\n1014/1014 [==============================] - 75s 68ms/step - loss: 901.1505 - accuracy: 0.8767 - val_loss: 899.8575 - val_accuracy: 0.8744\nEpoch 104/200\nlearning rate scheduled to 0.0003516093024518341\n1014/1014 [==============================] - 75s 68ms/step - loss: 898.5721 - accuracy: 0.8763 - val_loss: 897.2979 - val_accuracy: 0.8730\nEpoch 105/200\nlearning rate scheduled to 0.0003480932197999209\n1014/1014 [==============================] - 75s 68ms/step - loss: 896.0246 - accuracy: 0.8774 - val_loss: 894.7598 - val_accuracy: 0.8764\nEpoch 106/200\nlearning rate scheduled to 0.0003446122887544334\n1014/1014 [==============================] - 75s 68ms/step - loss: 893.5126 - accuracy: 0.8766 - val_loss: 892.2669 - val_accuracy: 0.8746\nEpoch 107/200\nlearning rate scheduled to 0.00034116616356186566\n1014/1014 [==============================] - 75s 68ms/step - loss: 891.0281 - accuracy: 0.8781 - val_loss: 889.8167 - val_accuracy: 0.8651\nEpoch 108/200\nlearning rate scheduled to 0.00033775449846871195\n1014/1014 [==============================] - 74s 68ms/step - loss: 888.5771 - accuracy: 0.8785 - val_loss: 887.3527 - val_accuracy: 0.8823\nEpoch 109/200\nlearning rate scheduled to 0.0003343769477214664\n1014/1014 [==============================] - 75s 68ms/step - loss: 886.1590 - accuracy: 0.8785 - val_loss: 884.9501 - val_accuracy: 0.8814\nEpoch 110/200\nlearning rate scheduled to 0.0003310331655666232\n1014/1014 [==============================] - 74s 68ms/step - loss: 883.7710 - accuracy: 0.8788 - val_loss: 882.5873 - val_accuracy: 0.8770\nEpoch 111/200\nlearning rate scheduled to 0.00032772283506346864\n1014/1014 [==============================] - 74s 68ms/step - loss: 881.4131 - accuracy: 0.8795 - val_loss: 880.2445 - val_accuracy: 0.8778\nEpoch 112/200\nlearning rate scheduled to 0.00032444561045849693\n1014/1014 [==============================] - 75s 68ms/step - loss: 879.0873 - accuracy: 0.8800 - val_loss: 877.9219 - val_accuracy: 0.8824\nEpoch 113/200\nlearning rate scheduled to 0.00032120114599820226\n1014/1014 [==============================] - 74s 68ms/step - loss: 876.7881 - accuracy: 0.8790 - val_loss: 875.6371 - val_accuracy: 0.8826\nEpoch 114/200\nlearning rate scheduled to 0.00031798912474187093\n1014/1014 [==============================] - 75s 68ms/step - loss: 874.5186 - accuracy: 0.8801 - val_loss: 873.3893 - val_accuracy: 0.8802\nEpoch 115/200\nlearning rate scheduled to 0.0003148092297487892\n1014/1014 [==============================] - 75s 68ms/step - loss: 872.2761 - accuracy: 0.8806 - val_loss: 871.1566 - val_accuracy: 0.8823\nEpoch 116/200\nlearning rate scheduled to 0.0003116611440782435\n1014/1014 [==============================] - 74s 68ms/step - loss: 870.0620 - accuracy: 0.8811 - val_loss: 868.9590 - val_accuracy: 0.8814\nEpoch 117/200\nlearning rate scheduled to 0.000308544521976728\n1014/1014 [==============================] - 74s 68ms/step - loss: 867.8752 - accuracy: 0.8813 - val_loss: 866.7902 - val_accuracy: 0.8800\nEpoch 118/200\nlearning rate scheduled to 0.0003054590753163211\n1014/1014 [==============================] - 74s 68ms/step - loss: 865.7159 - accuracy: 0.8814 - val_loss: 864.6364 - val_accuracy: 0.8833\nEpoch 119/200\nlearning rate scheduled to 0.0003024044871563092\n1014/1014 [==============================] - 74s 68ms/step - loss: 863.5844 - accuracy: 0.8821 - val_loss: 862.5212 - val_accuracy: 0.8839\nEpoch 120/200\nlearning rate scheduled to 0.00029938044055597855\n1014/1014 [==============================] - 74s 68ms/step - loss: 861.4811 - accuracy: 0.8819 - val_loss: 860.4346 - val_accuracy: 0.8803\nEpoch 121/200\nlearning rate scheduled to 0.0002963866473874077\n1014/1014 [==============================] - 74s 68ms/step - loss: 859.4014 - accuracy: 0.8825 - val_loss: 858.3678 - val_accuracy: 0.8826\nEpoch 122/200\nlearning rate scheduled to 0.000293422790709883\n1014/1014 [==============================] - 74s 67ms/step - loss: 857.3497 - accuracy: 0.8822 - val_loss: 856.3292 - val_accuracy: 0.8829\nEpoch 123/200\nlearning rate scheduled to 0.00029048855358269063\n1014/1014 [==============================] - 74s 68ms/step - loss: 855.3219 - accuracy: 0.8831 - val_loss: 854.3126 - val_accuracy: 0.8838\nEpoch 124/200\nlearning rate scheduled to 0.0002875836766907014\n1014/1014 [==============================] - 74s 68ms/step - loss: 853.3188 - accuracy: 0.8839 - val_loss: 852.3250 - val_accuracy: 0.8808\nEpoch 125/200\nlearning rate scheduled to 0.0002847078430932015\n1014/1014 [==============================] - 74s 68ms/step - loss: 851.3395 - accuracy: 0.8832 - val_loss: 850.3611 - val_accuracy: 0.8814\nEpoch 126/200\nlearning rate scheduled to 0.0002818607646622695\n1014/1014 [==============================] - 75s 68ms/step - loss: 849.3860 - accuracy: 0.8829 - val_loss: 848.4164 - val_accuracy: 0.8809\nEpoch 127/200\nlearning rate scheduled to 0.00027904215326998385\n1014/1014 [==============================] - 74s 67ms/step - loss: 847.4531 - accuracy: 0.8847 - val_loss: 846.4919 - val_accuracy: 0.8847\nEpoch 128/200\nlearning rate scheduled to 0.00027625172078842296\n1014/1014 [==============================] - 68s 62ms/step - loss: 845.5479 - accuracy: 0.8839 - val_loss: 844.6005 - val_accuracy: 0.8837\nEpoch 129/200\nlearning rate scheduled to 0.0002734892079024576\n1014/1014 [==============================] - 67s 61ms/step - loss: 843.6643 - accuracy: 0.8844 - val_loss: 842.7239 - val_accuracy: 0.8872\nEpoch 130/200\nlearning rate scheduled to 0.0002707543264841661\n1014/1014 [==============================] - 67s 61ms/step - loss: 841.8054 - accuracy: 0.8846 - val_loss: 840.8839 - val_accuracy: 0.8837\nEpoch 131/200\nlearning rate scheduled to 0.000268046788405627\n1014/1014 [==============================] - 68s 61ms/step - loss: 839.9678 - accuracy: 0.8847 - val_loss: 839.0527 - val_accuracy: 0.8843\nEpoch 132/200\nlearning rate scheduled to 0.000265366334351711\n1014/1014 [==============================] - 68s 61ms/step - loss: 838.1542 - accuracy: 0.8848 - val_loss: 837.2480 - val_accuracy: 0.8861\nEpoch 133/200\nlearning rate scheduled to 0.00026271267619449646\n1014/1014 [==============================] - 68s 61ms/step - loss: 836.3619 - accuracy: 0.8852 - val_loss: 835.4756 - val_accuracy: 0.8836\nEpoch 134/200\nlearning rate scheduled to 0.00026008555461885406\n1014/1014 [==============================] - 67s 61ms/step - loss: 834.5905 - accuracy: 0.8857 - val_loss: 833.7048 - val_accuracy: 0.8883\nEpoch 135/200\nlearning rate scheduled to 0.00025748471030965445\n1014/1014 [==============================] - 68s 61ms/step - loss: 832.8409 - accuracy: 0.8858 - val_loss: 831.9641 - val_accuracy: 0.8906\nEpoch 136/200\nlearning rate scheduled to 0.0002549098551389761\n1014/1014 [==============================] - 67s 61ms/step - loss: 831.1101 - accuracy: 0.8861 - val_loss: 830.2487 - val_accuracy: 0.8866\nEpoch 137/200\nlearning rate scheduled to 0.00025236075860448183\n1014/1014 [==============================] - 67s 61ms/step - loss: 829.4006 - accuracy: 0.8864 - val_loss: 828.5543 - val_accuracy: 0.8836\nEpoch 138/200\nlearning rate scheduled to 0.0002498371613910422\n1014/1014 [==============================] - 68s 61ms/step - loss: 827.7115 - accuracy: 0.8870 - val_loss: 826.8689 - val_accuracy: 0.8873\nEpoch 139/200\nlearning rate scheduled to 0.0002473388041835278\n1014/1014 [==============================] - 68s 61ms/step - loss: 826.0449 - accuracy: 0.8872 - val_loss: 825.2180 - val_accuracy: 0.8844\nEpoch 140/200\nlearning rate scheduled to 0.0002448654276668094\n1014/1014 [==============================] - 67s 61ms/step - loss: 824.3961 - accuracy: 0.8877 - val_loss: 823.5811 - val_accuracy: 0.8846\nEpoch 141/200\nlearning rate scheduled to 0.00024241677252575755\n1014/1014 [==============================] - 67s 61ms/step - loss: 822.7702 - accuracy: 0.8878 - val_loss: 821.9577 - val_accuracy: 0.8919\nEpoch 142/200\nlearning rate scheduled to 0.00023999260825803502\n1014/1014 [==============================] - 67s 61ms/step - loss: 821.1635 - accuracy: 0.8879 - val_loss: 820.3641 - val_accuracy: 0.8872\nEpoch 143/200\nlearning rate scheduled to 0.00023759267554851249\n1014/1014 [==============================] - 67s 61ms/step - loss: 819.5764 - accuracy: 0.8877 - val_loss: 818.7906 - val_accuracy: 0.8861\nEpoch 144/200\nlearning rate scheduled to 0.0002352167438948527\n1014/1014 [==============================] - 67s 61ms/step - loss: 818.0060 - accuracy: 0.8874 - val_loss: 817.2237 - val_accuracy: 0.8888\nEpoch 145/200\nlearning rate scheduled to 0.00023286458279471843\n1014/1014 [==============================] - 67s 60ms/step - loss: 816.4547 - accuracy: 0.8882 - val_loss: 815.6844 - val_accuracy: 0.8890\nEpoch 146/200\nlearning rate scheduled to 0.00023053593293298035\n1014/1014 [==============================] - 74s 68ms/step - loss: 814.9223 - accuracy: 0.8889 - val_loss: 814.1595 - val_accuracy: 0.8867\nEpoch 147/200\nlearning rate scheduled to 0.0002282305782136973\n1014/1014 [==============================] - 74s 68ms/step - loss: 813.4076 - accuracy: 0.8882 - val_loss: 812.6544 - val_accuracy: 0.8884\nEpoch 148/200\nlearning rate scheduled to 0.00022594827372813598\n1014/1014 [==============================] - 74s 68ms/step - loss: 811.9092 - accuracy: 0.8890 - val_loss: 811.1645 - val_accuracy: 0.8884\nEpoch 149/200\nlearning rate scheduled to 0.00022368878897395915\n1014/1014 [==============================] - 75s 68ms/step - loss: 810.4279 - accuracy: 0.8893 - val_loss: 809.6944 - val_accuracy: 0.8897\nEpoch 150/200\nlearning rate scheduled to 0.00022145190785522573\n1014/1014 [==============================] - 74s 68ms/step - loss: 808.9639 - accuracy: 0.8896 - val_loss: 808.2369 - val_accuracy: 0.8881\nEpoch 151/200\nlearning rate scheduled to 0.00021923738546320237\n1014/1014 [==============================] - 74s 68ms/step - loss: 807.5212 - accuracy: 0.8889 - val_loss: 806.7988 - val_accuracy: 0.8907\nEpoch 152/200\nlearning rate scheduled to 0.00021704500570194797\n1014/1014 [==============================] - 74s 68ms/step - loss: 806.0949 - accuracy: 0.8886 - val_loss: 805.3841 - val_accuracy: 0.8879\nEpoch 153/200\nlearning rate scheduled to 0.00021487455247552135\n1014/1014 [==============================] - 74s 68ms/step - loss: 804.6822 - accuracy: 0.8901 - val_loss: 803.9739 - val_accuracy: 0.8936\nEpoch 154/200\nlearning rate scheduled to 0.00021272580968798138\n1014/1014 [==============================] - 74s 68ms/step - loss: 803.2890 - accuracy: 0.8897 - val_loss: 802.5922 - val_accuracy: 0.8909\nEpoch 155/200\nlearning rate scheduled to 0.00021059854683699086\n1014/1014 [==============================] - 75s 68ms/step - loss: 801.9119 - accuracy: 0.8905 - val_loss: 801.2265 - val_accuracy: 0.8898\nEpoch 156/200\nlearning rate scheduled to 0.0002084925622330047\n1014/1014 [==============================] - 74s 68ms/step - loss: 800.5506 - accuracy: 0.8909 - val_loss: 799.8730 - val_accuracy: 0.8883\nEpoch 157/200\nlearning rate scheduled to 0.0002064076397800818\n1014/1014 [==============================] - 75s 68ms/step - loss: 799.2064 - accuracy: 0.8903 - val_loss: 798.5394 - val_accuracy: 0.8878\nEpoch 158/200\nlearning rate scheduled to 0.000204343563382281\n1014/1014 [==============================] - 74s 68ms/step - loss: 797.8776 - accuracy: 0.8908 - val_loss: 797.2144 - val_accuracy: 0.8913\nEpoch 159/200\nlearning rate scheduled to 0.0002023001313500572\n1014/1014 [==============================] - 75s 68ms/step - loss: 796.5616 - accuracy: 0.8914 - val_loss: 795.9055 - val_accuracy: 0.8916\nEpoch 160/200\nlearning rate scheduled to 0.0002002771275874693\n1014/1014 [==============================] - 74s 68ms/step - loss: 795.2631 - accuracy: 0.8912 - val_loss: 794.6128 - val_accuracy: 0.8929\nEpoch 161/200\nlearning rate scheduled to 0.0001982743504049722\n1014/1014 [==============================] - 75s 68ms/step - loss: 793.9785 - accuracy: 0.8912 - val_loss: 793.3416 - val_accuracy: 0.8886\nEpoch 162/200\nlearning rate scheduled to 0.00019629161251941696\n1014/1014 [==============================] - 74s 68ms/step - loss: 792.7075 - accuracy: 0.8916 - val_loss: 792.0738 - val_accuracy: 0.8944\nEpoch 163/200\nlearning rate scheduled to 0.0001943286978348624\n1014/1014 [==============================] - 74s 68ms/step - loss: 791.4522 - accuracy: 0.8911 - val_loss: 790.8218 - val_accuracy: 0.8943\nEpoch 164/200\nlearning rate scheduled to 0.00019238540466176346\n1014/1014 [==============================] - 74s 68ms/step - loss: 790.2101 - accuracy: 0.8919 - val_loss: 789.5922 - val_accuracy: 0.8918\nEpoch 165/200\nlearning rate scheduled to 0.00019046154571697116\n1014/1014 [==============================] - 74s 68ms/step - loss: 788.9816 - accuracy: 0.8929 - val_loss: 788.3724 - val_accuracy: 0.8919\nEpoch 166/200\nlearning rate scheduled to 0.0001885569337173365\n1014/1014 [==============================] - 74s 68ms/step - loss: 787.7698 - accuracy: 0.8918 - val_loss: 787.1672 - val_accuracy: 0.8914\nEpoch 167/200\nlearning rate scheduled to 0.00018667136697331444\n1014/1014 [==============================] - 74s 68ms/step - loss: 786.5726 - accuracy: 0.8925 - val_loss: 785.9712 - val_accuracy: 0.8949\nEpoch 168/200\nlearning rate scheduled to 0.00018480465820175596\n1014/1014 [==============================] - 74s 68ms/step - loss: 785.3892 - accuracy: 0.8919 - val_loss: 784.7970 - val_accuracy: 0.8913\nEpoch 169/200\nlearning rate scheduled to 0.000182956605713116\n1014/1014 [==============================] - 74s 68ms/step - loss: 784.2177 - accuracy: 0.8936 - val_loss: 783.6370 - val_accuracy: 0.8930\nEpoch 170/200\nlearning rate scheduled to 0.00018112703663064166\n1014/1014 [==============================] - 74s 68ms/step - loss: 783.0618 - accuracy: 0.8932 - val_loss: 782.4856 - val_accuracy: 0.8936\nEpoch 171/200\nlearning rate scheduled to 0.00017931576367118395\n1014/1014 [==============================] - 74s 68ms/step - loss: 781.9198 - accuracy: 0.8928 - val_loss: 781.3572 - val_accuracy: 0.8910\nEpoch 172/200\nlearning rate scheduled to 0.0001775225995515939\n1014/1014 [==============================] - 74s 68ms/step - loss: 780.7898 - accuracy: 0.8927 - val_loss: 780.2228 - val_accuracy: 0.8965\nEpoch 173/200\nlearning rate scheduled to 0.00017574737139511854\n1014/1014 [==============================] - 74s 68ms/step - loss: 779.6728 - accuracy: 0.8932 - val_loss: 779.1207 - val_accuracy: 0.8912\nEpoch 174/200\nlearning rate scheduled to 0.00017398989191860892\n1014/1014 [==============================] - 74s 68ms/step - loss: 778.5696 - accuracy: 0.8929 - val_loss: 778.0200 - val_accuracy: 0.8937\nEpoch 175/200\nlearning rate scheduled to 0.00017224998824531213\n1014/1014 [==============================] - 71s 64ms/step - loss: 777.4769 - accuracy: 0.8939 - val_loss: 776.9336 - val_accuracy: 0.8951\nEpoch 176/200\nlearning rate scheduled to 0.00017052748749847522\n1014/1014 [==============================] - 68s 62ms/step - loss: 776.3969 - accuracy: 0.8933 - val_loss: 775.8597 - val_accuracy: 0.8956\nEpoch 177/200\nlearning rate scheduled to 0.00016882221680134535\n1014/1014 [==============================] - 68s 62ms/step - loss: 775.3287 - accuracy: 0.8945 - val_loss: 774.7969 - val_accuracy: 0.8955\nEpoch 178/200\nlearning rate scheduled to 0.00016713398887077346\n1014/1014 [==============================] - 69s 62ms/step - loss: 774.2739 - accuracy: 0.8935 - val_loss: 773.7478 - val_accuracy: 0.8938\nEpoch 179/200\nlearning rate scheduled to 0.00016546264523640276\n1014/1014 [==============================] - 69s 62ms/step - loss: 773.2261 - accuracy: 0.8945 - val_loss: 772.7047 - val_accuracy: 0.8966\nEpoch 180/200\nlearning rate scheduled to 0.00016380801302148028\n1014/1014 [==============================] - 68s 62ms/step - loss: 772.1938 - accuracy: 0.8942 - val_loss: 771.6794 - val_accuracy: 0.8945\nEpoch 181/200\nlearning rate scheduled to 0.00016216993375564926\n1014/1014 [==============================] - 69s 62ms/step - loss: 771.1704 - accuracy: 0.8945 - val_loss: 770.6617 - val_accuracy: 0.8952\nEpoch 182/200\nlearning rate scheduled to 0.00016054823456215672\n1014/1014 [==============================] - 69s 62ms/step - loss: 770.1598 - accuracy: 0.8950 - val_loss: 769.6535 - val_accuracy: 0.8967\nEpoch 183/200\nlearning rate scheduled to 0.00015894275697064586\n1014/1014 [==============================] - 69s 62ms/step - loss: 769.1633 - accuracy: 0.8949 - val_loss: 768.6640 - val_accuracy: 0.8951\nEpoch 184/200\nlearning rate scheduled to 0.00015735332810436374\n1014/1014 [==============================] - 69s 62ms/step - loss: 768.1769 - accuracy: 0.8946 - val_loss: 767.6885 - val_accuracy: 0.8936\nEpoch 185/200\nlearning rate scheduled to 0.00015577978949295356\n1014/1014 [==============================] - 69s 62ms/step - loss: 767.2025 - accuracy: 0.8955 - val_loss: 766.7144 - val_accuracy: 0.8967\nEpoch 186/200\nlearning rate scheduled to 0.00015422199707245453\n1014/1014 [==============================] - 69s 62ms/step - loss: 766.2398 - accuracy: 0.8958 - val_loss: 765.7561 - val_accuracy: 0.8979\nEpoch 187/200\nlearning rate scheduled to 0.00015267977796611375\n1014/1014 [==============================] - 69s 62ms/step - loss: 765.2888 - accuracy: 0.8945 - val_loss: 764.8128 - val_accuracy: 0.8951\nEpoch 188/200\nlearning rate scheduled to 0.00015115297370357439\n1014/1014 [==============================] - 69s 62ms/step - loss: 764.3468 - accuracy: 0.8955 - val_loss: 763.8785 - val_accuracy: 0.8948\nEpoch 189/200\nlearning rate scheduled to 0.00014964144022087568\n1014/1014 [==============================] - 68s 62ms/step - loss: 763.4161 - accuracy: 0.8953 - val_loss: 762.9564 - val_accuracy: 0.8950\nEpoch 190/200\nlearning rate scheduled to 0.00014814501904766075\n1014/1014 [==============================] - 69s 62ms/step - loss: 762.4955 - accuracy: 0.8959 - val_loss: 762.0336 - val_accuracy: 0.8990\nEpoch 191/200\nlearning rate scheduled to 0.00014666356611996888\n1014/1014 [==============================] - 68s 62ms/step - loss: 761.5852 - accuracy: 0.8967 - val_loss: 761.1318 - val_accuracy: 0.8976\nEpoch 192/200\nlearning rate scheduled to 0.00014519693737383933\n1014/1014 [==============================] - 69s 62ms/step - loss: 760.6844 - accuracy: 0.8968 - val_loss: 760.2466 - val_accuracy: 0.8906\nEpoch 193/200\nlearning rate scheduled to 0.0001437449743389152\n1014/1014 [==============================] - 69s 62ms/step - loss: 759.7961 - accuracy: 0.8965 - val_loss: 759.3519 - val_accuracy: 0.8978\nEpoch 194/200\nlearning rate scheduled to 0.00014230751854483968\n1014/1014 [==============================] - 69s 63ms/step - loss: 758.9156 - accuracy: 0.8964 - val_loss: 758.4775 - val_accuracy: 0.8951\nEpoch 195/200\nlearning rate scheduled to 0.00014088444033404812\n1014/1014 [==============================] - 73s 66ms/step - loss: 758.0461 - accuracy: 0.8961 - val_loss: 757.6118 - val_accuracy: 0.8951\nEpoch 196/200\nlearning rate scheduled to 0.0001394755956425797\n1014/1014 [==============================] - 69s 62ms/step - loss: 757.1847 - accuracy: 0.8962 - val_loss: 756.7560 - val_accuracy: 0.8976\nEpoch 197/200\nlearning rate scheduled to 0.00013808084040647372\n1014/1014 [==============================] - 69s 62ms/step - loss: 756.3322 - accuracy: 0.8965 - val_loss: 755.9015 - val_accuracy: 0.9007\nEpoch 198/200\nlearning rate scheduled to 0.00013670003056176939\n1014/1014 [==============================] - 69s 62ms/step - loss: 755.4884 - accuracy: 0.8959 - val_loss: 755.0716 - val_accuracy: 0.8936\nEpoch 199/200\nlearning rate scheduled to 0.000135333036450902\n1014/1014 [==============================] - 69s 62ms/step - loss: 754.6528 - accuracy: 0.8965 - val_loss: 754.2381 - val_accuracy: 0.8949\nEpoch 200/200\nlearning rate scheduled to 0.00013397969960351474\n1014/1014 [==============================] - 71s 64ms/step - loss: 753.8255 - accuracy: 0.8969 - val_loss: 753.4128 - val_accuracy: 0.8993\n"
    }
   ],
   "source": [
    "history_original_siamese_model_3 = original_siamese_model_3.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                             model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "254/254 [==============================] - 11s 21ms/step - loss: 753.4119 - accuracy: 0.8999\n"
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model_3.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Run - 30k Pairs gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_30k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_30k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_6 (InputLayer)         [(None, 105, 105, 1)]     0         \n_________________________________________________________________\nConv1 (Conv2D)               (None, 96, 96, 64)        6464      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 48, 48, 64)        0         \n_________________________________________________________________\nConv2 (Conv2D)               (None, 42, 42, 128)       401536    \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 21, 21, 128)       0         \n_________________________________________________________________\nConv3 (Conv2D)               (None, 18, 18, 128)       262272    \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 9, 9, 128)         0         \n_________________________________________________________________\nConv4 (Conv2D)               (None, 6, 6, 256)         524544    \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 9216)              0         \n_________________________________________________________________\nDense1 (Dense)               (None, 4096)              37752832  \n=================================================================\nTotal params: 38,947,648\nTrainable params: 38,947,648\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_4 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1972vkrr\" target=\"_blank\">easy-eon-8</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 30k - Grayscale\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "original_siamese_model_4.compile(loss=config.loss_function,\n",
    "                                 optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_30k_Gray\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/200\nlearning rate scheduled to 0.0009900000470224768\n  6/203 [..............................] - ETA: 8s - loss: 1510.6082 - accuracy: 0.4905WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0220s vs `on_train_batch_end` time: 0.0229s). Check your callbacks.\n203/203 [==============================] - 12s 55ms/step - loss: 1509.4403 - accuracy: 0.5015 - val_loss: 1508.2151 - val_accuracy: 0.5015\nEpoch 2/200\nlearning rate scheduled to 0.000980100086890161\n203/203 [==============================] - 11s 52ms/step - loss: 1507.0282 - accuracy: 0.5073 - val_loss: 1505.8185 - val_accuracy: 0.5021\nEpoch 3/200\nlearning rate scheduled to 0.0009702991275116801\n203/203 [==============================] - 11s 53ms/step - loss: 1504.6437 - accuracy: 0.5033 - val_loss: 1503.4476 - val_accuracy: 0.4926\nEpoch 4/200\nlearning rate scheduled to 0.0009605961316265165\n203/203 [==============================] - 11s 52ms/step - loss: 1502.2870 - accuracy: 0.5165 - val_loss: 1501.1044 - val_accuracy: 0.5160\nEpoch 5/200\nlearning rate scheduled to 0.0009509901772253215\n203/203 [==============================] - 11s 52ms/step - loss: 1499.9572 - accuracy: 0.5246 - val_loss: 1498.7887 - val_accuracy: 0.5206\nEpoch 6/200\nlearning rate scheduled to 0.0009414802846731617\n203/203 [==============================] - 11s 52ms/step - loss: 1497.6536 - accuracy: 0.5256 - val_loss: 1496.4982 - val_accuracy: 0.5207\nEpoch 7/200\nlearning rate scheduled to 0.0009320654743351042\n203/203 [==============================] - 11s 53ms/step - loss: 1495.3767 - accuracy: 0.5234 - val_loss: 1494.2349 - val_accuracy: 0.5221\nEpoch 8/200\nlearning rate scheduled to 0.0009227448242017999\n203/203 [==============================] - 11s 52ms/step - loss: 1493.1257 - accuracy: 0.5302 - val_loss: 1491.9957 - val_accuracy: 0.5241\nEpoch 9/200\nlearning rate scheduled to 0.0009135173546383158\n203/203 [==============================] - 11s 52ms/step - loss: 1490.8995 - accuracy: 0.5215 - val_loss: 1489.7816 - val_accuracy: 0.5262\nEpoch 10/200\nlearning rate scheduled to 0.0009043822012608871\n203/203 [==============================] - 11s 52ms/step - loss: 1488.6998 - accuracy: 0.5208 - val_loss: 1487.5941 - val_accuracy: 0.5273\nEpoch 11/200\nlearning rate scheduled to 0.0008953383844345808\n203/203 [==============================] - 11s 53ms/step - loss: 1486.5243 - accuracy: 0.5319 - val_loss: 1485.4326 - val_accuracy: 0.5515\nEpoch 12/200\nlearning rate scheduled to 0.000886384982150048\n203/203 [==============================] - 11s 52ms/step - loss: 1484.3737 - accuracy: 0.5488 - val_loss: 1483.2953 - val_accuracy: 0.5485\nEpoch 13/200\nlearning rate scheduled to 0.0008775211300235242\n203/203 [==============================] - 11s 53ms/step - loss: 1482.2482 - accuracy: 0.5617 - val_loss: 1481.1794 - val_accuracy: 0.5654\nEpoch 14/200\nlearning rate scheduled to 0.0008687459060456604\n203/203 [==============================] - 11s 52ms/step - loss: 1480.1456 - accuracy: 0.5658 - val_loss: 1479.0908 - val_accuracy: 0.5732\nEpoch 15/200\nlearning rate scheduled to 0.0008600584458326921\n203/203 [==============================] - 11s 52ms/step - loss: 1478.0690 - accuracy: 0.5710 - val_loss: 1477.0232 - val_accuracy: 0.5843\nEpoch 16/200\nlearning rate scheduled to 0.0008514578850008547\n203/203 [==============================] - 11s 52ms/step - loss: 1476.0139 - accuracy: 0.5787 - val_loss: 1474.9812 - val_accuracy: 0.5948\nEpoch 17/200\nlearning rate scheduled to 0.0008429433015407995\n203/203 [==============================] - 11s 53ms/step - loss: 1473.9847 - accuracy: 0.5814 - val_loss: 1472.9683 - val_accuracy: 0.5886\nEpoch 18/200\nlearning rate scheduled to 0.0008345138886943459\n203/203 [==============================] - 11s 52ms/step - loss: 1471.9773 - accuracy: 0.5859 - val_loss: 1470.9718 - val_accuracy: 0.5843\nEpoch 19/200\nlearning rate scheduled to 0.0008261687244521454\n203/203 [==============================] - 11s 53ms/step - loss: 1469.9944 - accuracy: 0.5882 - val_loss: 1468.9968 - val_accuracy: 0.5858\nEpoch 20/200\nlearning rate scheduled to 0.0008179070596816018\n203/203 [==============================] - 11s 52ms/step - loss: 1468.0322 - accuracy: 0.5879 - val_loss: 1467.0482 - val_accuracy: 0.5875\nEpoch 21/200\nlearning rate scheduled to 0.0008097279723733664\n203/203 [==============================] - 11s 52ms/step - loss: 1466.0930 - accuracy: 0.5921 - val_loss: 1465.1215 - val_accuracy: 0.5920\nEpoch 22/200\nlearning rate scheduled to 0.000801630713394843\n203/203 [==============================] - 11s 52ms/step - loss: 1464.1755 - accuracy: 0.5935 - val_loss: 1463.2141 - val_accuracy: 0.5990\nEpoch 23/200\nlearning rate scheduled to 0.0007936144183622674\n203/203 [==============================] - 11s 52ms/step - loss: 1462.2808 - accuracy: 0.5947 - val_loss: 1461.3291 - val_accuracy: 0.5928\nEpoch 24/200\nlearning rate scheduled to 0.0007856782805174589\n203/203 [==============================] - 11s 53ms/step - loss: 1460.4041 - accuracy: 0.6020 - val_loss: 1459.4640 - val_accuracy: 0.6037\nEpoch 25/200\nlearning rate scheduled to 0.0007778214931022376\n203/203 [==============================] - 11s 53ms/step - loss: 1458.5511 - accuracy: 0.6038 - val_loss: 1457.6206 - val_accuracy: 0.5963\nEpoch 26/200\nlearning rate scheduled to 0.0007700433069840073\n203/203 [==============================] - 11s 52ms/step - loss: 1456.7183 - accuracy: 0.6043 - val_loss: 1455.8013 - val_accuracy: 0.6039\nEpoch 27/200\nlearning rate scheduled to 0.0007623428577790037\n203/203 [==============================] - 11s 53ms/step - loss: 1454.9061 - accuracy: 0.6104 - val_loss: 1453.9946 - val_accuracy: 0.6171\nEpoch 28/200\nlearning rate scheduled to 0.0007547194539802149\n203/203 [==============================] - 11s 52ms/step - loss: 1453.1136 - accuracy: 0.6137 - val_loss: 1452.2104 - val_accuracy: 0.6254\nEpoch 29/200\nlearning rate scheduled to 0.0007471722312038764\n203/203 [==============================] - 11s 53ms/step - loss: 1451.3372 - accuracy: 0.6209 - val_loss: 1450.4498 - val_accuracy: 0.6167\nEpoch 30/200\nlearning rate scheduled to 0.0007397004979429766\n203/203 [==============================] - 11s 52ms/step - loss: 1449.5854 - accuracy: 0.6248 - val_loss: 1448.7058 - val_accuracy: 0.6269\nEpoch 31/200\nlearning rate scheduled to 0.0007323035050649196\n203/203 [==============================] - 11s 53ms/step - loss: 1447.8516 - accuracy: 0.6313 - val_loss: 1446.9811 - val_accuracy: 0.6393\nEpoch 32/200\nlearning rate scheduled to 0.000724980445811525\n203/203 [==============================] - 11s 52ms/step - loss: 1446.1365 - accuracy: 0.6344 - val_loss: 1445.2760 - val_accuracy: 0.6286\nEpoch 33/200\nlearning rate scheduled to 0.0007177306286757812\n203/203 [==============================] - 11s 52ms/step - loss: 1444.4414 - accuracy: 0.6363 - val_loss: 1443.5919 - val_accuracy: 0.6365\nEpoch 34/200\nlearning rate scheduled to 0.0007105533045250923\n203/203 [==============================] - 11s 53ms/step - loss: 1442.7643 - accuracy: 0.6402 - val_loss: 1441.9241 - val_accuracy: 0.6410\nEpoch 35/200\nlearning rate scheduled to 0.0007034477818524464\n203/203 [==============================] - 11s 52ms/step - loss: 1441.1075 - accuracy: 0.6400 - val_loss: 1440.2775 - val_accuracy: 0.6369\nEpoch 36/200\nlearning rate scheduled to 0.000696413311525248\n203/203 [==============================] - 11s 52ms/step - loss: 1439.4672 - accuracy: 0.6430 - val_loss: 1438.6465 - val_accuracy: 0.6423\nEpoch 37/200\nlearning rate scheduled to 0.0006894492020364851\n203/203 [==============================] - 11s 53ms/step - loss: 1437.8459 - accuracy: 0.6474 - val_loss: 1437.0399 - val_accuracy: 0.6352\nEpoch 38/200\nlearning rate scheduled to 0.0006825547042535618\n203/203 [==============================] - 11s 52ms/step - loss: 1436.2434 - accuracy: 0.6476 - val_loss: 1435.4417 - val_accuracy: 0.6444\nEpoch 39/200\nlearning rate scheduled to 0.0006757291842950508\n203/203 [==============================] - 11s 52ms/step - loss: 1434.6582 - accuracy: 0.6483 - val_loss: 1433.8972 - val_accuracy: 0.6297\nEpoch 40/200\nlearning rate scheduled to 0.0006689718930283561\n203/203 [==============================] - 11s 52ms/step - loss: 1433.0955 - accuracy: 0.6501 - val_loss: 1432.3058 - val_accuracy: 0.6520\nEpoch 41/200\nlearning rate scheduled to 0.0006622821965720505\n203/203 [==============================] - 11s 52ms/step - loss: 1431.5420 - accuracy: 0.6542 - val_loss: 1430.7660 - val_accuracy: 0.6514\nEpoch 42/200\nlearning rate scheduled to 0.0006556594034191221\n203/203 [==============================] - 11s 53ms/step - loss: 1430.0093 - accuracy: 0.6545 - val_loss: 1429.2399 - val_accuracy: 0.6621\nEpoch 43/200\nlearning rate scheduled to 0.0006491028220625594\n203/203 [==============================] - 11s 52ms/step - loss: 1428.4930 - accuracy: 0.6591 - val_loss: 1427.7347 - val_accuracy: 0.6563\nEpoch 44/200\nlearning rate scheduled to 0.0006426118186209351\n203/203 [==============================] - 11s 52ms/step - loss: 1426.9932 - accuracy: 0.6597 - val_loss: 1426.2444 - val_accuracy: 0.6531\nEpoch 45/200\nlearning rate scheduled to 0.0006361857015872375\n203/203 [==============================] - 11s 52ms/step - loss: 1425.5115 - accuracy: 0.6606 - val_loss: 1424.7723 - val_accuracy: 0.6597\nEpoch 46/200\nlearning rate scheduled to 0.0006298238370800391\n203/203 [==============================] - 11s 53ms/step - loss: 1424.0460 - accuracy: 0.6601 - val_loss: 1423.3079 - val_accuracy: 0.6599\nEpoch 47/200\nlearning rate scheduled to 0.0006235255912179127\n203/203 [==============================] - 11s 53ms/step - loss: 1422.5961 - accuracy: 0.6653 - val_loss: 1421.8700 - val_accuracy: 0.6597\nEpoch 48/200\nlearning rate scheduled to 0.000617290330119431\n203/203 [==============================] - 11s 52ms/step - loss: 1421.1615 - accuracy: 0.6634 - val_loss: 1420.4432 - val_accuracy: 0.6623\nEpoch 49/200\nlearning rate scheduled to 0.0006111174199031666\n203/203 [==============================] - 11s 52ms/step - loss: 1419.7446 - accuracy: 0.6647 - val_loss: 1419.0321 - val_accuracy: 0.6712\nEpoch 50/200\nlearning rate scheduled to 0.0006050062266876921\n203/203 [==============================] - 11s 52ms/step - loss: 1418.3424 - accuracy: 0.6675 - val_loss: 1417.6973 - val_accuracy: 0.6314\nEpoch 51/200\nlearning rate scheduled to 0.0005989561742171646\n203/203 [==============================] - 11s 52ms/step - loss: 1416.9570 - accuracy: 0.6659 - val_loss: 1416.2668 - val_accuracy: 0.6638\nEpoch 52/200\nlearning rate scheduled to 0.0005929666286101564\n203/203 [==============================] - 11s 52ms/step - loss: 1415.5850 - accuracy: 0.6667 - val_loss: 1414.9261 - val_accuracy: 0.6488\nEpoch 53/200\nlearning rate scheduled to 0.0005870369559852406\n203/203 [==============================] - 11s 53ms/step - loss: 1414.2285 - accuracy: 0.6675 - val_loss: 1413.5513 - val_accuracy: 0.6691\nEpoch 54/200\nlearning rate scheduled to 0.000581166580086574\n203/203 [==============================] - 11s 52ms/step - loss: 1412.8865 - accuracy: 0.6688 - val_loss: 1412.2461 - val_accuracy: 0.6495\nEpoch 55/200\nlearning rate scheduled to 0.0005753549246583134\n203/203 [==============================] - 11s 52ms/step - loss: 1411.5610 - accuracy: 0.6682 - val_loss: 1410.8932 - val_accuracy: 0.6663\nEpoch 56/200\nlearning rate scheduled to 0.0005696013558190316\n203/203 [==============================] - 11s 52ms/step - loss: 1410.2473 - accuracy: 0.6709 - val_loss: 1409.5908 - val_accuracy: 0.6595\nEpoch 57/200\nlearning rate scheduled to 0.0005639053549384699\n203/203 [==============================] - 11s 52ms/step - loss: 1408.9492 - accuracy: 0.6681 - val_loss: 1408.3102 - val_accuracy: 0.6614\nEpoch 58/200\nlearning rate scheduled to 0.0005582662881352008\n203/203 [==============================] - 11s 52ms/step - loss: 1407.6654 - accuracy: 0.6712 - val_loss: 1407.0249 - val_accuracy: 0.6738\nEpoch 59/200\nlearning rate scheduled to 0.0005526836367789656\n203/203 [==============================] - 11s 52ms/step - loss: 1406.3962 - accuracy: 0.6699 - val_loss: 1405.7565 - val_accuracy: 0.6689\nEpoch 60/200\nlearning rate scheduled to 0.0005471568246139213\n203/203 [==============================] - 11s 52ms/step - loss: 1405.1412 - accuracy: 0.6702 - val_loss: 1404.5189 - val_accuracy: 0.6618\nEpoch 61/200\nlearning rate scheduled to 0.000541685275384225\n203/203 [==============================] - 11s 52ms/step - loss: 1403.8988 - accuracy: 0.6732 - val_loss: 1403.3021 - val_accuracy: 0.6521\nEpoch 62/200\nlearning rate scheduled to 0.0005362684128340334\n203/203 [==============================] - 11s 52ms/step - loss: 1402.6691 - accuracy: 0.6749 - val_loss: 1402.1143 - val_accuracy: 0.6097\nEpoch 63/200\nlearning rate scheduled to 0.0005309057183330878\n203/203 [==============================] - 11s 52ms/step - loss: 1401.4563 - accuracy: 0.6687 - val_loss: 1400.8491 - val_accuracy: 0.6691\nEpoch 64/200\nlearning rate scheduled to 0.0005255966732511297\n203/203 [==============================] - 11s 52ms/step - loss: 1400.2512 - accuracy: 0.6763 - val_loss: 1399.7190 - val_accuracy: 0.6390\nEpoch 65/200\nlearning rate scheduled to 0.0005203407013323158\n203/203 [==============================] - 11s 52ms/step - loss: 1399.0669 - accuracy: 0.6698 - val_loss: 1398.5291 - val_accuracy: 0.6103\nEpoch 66/200\nlearning rate scheduled to 0.0005151372839463875\n203/203 [==============================] - 11s 52ms/step - loss: 1397.8867 - accuracy: 0.6740 - val_loss: 1397.3376 - val_accuracy: 0.6521\nEpoch 67/200\nlearning rate scheduled to 0.0005099859024630859\n203/203 [==============================] - 11s 53ms/step - loss: 1396.7260 - accuracy: 0.6712 - val_loss: 1396.1422 - val_accuracy: 0.6718\nEpoch 68/200\nlearning rate scheduled to 0.0005048860382521525\n203/203 [==============================] - 11s 53ms/step - loss: 1395.5723 - accuracy: 0.6765 - val_loss: 1395.0714 - val_accuracy: 0.5905\nEpoch 69/200\nlearning rate scheduled to 0.0004998371726833284\n203/203 [==============================] - 11s 52ms/step - loss: 1394.4355 - accuracy: 0.6753 - val_loss: 1393.8724 - val_accuracy: 0.6704\nEpoch 70/200\nlearning rate scheduled to 0.0004948387871263549\n203/203 [==============================] - 11s 52ms/step - loss: 1393.3058 - accuracy: 0.6785 - val_loss: 1392.7407 - val_accuracy: 0.6848\nEpoch 71/200\nlearning rate scheduled to 0.0004898904205765575\n203/203 [==============================] - 11s 53ms/step - loss: 1392.1957 - accuracy: 0.6760 - val_loss: 1391.6606 - val_accuracy: 0.6525\nEpoch 72/200\nlearning rate scheduled to 0.00048499149677809326\n203/203 [==============================] - 11s 53ms/step - loss: 1391.0951 - accuracy: 0.6742 - val_loss: 1390.5460 - val_accuracy: 0.6702\nEpoch 73/200\nlearning rate scheduled to 0.00048014158353907986\n203/203 [==============================] - 11s 52ms/step - loss: 1390.0029 - accuracy: 0.6791 - val_loss: 1389.4504 - val_accuracy: 0.6812\nEpoch 74/200\nlearning rate scheduled to 0.00047534016222925855\n203/203 [==============================] - 11s 53ms/step - loss: 1388.9249 - accuracy: 0.6782 - val_loss: 1388.4384 - val_accuracy: 0.6261\nEpoch 75/200\nlearning rate scheduled to 0.0004705867718439549\n203/203 [==============================] - 11s 53ms/step - loss: 1387.8583 - accuracy: 0.6804 - val_loss: 1387.3459 - val_accuracy: 0.6659\nEpoch 76/200\nlearning rate scheduled to 0.0004658808937529102\n203/203 [==============================] - 11s 53ms/step - loss: 1386.8043 - accuracy: 0.6773 - val_loss: 1386.3445 - val_accuracy: 0.6397\nEpoch 77/200\nlearning rate scheduled to 0.0004612220957642421\n203/203 [==============================] - 11s 52ms/step - loss: 1385.7582 - accuracy: 0.6822 - val_loss: 1385.3202 - val_accuracy: 0.6286\nEpoch 78/200\nlearning rate scheduled to 0.00045660988806048406\n203/203 [==============================] - 11s 52ms/step - loss: 1384.7280 - accuracy: 0.6777 - val_loss: 1384.2395 - val_accuracy: 0.6659\nEpoch 79/200\nlearning rate scheduled to 0.0004520437808241695\n203/203 [==============================] - 11s 52ms/step - loss: 1383.7065 - accuracy: 0.6792 - val_loss: 1383.2064 - val_accuracy: 0.6650\nEpoch 80/200\nlearning rate scheduled to 0.0004475233418634161\n203/203 [==============================] - 11s 52ms/step - loss: 1382.6965 - accuracy: 0.6774 - val_loss: 1382.2761 - val_accuracy: 0.5903\nEpoch 81/200\nlearning rate scheduled to 0.0004430481101735495\n203/203 [==============================] - 11s 52ms/step - loss: 1381.7013 - accuracy: 0.6745 - val_loss: 1381.2622 - val_accuracy: 0.6386\nEpoch 82/200\nlearning rate scheduled to 0.0004386176247498952\n203/203 [==============================] - 11s 52ms/step - loss: 1380.7094 - accuracy: 0.6802 - val_loss: 1380.2235 - val_accuracy: 0.6712\nEpoch 83/200\nlearning rate scheduled to 0.00043423145340057087\n203/203 [==============================] - 11s 53ms/step - loss: 1379.7289 - accuracy: 0.6816 - val_loss: 1379.2933 - val_accuracy: 0.6472\nEpoch 84/200\nlearning rate scheduled to 0.0004298891351209022\n203/203 [==============================] - 11s 52ms/step - loss: 1378.7628 - accuracy: 0.6776 - val_loss: 1378.2764 - val_accuracy: 0.6733\nEpoch 85/200\nlearning rate scheduled to 0.00042559023771900686\n203/203 [==============================] - 11s 52ms/step - loss: 1377.8020 - accuracy: 0.6827 - val_loss: 1377.3372 - val_accuracy: 0.6748\nEpoch 86/200\nlearning rate scheduled to 0.0004213343290030025\n203/203 [==============================] - 11s 52ms/step - loss: 1376.8552 - accuracy: 0.6813 - val_loss: 1376.4315 - val_accuracy: 0.6533\nEpoch 87/200\nlearning rate scheduled to 0.00041712097678100687\n203/203 [==============================] - 11s 52ms/step - loss: 1375.9205 - accuracy: 0.6807 - val_loss: 1375.5498 - val_accuracy: 0.6259\nEpoch 88/200\nlearning rate scheduled to 0.00041294977767392994\n203/203 [==============================] - 11s 53ms/step - loss: 1374.9927 - accuracy: 0.6811 - val_loss: 1374.5419 - val_accuracy: 0.6618\nEpoch 89/200\nlearning rate scheduled to 0.0004088202706770971\n203/203 [==============================] - 11s 52ms/step - loss: 1374.0754 - accuracy: 0.6807 - val_loss: 1373.6178 - val_accuracy: 0.6746\nEpoch 90/200\nlearning rate scheduled to 0.00040473208122421056\n203/203 [==============================] - 11s 52ms/step - loss: 1373.1680 - accuracy: 0.6827 - val_loss: 1372.7444 - val_accuracy: 0.6561\nEpoch 91/200\nlearning rate scheduled to 0.0004006847483105957\n203/203 [==============================] - 11s 52ms/step - loss: 1372.2693 - accuracy: 0.6815 - val_loss: 1371.8822 - val_accuracy: 0.6220\nEpoch 92/200\nlearning rate scheduled to 0.0003966778973699547\n203/203 [==============================] - 11s 52ms/step - loss: 1371.3813 - accuracy: 0.6835 - val_loss: 1370.9305 - val_accuracy: 0.6874\nEpoch 93/200\nlearning rate scheduled to 0.0003927111250231974\n203/203 [==============================] - 11s 52ms/step - loss: 1370.5018 - accuracy: 0.6859 - val_loss: 1370.0966 - val_accuracy: 0.6484\nEpoch 94/200\nlearning rate scheduled to 0.0003887840278912336\n203/203 [==============================] - 11s 52ms/step - loss: 1369.6331 - accuracy: 0.6835 - val_loss: 1369.1943 - val_accuracy: 0.6917\nEpoch 95/200\nlearning rate scheduled to 0.000384896173782181\n203/203 [==============================] - 11s 53ms/step - loss: 1368.7740 - accuracy: 0.6812 - val_loss: 1368.3643 - val_accuracy: 0.6678\nEpoch 96/200\nlearning rate scheduled to 0.00038104721694253384\n203/203 [==============================] - 11s 53ms/step - loss: 1367.9215 - accuracy: 0.6846 - val_loss: 1367.5090 - val_accuracy: 0.6646\nEpoch 97/200\nlearning rate scheduled to 0.000377236753993202\n203/203 [==============================] - 11s 52ms/step - loss: 1367.0798 - accuracy: 0.6848 - val_loss: 1366.7057 - val_accuracy: 0.6416\nEpoch 98/200\nlearning rate scheduled to 0.0003734643815550953\n203/203 [==============================] - 11s 52ms/step - loss: 1366.2479 - accuracy: 0.6847 - val_loss: 1365.8292 - val_accuracy: 0.6802\nEpoch 99/200\nlearning rate scheduled to 0.0003697297250619158\n203/203 [==============================] - 11s 52ms/step - loss: 1365.4215 - accuracy: 0.6872 - val_loss: 1365.0481 - val_accuracy: 0.6591\nEpoch 100/200\nlearning rate scheduled to 0.0003660324387601577\n203/203 [==============================] - 11s 53ms/step - loss: 1364.6066 - accuracy: 0.6849 - val_loss: 1364.2988 - val_accuracy: 0.5739\nEpoch 101/200\nlearning rate scheduled to 0.00036237211927073074\n203/203 [==============================] - 11s 52ms/step - loss: 1363.8068 - accuracy: 0.6778 - val_loss: 1363.3947 - val_accuracy: 0.6838\nEpoch 102/200\nlearning rate scheduled to 0.0003587483920273371\n203/203 [==============================] - 11s 53ms/step - loss: 1363.0020 - accuracy: 0.6837 - val_loss: 1362.5969 - val_accuracy: 0.6863\nEpoch 103/200\nlearning rate scheduled to 0.0003551609112764709\n203/203 [==============================] - 11s 53ms/step - loss: 1362.2111 - accuracy: 0.6878 - val_loss: 1361.8169 - val_accuracy: 0.6934\nEpoch 104/200\nlearning rate scheduled to 0.0003516093024518341\n203/203 [==============================] - 11s 52ms/step - loss: 1361.4301 - accuracy: 0.6872 - val_loss: 1361.0297 - val_accuracy: 0.6917\nEpoch 105/200\nlearning rate scheduled to 0.0003480932197999209\n203/203 [==============================] - 11s 52ms/step - loss: 1360.6569 - accuracy: 0.6867 - val_loss: 1360.2600 - val_accuracy: 0.6917\nEpoch 106/200\nlearning rate scheduled to 0.0003446122887544334\n203/203 [==============================] - 11s 53ms/step - loss: 1359.8911 - accuracy: 0.6890 - val_loss: 1359.5043 - val_accuracy: 0.6948\nEpoch 107/200\nlearning rate scheduled to 0.00034116616356186566\n203/203 [==============================] - 11s 52ms/step - loss: 1359.1338 - accuracy: 0.6874 - val_loss: 1358.8555 - val_accuracy: 0.5754\nEpoch 108/200\nlearning rate scheduled to 0.00033775449846871195\n203/203 [==============================] - 11s 52ms/step - loss: 1358.3850 - accuracy: 0.6875 - val_loss: 1358.0333 - val_accuracy: 0.6736\nEpoch 109/200\nlearning rate scheduled to 0.0003343769477214664\n203/203 [==============================] - 11s 52ms/step - loss: 1357.6451 - accuracy: 0.6858 - val_loss: 1357.3302 - val_accuracy: 0.6571\nEpoch 110/200\nlearning rate scheduled to 0.0003310331655666232\n203/203 [==============================] - 11s 52ms/step - loss: 1356.9111 - accuracy: 0.6876 - val_loss: 1356.5498 - val_accuracy: 0.6855\nEpoch 111/200\nlearning rate scheduled to 0.00032772283506346864\n203/203 [==============================] - 11s 52ms/step - loss: 1356.1852 - accuracy: 0.6877 - val_loss: 1355.9739 - val_accuracy: 0.5997\nEpoch 112/200\nlearning rate scheduled to 0.00032444561045849693\n203/203 [==============================] - 11s 52ms/step - loss: 1355.4706 - accuracy: 0.6919 - val_loss: 1355.2352 - val_accuracy: 0.5364\nEpoch 113/200\nlearning rate scheduled to 0.00032120114599820226\n203/203 [==============================] - 11s 52ms/step - loss: 1354.7782 - accuracy: 0.6658 - val_loss: 1354.4050 - val_accuracy: 0.6829\nEpoch 114/200\nlearning rate scheduled to 0.00031798912474187093\n203/203 [==============================] - 11s 53ms/step - loss: 1354.0563 - accuracy: 0.6908 - val_loss: 1353.7091 - val_accuracy: 0.6831\nEpoch 115/200\nlearning rate scheduled to 0.0003148092297487892\n203/203 [==============================] - 11s 52ms/step - loss: 1353.3585 - accuracy: 0.6911 - val_loss: 1353.0134 - val_accuracy: 0.6914\nEpoch 116/200\nlearning rate scheduled to 0.0003116611440782435\n203/203 [==============================] - 11s 52ms/step - loss: 1352.6670 - accuracy: 0.6942 - val_loss: 1352.3274 - val_accuracy: 0.6887\nEpoch 117/200\nlearning rate scheduled to 0.000308544521976728\n203/203 [==============================] - 11s 52ms/step - loss: 1351.9882 - accuracy: 0.6900 - val_loss: 1351.6736 - val_accuracy: 0.6702\nEpoch 118/200\nlearning rate scheduled to 0.0003054590753163211\n203/203 [==============================] - 11s 53ms/step - loss: 1351.3151 - accuracy: 0.6887 - val_loss: 1350.9966 - val_accuracy: 0.6727\nEpoch 119/200\nlearning rate scheduled to 0.0003024044871563092\n203/203 [==============================] - 11s 52ms/step - loss: 1350.6497 - accuracy: 0.6894 - val_loss: 1350.3218 - val_accuracy: 0.6819\nEpoch 120/200\nlearning rate scheduled to 0.00029938044055597855\n203/203 [==============================] - 11s 52ms/step - loss: 1349.9883 - accuracy: 0.6903 - val_loss: 1349.6533 - val_accuracy: 0.6983\nEpoch 121/200\nlearning rate scheduled to 0.0002963866473874077\n203/203 [==============================] - 11s 52ms/step - loss: 1349.3357 - accuracy: 0.6896 - val_loss: 1349.0171 - val_accuracy: 0.6891\nEpoch 122/200\nlearning rate scheduled to 0.000293422790709883\n203/203 [==============================] - 11s 52ms/step - loss: 1348.6898 - accuracy: 0.6914 - val_loss: 1348.4771 - val_accuracy: 0.6216\nEpoch 123/200\nlearning rate scheduled to 0.00029048855358269063\n203/203 [==============================] - 11s 52ms/step - loss: 1348.0526 - accuracy: 0.6890 - val_loss: 1347.7946 - val_accuracy: 0.6516\nEpoch 124/200\nlearning rate scheduled to 0.0002875836766907014\n203/203 [==============================] - 11s 52ms/step - loss: 1347.4198 - accuracy: 0.6894 - val_loss: 1347.1193 - val_accuracy: 0.6721\nEpoch 125/200\nlearning rate scheduled to 0.0002847078430932015\n203/203 [==============================] - 11s 52ms/step - loss: 1346.7904 - accuracy: 0.6901 - val_loss: 1346.5345 - val_accuracy: 0.6574\nEpoch 126/200\nlearning rate scheduled to 0.0002818607646622695\n203/203 [==============================] - 11s 52ms/step - loss: 1346.1726 - accuracy: 0.6894 - val_loss: 1345.8525 - val_accuracy: 0.6987\nEpoch 127/200\nlearning rate scheduled to 0.00027904215326998385\n203/203 [==============================] - 11s 52ms/step - loss: 1345.5575 - accuracy: 0.6928 - val_loss: 1345.2643 - val_accuracy: 0.6812\nEpoch 128/200\nlearning rate scheduled to 0.00027625172078842296\n203/203 [==============================] - 11s 52ms/step - loss: 1344.9504 - accuracy: 0.6938 - val_loss: 1344.6700 - val_accuracy: 0.6782\nEpoch 129/200\nlearning rate scheduled to 0.0002734892079024576\n203/203 [==============================] - 11s 52ms/step - loss: 1344.3535 - accuracy: 0.6902 - val_loss: 1344.0530 - val_accuracy: 0.6878\nEpoch 130/200\nlearning rate scheduled to 0.0002707543264841661\n203/203 [==============================] - 11s 52ms/step - loss: 1343.7582 - accuracy: 0.6922 - val_loss: 1343.6006 - val_accuracy: 0.6092\nEpoch 131/200\nlearning rate scheduled to 0.000268046788405627\n203/203 [==============================] - 11s 53ms/step - loss: 1343.1754 - accuracy: 0.6878 - val_loss: 1343.0172 - val_accuracy: 0.5206\nEpoch 132/200\nlearning rate scheduled to 0.000265366334351711\n203/203 [==============================] - 11s 52ms/step - loss: 1342.6758 - accuracy: 0.5856 - val_loss: 1342.3307 - val_accuracy: 0.6695\nEpoch 133/200\nlearning rate scheduled to 0.00026271267619449646\n203/203 [==============================] - 11s 52ms/step - loss: 1342.0173 - accuracy: 0.6929 - val_loss: 1341.7295 - val_accuracy: 0.6902\nEpoch 134/200\nlearning rate scheduled to 0.00026008555461885406\n203/203 [==============================] - 11s 52ms/step - loss: 1341.4459 - accuracy: 0.6927 - val_loss: 1341.2146 - val_accuracy: 0.6386\nEpoch 135/200\nlearning rate scheduled to 0.00025748471030965445\n203/203 [==============================] - 11s 53ms/step - loss: 1340.8816 - accuracy: 0.6947 - val_loss: 1340.5874 - val_accuracy: 0.6982\nEpoch 136/200\nlearning rate scheduled to 0.0002549098551389761\n203/203 [==============================] - 11s 53ms/step - loss: 1340.3251 - accuracy: 0.6906 - val_loss: 1340.0387 - val_accuracy: 0.6906\nEpoch 137/200\nlearning rate scheduled to 0.00025236075860448183\n203/203 [==============================] - 11s 52ms/step - loss: 1339.7708 - accuracy: 0.6949 - val_loss: 1339.4993 - val_accuracy: 0.6833\nEpoch 138/200\nlearning rate scheduled to 0.0002498371613910422\n203/203 [==============================] - 11s 52ms/step - loss: 1339.2234 - accuracy: 0.6943 - val_loss: 1338.9600 - val_accuracy: 0.6814\nEpoch 139/200\nlearning rate scheduled to 0.0002473388041835278\n203/203 [==============================] - 11s 53ms/step - loss: 1338.6820 - accuracy: 0.6925 - val_loss: 1338.4279 - val_accuracy: 0.6827\nEpoch 140/200\nlearning rate scheduled to 0.0002448654276668094\n203/203 [==============================] - 11s 52ms/step - loss: 1338.1473 - accuracy: 0.6946 - val_loss: 1337.8855 - val_accuracy: 0.6833\nEpoch 141/200\nlearning rate scheduled to 0.00024241677252575755\n203/203 [==============================] - 11s 53ms/step - loss: 1337.6183 - accuracy: 0.6932 - val_loss: 1337.3556 - val_accuracy: 0.6925\nEpoch 142/200\nlearning rate scheduled to 0.00023999260825803502\n203/203 [==============================] - 11s 53ms/step - loss: 1337.0948 - accuracy: 0.6930 - val_loss: 1336.8395 - val_accuracy: 0.6906\nEpoch 143/200\nlearning rate scheduled to 0.00023759267554851249\n203/203 [==============================] - 11s 53ms/step - loss: 1336.5789 - accuracy: 0.6929 - val_loss: 1336.3173 - val_accuracy: 0.6859\nEpoch 144/200\nlearning rate scheduled to 0.0002352167438948527\n203/203 [==============================] - 11s 53ms/step - loss: 1336.0624 - accuracy: 0.6970 - val_loss: 1335.8035 - val_accuracy: 0.6940\nEpoch 145/200\nlearning rate scheduled to 0.00023286458279471843\n203/203 [==============================] - 11s 52ms/step - loss: 1335.5562 - accuracy: 0.6964 - val_loss: 1335.2974 - val_accuracy: 0.7012\nEpoch 146/200\nlearning rate scheduled to 0.00023053593293298035\n203/203 [==============================] - 11s 52ms/step - loss: 1335.0557 - accuracy: 0.6934 - val_loss: 1334.7961 - val_accuracy: 0.7010\nEpoch 147/200\nlearning rate scheduled to 0.0002282305782136973\n203/203 [==============================] - 11s 52ms/step - loss: 1334.5569 - accuracy: 0.6964 - val_loss: 1334.3107 - val_accuracy: 0.6982\nEpoch 148/200\nlearning rate scheduled to 0.00022594827372813598\n203/203 [==============================] - 11s 53ms/step - loss: 1334.0635 - accuracy: 0.6948 - val_loss: 1333.9050 - val_accuracy: 0.6344\nEpoch 149/200\nlearning rate scheduled to 0.00022368878897395915\n203/203 [==============================] - 11s 53ms/step - loss: 1333.5806 - accuracy: 0.6908 - val_loss: 1333.3330 - val_accuracy: 0.6980\nEpoch 150/200\nlearning rate scheduled to 0.00022145190785522573\n203/203 [==============================] - 11s 52ms/step - loss: 1333.0942 - accuracy: 0.6950 - val_loss: 1332.8542 - val_accuracy: 0.6900\nEpoch 151/200\nlearning rate scheduled to 0.00021923738546320237\n203/203 [==============================] - 11s 53ms/step - loss: 1332.6141 - accuracy: 0.6974 - val_loss: 1332.4182 - val_accuracy: 0.6631\nEpoch 152/200\nlearning rate scheduled to 0.00021704500570194797\n203/203 [==============================] - 11s 52ms/step - loss: 1332.1437 - accuracy: 0.6972 - val_loss: 1331.9066 - val_accuracy: 0.6874\nEpoch 153/200\nlearning rate scheduled to 0.00021487455247552135\n203/203 [==============================] - 11s 53ms/step - loss: 1331.6769 - accuracy: 0.6974 - val_loss: 1331.4821 - val_accuracy: 0.6702\nEpoch 154/200\nlearning rate scheduled to 0.00021272580968798138\n203/203 [==============================] - 11s 52ms/step - loss: 1331.2152 - accuracy: 0.6964 - val_loss: 1330.9918 - val_accuracy: 0.6944\nEpoch 155/200\nlearning rate scheduled to 0.00021059854683699086\n203/203 [==============================] - 11s 52ms/step - loss: 1330.7594 - accuracy: 0.6936 - val_loss: 1330.5311 - val_accuracy: 0.6982\nEpoch 156/200\nlearning rate scheduled to 0.0002084925622330047\n203/203 [==============================] - 11s 52ms/step - loss: 1330.3064 - accuracy: 0.6951 - val_loss: 1330.1074 - val_accuracy: 0.6750\nEpoch 157/200\nlearning rate scheduled to 0.0002064076397800818\n203/203 [==============================] - 11s 52ms/step - loss: 1329.8588 - accuracy: 0.6967 - val_loss: 1329.7164 - val_accuracy: 0.6167\nEpoch 158/200\nlearning rate scheduled to 0.000204343563382281\n203/203 [==============================] - 11s 52ms/step - loss: 1329.4185 - accuracy: 0.6927 - val_loss: 1329.3256 - val_accuracy: 0.5437\nEpoch 159/200\nlearning rate scheduled to 0.0002023001313500572\n203/203 [==============================] - 11s 52ms/step - loss: 1329.0001 - accuracy: 0.6718 - val_loss: 1328.7578 - val_accuracy: 0.6965\nEpoch 160/200\nlearning rate scheduled to 0.0002002771275874693\n203/203 [==============================] - 11s 53ms/step - loss: 1328.5421 - accuracy: 0.6999 - val_loss: 1328.3357 - val_accuracy: 0.6872\nEpoch 161/200\nlearning rate scheduled to 0.0001982743504049722\n203/203 [==============================] - 11s 53ms/step - loss: 1328.1106 - accuracy: 0.6987 - val_loss: 1327.9596 - val_accuracy: 0.6331\nEpoch 162/200\nlearning rate scheduled to 0.00019629161251941696\n203/203 [==============================] - 11s 53ms/step - loss: 1327.6884 - accuracy: 0.6968 - val_loss: 1327.4810 - val_accuracy: 0.6899\nEpoch 163/200\nlearning rate scheduled to 0.0001943286978348624\n203/203 [==============================] - 11s 52ms/step - loss: 1327.2654 - accuracy: 0.6964 - val_loss: 1327.0522 - val_accuracy: 0.6980\nEpoch 164/200\nlearning rate scheduled to 0.00019238540466176346\n203/203 [==============================] - 11s 52ms/step - loss: 1326.8483 - accuracy: 0.6954 - val_loss: 1326.6434 - val_accuracy: 0.6914\nEpoch 165/200\nlearning rate scheduled to 0.00019046154571697116\n203/203 [==============================] - 11s 53ms/step - loss: 1326.4358 - accuracy: 0.6974 - val_loss: 1326.2461 - val_accuracy: 0.6793\nEpoch 166/200\nlearning rate scheduled to 0.0001885569337173365\n203/203 [==============================] - 11s 53ms/step - loss: 1326.0284 - accuracy: 0.6948 - val_loss: 1325.8148 - val_accuracy: 0.7098\nEpoch 167/200\nlearning rate scheduled to 0.00018667136697331444\n203/203 [==============================] - 11s 52ms/step - loss: 1325.6211 - accuracy: 0.6995 - val_loss: 1325.4451 - val_accuracy: 0.6785\nEpoch 168/200\nlearning rate scheduled to 0.00018480465820175596\n203/203 [==============================] - 11s 52ms/step - loss: 1325.2219 - accuracy: 0.6982 - val_loss: 1325.0383 - val_accuracy: 0.6846\nEpoch 169/200\nlearning rate scheduled to 0.000182956605713116\n203/203 [==============================] - 11s 52ms/step - loss: 1324.8270 - accuracy: 0.6972 - val_loss: 1324.6503 - val_accuracy: 0.6712\nEpoch 170/200\nlearning rate scheduled to 0.00018112703663064166\n203/203 [==============================] - 11s 52ms/step - loss: 1324.4379 - accuracy: 0.6962 - val_loss: 1324.2643 - val_accuracy: 0.6802\nEpoch 171/200\nlearning rate scheduled to 0.00017931576367118395\n203/203 [==============================] - 11s 52ms/step - loss: 1324.0494 - accuracy: 0.6988 - val_loss: 1323.8518 - val_accuracy: 0.7006\nEpoch 172/200\nlearning rate scheduled to 0.0001775225995515939\n203/203 [==============================] - 11s 52ms/step - loss: 1323.6650 - accuracy: 0.6973 - val_loss: 1323.4758 - val_accuracy: 0.6989\nEpoch 173/200\nlearning rate scheduled to 0.00017574737139511854\n203/203 [==============================] - 11s 52ms/step - loss: 1323.2853 - accuracy: 0.6998 - val_loss: 1323.0920 - val_accuracy: 0.7055\nEpoch 174/200\nlearning rate scheduled to 0.00017398989191860892\n203/203 [==============================] - 11s 52ms/step - loss: 1322.9124 - accuracy: 0.6986 - val_loss: 1322.7759 - val_accuracy: 0.6552\nEpoch 175/200\nlearning rate scheduled to 0.00017224998824531213\n203/203 [==============================] - 11s 52ms/step - loss: 1322.5443 - accuracy: 0.6944 - val_loss: 1322.3671 - val_accuracy: 0.6880\nEpoch 176/200\nlearning rate scheduled to 0.00017052748749847522\n203/203 [==============================] - 11s 53ms/step - loss: 1322.1733 - accuracy: 0.6969 - val_loss: 1321.9845 - val_accuracy: 0.6961\nEpoch 177/200\nlearning rate scheduled to 0.00016882221680134535\n203/203 [==============================] - 11s 52ms/step - loss: 1321.8083 - accuracy: 0.6997 - val_loss: 1321.6392 - val_accuracy: 0.6883\nEpoch 178/200\nlearning rate scheduled to 0.00016713398887077346\n203/203 [==============================] - 11s 52ms/step - loss: 1321.4509 - accuracy: 0.6963 - val_loss: 1321.2651 - val_accuracy: 0.6917\nEpoch 179/200\nlearning rate scheduled to 0.00016546264523640276\n203/203 [==============================] - 11s 52ms/step - loss: 1321.0862 - accuracy: 0.7031 - val_loss: 1320.9282 - val_accuracy: 0.6906\nEpoch 180/200\nlearning rate scheduled to 0.00016380801302148028\n203/203 [==============================] - 11s 52ms/step - loss: 1320.7360 - accuracy: 0.6974 - val_loss: 1320.5632 - val_accuracy: 0.7032\nEpoch 181/200\nlearning rate scheduled to 0.00016216993375564926\n203/203 [==============================] - 11s 52ms/step - loss: 1320.3833 - accuracy: 0.7001 - val_loss: 1320.2109 - val_accuracy: 0.6948\nEpoch 182/200\nlearning rate scheduled to 0.00016054823456215672\n203/203 [==============================] - 11s 52ms/step - loss: 1320.0374 - accuracy: 0.6996 - val_loss: 1319.8682 - val_accuracy: 0.7049\nEpoch 183/200\nlearning rate scheduled to 0.00015894275697064586\n203/203 [==============================] - 11s 52ms/step - loss: 1319.6941 - accuracy: 0.6997 - val_loss: 1319.5293 - val_accuracy: 0.6906\nEpoch 184/200\nlearning rate scheduled to 0.00015735332810436374\n203/203 [==============================] - 11s 52ms/step - loss: 1319.3564 - accuracy: 0.7008 - val_loss: 1319.1714 - val_accuracy: 0.7157\nEpoch 185/200\nlearning rate scheduled to 0.00015577978949295356\n203/203 [==============================] - 11s 53ms/step - loss: 1319.0184 - accuracy: 0.7018 - val_loss: 1318.8563 - val_accuracy: 0.6914\nEpoch 186/200\nlearning rate scheduled to 0.00015422199707245453\n203/203 [==============================] - 11s 52ms/step - loss: 1318.6907 - accuracy: 0.6987 - val_loss: 1318.5211 - val_accuracy: 0.6993\nEpoch 187/200\nlearning rate scheduled to 0.00015267977796611375\n203/203 [==============================] - 11s 53ms/step - loss: 1318.3610 - accuracy: 0.6991 - val_loss: 1318.2012 - val_accuracy: 0.7029\nEpoch 188/200\nlearning rate scheduled to 0.00015115297370357439\n203/203 [==============================] - 11s 53ms/step - loss: 1318.0366 - accuracy: 0.7005 - val_loss: 1317.8999 - val_accuracy: 0.6774\nEpoch 189/200\nlearning rate scheduled to 0.00014964144022087568\n203/203 [==============================] - 11s 52ms/step - loss: 1317.7140 - accuracy: 0.7016 - val_loss: 1317.5558 - val_accuracy: 0.6974\nEpoch 190/200\nlearning rate scheduled to 0.00014814501904766075\n203/203 [==============================] - 11s 53ms/step - loss: 1317.3983 - accuracy: 0.6965 - val_loss: 1317.2343 - val_accuracy: 0.7034\nEpoch 191/200\nlearning rate scheduled to 0.00014666356611996888\n203/203 [==============================] - 11s 53ms/step - loss: 1317.0825 - accuracy: 0.7013 - val_loss: 1316.9268 - val_accuracy: 0.7059\nEpoch 192/200\nlearning rate scheduled to 0.00014519693737383933\n203/203 [==============================] - 11s 52ms/step - loss: 1316.7694 - accuracy: 0.7028 - val_loss: 1316.6338 - val_accuracy: 0.6861\nEpoch 193/200\nlearning rate scheduled to 0.0001437449743389152\n203/203 [==============================] - 11s 53ms/step - loss: 1316.4625 - accuracy: 0.6995 - val_loss: 1316.3761 - val_accuracy: 0.6640\nEpoch 194/200\nlearning rate scheduled to 0.00014230751854483968\n203/203 [==============================] - 11s 52ms/step - loss: 1316.1606 - accuracy: 0.6972 - val_loss: 1316.0061 - val_accuracy: 0.6914\nEpoch 195/200\nlearning rate scheduled to 0.00014088444033404812\n203/203 [==============================] - 11s 52ms/step - loss: 1315.8536 - accuracy: 0.7022 - val_loss: 1315.7009 - val_accuracy: 0.7053\nEpoch 196/200\nlearning rate scheduled to 0.0001394755956425797\n203/203 [==============================] - 11s 53ms/step - loss: 1315.5541 - accuracy: 0.7031 - val_loss: 1315.4050 - val_accuracy: 0.7010\nEpoch 197/200\nlearning rate scheduled to 0.00013808084040647372\n203/203 [==============================] - 11s 53ms/step - loss: 1315.2576 - accuracy: 0.7015 - val_loss: 1315.1155 - val_accuracy: 0.6970\nEpoch 198/200\nlearning rate scheduled to 0.00013670003056176939\n203/203 [==============================] - 11s 52ms/step - loss: 1314.9650 - accuracy: 0.6997 - val_loss: 1314.8246 - val_accuracy: 0.6949\nEpoch 199/200\nlearning rate scheduled to 0.000135333036450902\n203/203 [==============================] - 11s 52ms/step - loss: 1314.6730 - accuracy: 0.7024 - val_loss: 1314.5323 - val_accuracy: 0.6985\nEpoch 200/200\nlearning rate scheduled to 0.00013397969960351474\n203/203 [==============================] - 11s 53ms/step - loss: 1314.3854 - accuracy: 0.7013 - val_loss: 1314.2648 - val_accuracy: 0.6778\n"
    }
   ],
   "source": [
    "history_original_siamese_model_4 = original_siamese_model_4.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                             model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "51/51 [==============================] - 2s 18ms/step - loss: 1314.2748 - accuracy: 0.6682\n"
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model_4.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fifth Run - 90k Pairs gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_90k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_90k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 105, 105, 1)]     0         \n_________________________________________________________________\nConv1 (Conv2D)               (None, 96, 96, 64)        6464      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 48, 48, 64)        0         \n_________________________________________________________________\nConv2 (Conv2D)               (None, 42, 42, 128)       401536    \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 21, 21, 128)       0         \n_________________________________________________________________\nConv3 (Conv2D)               (None, 18, 18, 128)       262272    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n_________________________________________________________________\nConv4 (Conv2D)               (None, 6, 6, 256)         524544    \n_________________________________________________________________\nflatten (Flatten)            (None, 9216)              0         \n_________________________________________________________________\nDense1 (Dense)               (None, 4096)              37752832  \n=================================================================\nTotal params: 38,947,648\nTrainable params: 38,947,648\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_5 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1cfrc6zk\" target=\"_blank\">dark-sun-11</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 90k - Grayscale\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "original_siamese_model_5.compile(loss=config.loss_function,\n",
    "                                 optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_90k_Gray\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/200\nlearning rate scheduled to 0.0009900000470224768\n610/610 [==============================] - 38s 53ms/step - loss: 1507.2825 - accuracy: 0.5475 - val_loss: 1503.6361 - val_accuracy: 0.5430\nEpoch 2/200\nlearning rate scheduled to 0.000980100086890161\n610/610 [==============================] - 34s 53ms/step - loss: 1500.0554 - accuracy: 0.5328 - val_loss: 1496.4590 - val_accuracy: 0.5303\nEpoch 3/200\nlearning rate scheduled to 0.0009702991275116801\n610/610 [==============================] - 34s 53ms/step - loss: 1492.9326 - accuracy: 0.5362 - val_loss: 1489.3882 - val_accuracy: 0.5388\nEpoch 4/200\nlearning rate scheduled to 0.0009605961316265165\n610/610 [==============================] - 34s 53ms/step - loss: 1485.9120 - accuracy: 0.5510 - val_loss: 1482.4197 - val_accuracy: 0.5687\nEpoch 5/200\nlearning rate scheduled to 0.0009509901772253215\n610/610 [==============================] - 34s 53ms/step - loss: 1478.9958 - accuracy: 0.5724 - val_loss: 1475.5557 - val_accuracy: 0.5787\nEpoch 6/200\nlearning rate scheduled to 0.0009414802846731617\n610/610 [==============================] - 34s 53ms/step - loss: 1472.1804 - accuracy: 0.5823 - val_loss: 1468.7883 - val_accuracy: 0.5871\nEpoch 7/200\nlearning rate scheduled to 0.0009320654743351042\n610/610 [==============================] - 34s 53ms/step - loss: 1465.4656 - accuracy: 0.5876 - val_loss: 1462.1254 - val_accuracy: 0.5923\nEpoch 8/200\nlearning rate scheduled to 0.0009227448242017999\n610/610 [==============================] - 34s 53ms/step - loss: 1458.8464 - accuracy: 0.5973 - val_loss: 1455.5537 - val_accuracy: 0.6005\nEpoch 9/200\nlearning rate scheduled to 0.0009135173546383158\n610/610 [==============================] - 34s 53ms/step - loss: 1452.3217 - accuracy: 0.6137 - val_loss: 1449.0704 - val_accuracy: 0.6263\nEpoch 10/200\nlearning rate scheduled to 0.0009043822012608871\n610/610 [==============================] - 34s 53ms/step - loss: 1445.8861 - accuracy: 0.6296 - val_loss: 1442.6852 - val_accuracy: 0.6374\nEpoch 11/200\nlearning rate scheduled to 0.0008953383844345808\n610/610 [==============================] - 34s 53ms/step - loss: 1439.5466 - accuracy: 0.6369 - val_loss: 1436.3905 - val_accuracy: 0.6404\nEpoch 12/200\nlearning rate scheduled to 0.000886384982150048\n610/610 [==============================] - 34s 53ms/step - loss: 1433.2946 - accuracy: 0.6456 - val_loss: 1430.1827 - val_accuracy: 0.6469\nEpoch 13/200\nlearning rate scheduled to 0.0008775211300235242\n610/610 [==============================] - 34s 53ms/step - loss: 1427.1323 - accuracy: 0.6529 - val_loss: 1424.0679 - val_accuracy: 0.6484\nEpoch 14/200\nlearning rate scheduled to 0.0008687459060456604\n610/610 [==============================] - 34s 53ms/step - loss: 1421.0590 - accuracy: 0.6577 - val_loss: 1418.0590 - val_accuracy: 0.6364\nEpoch 15/200\nlearning rate scheduled to 0.0008600584458326921\n610/610 [==============================] - 34s 53ms/step - loss: 1415.0747 - accuracy: 0.6605 - val_loss: 1412.1262 - val_accuracy: 0.6465\nEpoch 16/200\nlearning rate scheduled to 0.0008514578850008547\n610/610 [==============================] - 34s 53ms/step - loss: 1409.1764 - accuracy: 0.6636 - val_loss: 1406.2444 - val_accuracy: 0.6609\nEpoch 17/200\nlearning rate scheduled to 0.0008429433015407995\n610/610 [==============================] - 34s 53ms/step - loss: 1403.3593 - accuracy: 0.6671 - val_loss: 1400.4586 - val_accuracy: 0.6756\nEpoch 18/200\nlearning rate scheduled to 0.0008345138886943459\n610/610 [==============================] - 34s 53ms/step - loss: 1397.6266 - accuracy: 0.6691 - val_loss: 1394.7689 - val_accuracy: 0.6741\nEpoch 19/200\nlearning rate scheduled to 0.0008261687244521454\n610/610 [==============================] - 34s 53ms/step - loss: 1391.9756 - accuracy: 0.6705 - val_loss: 1389.1770 - val_accuracy: 0.6486\nEpoch 20/200\nlearning rate scheduled to 0.0008179070596816018\n610/610 [==============================] - 34s 52ms/step - loss: 1386.4011 - accuracy: 0.6745 - val_loss: 1383.7540 - val_accuracy: 0.5031\nEpoch 21/200\nlearning rate scheduled to 0.0008097279723733664\n610/610 [==============================] - 34s 53ms/step - loss: 1381.0121 - accuracy: 0.5324 - val_loss: 1378.2509 - val_accuracy: 0.6299\nEpoch 22/200\nlearning rate scheduled to 0.000801630713394843\n610/610 [==============================] - 34s 52ms/step - loss: 1375.5004 - accuracy: 0.6670 - val_loss: 1372.8046 - val_accuracy: 0.6678\nEpoch 23/200\nlearning rate scheduled to 0.0007936144183622674\n610/610 [==============================] - 34s 53ms/step - loss: 1370.1514 - accuracy: 0.6726 - val_loss: 1367.4934 - val_accuracy: 0.6787\nEpoch 24/200\nlearning rate scheduled to 0.0007856782805174589\n610/610 [==============================] - 34s 53ms/step - loss: 1364.8799 - accuracy: 0.6759 - val_loss: 1362.2526 - val_accuracy: 0.6797\nEpoch 25/200\nlearning rate scheduled to 0.0007778214931022376\n610/610 [==============================] - 34s 53ms/step - loss: 1359.6819 - accuracy: 0.6781 - val_loss: 1357.0930 - val_accuracy: 0.6848\nEpoch 26/200\nlearning rate scheduled to 0.0007700433069840073\n610/610 [==============================] - 34s 52ms/step - loss: 1354.5553 - accuracy: 0.6800 - val_loss: 1352.0170 - val_accuracy: 0.6686\nEpoch 27/200\nlearning rate scheduled to 0.0007623428577790037\n610/610 [==============================] - 34s 53ms/step - loss: 1349.5021 - accuracy: 0.6809 - val_loss: 1347.0203 - val_accuracy: 0.6591\nEpoch 28/200\nlearning rate scheduled to 0.0007547194539802149\n610/610 [==============================] - 34s 53ms/step - loss: 1344.5159 - accuracy: 0.6824 - val_loss: 1342.0361 - val_accuracy: 0.6777\nEpoch 29/200\nlearning rate scheduled to 0.0007471722312038764\n610/610 [==============================] - 34s 53ms/step - loss: 1339.5981 - accuracy: 0.6821 - val_loss: 1337.1476 - val_accuracy: 0.6823\nEpoch 30/200\nlearning rate scheduled to 0.0007397004979429766\n610/610 [==============================] - 34s 53ms/step - loss: 1334.7478 - accuracy: 0.6843 - val_loss: 1332.3521 - val_accuracy: 0.6688\nEpoch 31/200\nlearning rate scheduled to 0.0007323035050649196\n610/610 [==============================] - 34s 53ms/step - loss: 1329.9633 - accuracy: 0.6860 - val_loss: 1327.6147 - val_accuracy: 0.6660\nEpoch 32/200\nlearning rate scheduled to 0.000724980445811525\n610/610 [==============================] - 34s 53ms/step - loss: 1325.2435 - accuracy: 0.6862 - val_loss: 1322.8959 - val_accuracy: 0.6853\nEpoch 33/200\nlearning rate scheduled to 0.0007177306286757812\n610/610 [==============================] - 34s 53ms/step - loss: 1320.5862 - accuracy: 0.6886 - val_loss: 1318.2896 - val_accuracy: 0.6695\nEpoch 34/200\nlearning rate scheduled to 0.0007105533045250923\n610/610 [==============================] - 34s 53ms/step - loss: 1315.9928 - accuracy: 0.6884 - val_loss: 1313.8463 - val_accuracy: 0.6060\nEpoch 35/200\nlearning rate scheduled to 0.0007034477818524464\n610/610 [==============================] - 34s 53ms/step - loss: 1311.4630 - accuracy: 0.6887 - val_loss: 1309.2900 - val_accuracy: 0.6412\nEpoch 36/200\nlearning rate scheduled to 0.000696413311525248\n610/610 [==============================] - 34s 53ms/step - loss: 1306.9907 - accuracy: 0.6904 - val_loss: 1304.7594 - val_accuracy: 0.6925\nEpoch 37/200\nlearning rate scheduled to 0.0006894492020364851\n610/610 [==============================] - 34s 53ms/step - loss: 1302.5786 - accuracy: 0.6922 - val_loss: 1300.4629 - val_accuracy: 0.6161\nEpoch 38/200\nlearning rate scheduled to 0.0006825547042535618\n610/610 [==============================] - 34s 53ms/step - loss: 1298.2271 - accuracy: 0.6910 - val_loss: 1296.0573 - val_accuracy: 0.6961\nEpoch 39/200\nlearning rate scheduled to 0.0006757291842950508\n610/610 [==============================] - 34s 53ms/step - loss: 1293.9330 - accuracy: 0.6926 - val_loss: 1291.7932 - val_accuracy: 0.6966\nEpoch 40/200\nlearning rate scheduled to 0.0006689718930283561\n610/610 [==============================] - 34s 53ms/step - loss: 1289.6964 - accuracy: 0.6932 - val_loss: 1287.5839 - val_accuracy: 0.6925\nEpoch 41/200\nlearning rate scheduled to 0.0006622821965720505\n610/610 [==============================] - 34s 53ms/step - loss: 1285.5104 - accuracy: 0.6961 - val_loss: 1283.4495 - val_accuracy: 0.6823\nEpoch 42/200\nlearning rate scheduled to 0.0006556594034191221\n610/610 [==============================] - 34s 53ms/step - loss: 1281.3851 - accuracy: 0.6955 - val_loss: 1279.3361 - val_accuracy: 0.6919\nEpoch 43/200\nlearning rate scheduled to 0.0006491028220625594\n610/610 [==============================] - 34s 53ms/step - loss: 1277.3143 - accuracy: 0.6969 - val_loss: 1275.2850 - val_accuracy: 0.6976\nEpoch 44/200\nlearning rate scheduled to 0.0006426118186209351\n610/610 [==============================] - 34s 53ms/step - loss: 1273.2936 - accuracy: 0.6969 - val_loss: 1271.2938 - val_accuracy: 0.6974\nEpoch 45/200\nlearning rate scheduled to 0.0006361857015872375\n610/610 [==============================] - 34s 53ms/step - loss: 1269.3284 - accuracy: 0.6980 - val_loss: 1267.3649 - val_accuracy: 0.6936\nEpoch 46/200\nlearning rate scheduled to 0.0006298238370800391\n610/610 [==============================] - 34s 53ms/step - loss: 1265.4165 - accuracy: 0.6970 - val_loss: 1263.4657 - val_accuracy: 0.7017\nEpoch 47/200\nlearning rate scheduled to 0.0006235255912179127\n610/610 [==============================] - 34s 53ms/step - loss: 1261.5537 - accuracy: 0.6985 - val_loss: 1259.6301 - val_accuracy: 0.6984\nEpoch 48/200\nlearning rate scheduled to 0.000617290330119431\n610/610 [==============================] - 34s 53ms/step - loss: 1257.7382 - accuracy: 0.7017 - val_loss: 1255.8750 - val_accuracy: 0.6664\nEpoch 49/200\nlearning rate scheduled to 0.0006111174199031666\n610/610 [==============================] - 34s 52ms/step - loss: 1253.9767 - accuracy: 0.6991 - val_loss: 1252.1917 - val_accuracy: 0.6483\nEpoch 50/200\nlearning rate scheduled to 0.0006050062266876921\n610/610 [==============================] - 34s 53ms/step - loss: 1250.2627 - accuracy: 0.7005 - val_loss: 1248.4067 - val_accuracy: 0.7063\nEpoch 51/200\nlearning rate scheduled to 0.0005989561742171646\n610/610 [==============================] - 34s 53ms/step - loss: 1246.5955 - accuracy: 0.7003 - val_loss: 1244.7692 - val_accuracy: 0.6988\nEpoch 52/200\nlearning rate scheduled to 0.0005929666286101564\n610/610 [==============================] - 34s 53ms/step - loss: 1242.9771 - accuracy: 0.7018 - val_loss: 1241.1809 - val_accuracy: 0.6920\nEpoch 53/200\nlearning rate scheduled to 0.0005870369559852406\n610/610 [==============================] - 34s 53ms/step - loss: 1239.4025 - accuracy: 0.7014 - val_loss: 1237.6952 - val_accuracy: 0.6599\nEpoch 54/200\nlearning rate scheduled to 0.000581166580086574\n610/610 [==============================] - 34s 53ms/step - loss: 1235.8755 - accuracy: 0.7030 - val_loss: 1234.1466 - val_accuracy: 0.6768\nEpoch 55/200\nlearning rate scheduled to 0.0005753549246583134\n610/610 [==============================] - 34s 53ms/step - loss: 1232.3942 - accuracy: 0.7034 - val_loss: 1230.6749 - val_accuracy: 0.6940\nEpoch 56/200\nlearning rate scheduled to 0.0005696013558190316\n610/610 [==============================] - 34s 53ms/step - loss: 1228.9565 - accuracy: 0.7040 - val_loss: 1227.2417 - val_accuracy: 0.7079\nEpoch 57/200\nlearning rate scheduled to 0.0005639053549384699\n610/610 [==============================] - 34s 53ms/step - loss: 1225.5634 - accuracy: 0.7039 - val_loss: 1223.8872 - val_accuracy: 0.6983\nEpoch 58/200\nlearning rate scheduled to 0.0005582662881352008\n610/610 [==============================] - 34s 53ms/step - loss: 1222.2126 - accuracy: 0.7062 - val_loss: 1220.5410 - val_accuracy: 0.7049\nEpoch 59/200\nlearning rate scheduled to 0.0005526836367789656\n610/610 [==============================] - 34s 52ms/step - loss: 1218.9042 - accuracy: 0.7065 - val_loss: 1217.2539 - val_accuracy: 0.7120\nEpoch 60/200\nlearning rate scheduled to 0.0005471568246139213\n610/610 [==============================] - 34s 53ms/step - loss: 1215.6381 - accuracy: 0.7076 - val_loss: 1214.0082 - val_accuracy: 0.7042\nEpoch 61/200\nlearning rate scheduled to 0.000541685275384225\n610/610 [==============================] - 34s 52ms/step - loss: 1212.4163 - accuracy: 0.7066 - val_loss: 1210.8093 - val_accuracy: 0.7044\nEpoch 62/200\nlearning rate scheduled to 0.0005362684128340334\n610/610 [==============================] - 34s 53ms/step - loss: 1209.2311 - accuracy: 0.7094 - val_loss: 1207.6630 - val_accuracy: 0.6975\nEpoch 63/200\nlearning rate scheduled to 0.0005309057183330878\n610/610 [==============================] - 34s 53ms/step - loss: 1206.0865 - accuracy: 0.7083 - val_loss: 1204.5376 - val_accuracy: 0.6954\nEpoch 64/200\nlearning rate scheduled to 0.0005255966732511297\n610/610 [==============================] - 34s 53ms/step - loss: 1202.9821 - accuracy: 0.7096 - val_loss: 1201.6295 - val_accuracy: 0.5993\nEpoch 65/200\nlearning rate scheduled to 0.0005203407013323158\n610/610 [==============================] - 34s 53ms/step - loss: 1199.9178 - accuracy: 0.7098 - val_loss: 1198.3855 - val_accuracy: 0.7132\nEpoch 66/200\nlearning rate scheduled to 0.0005151372839463875\n610/610 [==============================] - 34s 53ms/step - loss: 1196.8888 - accuracy: 0.7112 - val_loss: 1195.4032 - val_accuracy: 0.6913\nEpoch 67/200\nlearning rate scheduled to 0.0005099859024630859\n610/610 [==============================] - 34s 53ms/step - loss: 1193.9003 - accuracy: 0.7124 - val_loss: 1192.4128 - val_accuracy: 0.7047\nEpoch 68/200\nlearning rate scheduled to 0.0005048860382521525\n610/610 [==============================] - 34s 53ms/step - loss: 1190.9469 - accuracy: 0.7139 - val_loss: 1189.4775 - val_accuracy: 0.7115\nEpoch 69/200\nlearning rate scheduled to 0.0004998371726833284\n610/610 [==============================] - 34s 53ms/step - loss: 1188.0325 - accuracy: 0.7138 - val_loss: 1186.5734 - val_accuracy: 0.7145\nEpoch 70/200\nlearning rate scheduled to 0.0004948387871263549\n610/610 [==============================] - 34s 53ms/step - loss: 1185.1526 - accuracy: 0.7142 - val_loss: 1183.7180 - val_accuracy: 0.7137\nEpoch 71/200\nlearning rate scheduled to 0.0004898904205765575\n610/610 [==============================] - 34s 53ms/step - loss: 1182.3088 - accuracy: 0.7141 - val_loss: 1180.8916 - val_accuracy: 0.7190\nEpoch 72/200\nlearning rate scheduled to 0.00048499149677809326\n610/610 [==============================] - 34s 53ms/step - loss: 1179.4988 - accuracy: 0.7168 - val_loss: 1178.0967 - val_accuracy: 0.7204\nEpoch 73/200\nlearning rate scheduled to 0.00048014158353907986\n610/610 [==============================] - 34s 53ms/step - loss: 1176.7272 - accuracy: 0.7151 - val_loss: 1175.3494 - val_accuracy: 0.7087\nEpoch 74/200\nlearning rate scheduled to 0.00047534016222925855\n610/610 [==============================] - 34s 53ms/step - loss: 1173.9871 - accuracy: 0.7177 - val_loss: 1172.6309 - val_accuracy: 0.7101\nEpoch 75/200\nlearning rate scheduled to 0.0004705867718439549\n610/610 [==============================] - 34s 53ms/step - loss: 1171.2820 - accuracy: 0.7168 - val_loss: 1169.9407 - val_accuracy: 0.7083\nEpoch 76/200\nlearning rate scheduled to 0.0004658808937529102\n610/610 [==============================] - 34s 53ms/step - loss: 1168.6089 - accuracy: 0.7181 - val_loss: 1167.2717 - val_accuracy: 0.7206\nEpoch 77/200\nlearning rate scheduled to 0.0004612220957642421\n610/610 [==============================] - 34s 53ms/step - loss: 1165.9683 - accuracy: 0.7175 - val_loss: 1164.6523 - val_accuracy: 0.7209\nEpoch 78/200\nlearning rate scheduled to 0.00045660988806048406\n610/610 [==============================] - 34s 53ms/step - loss: 1163.3588 - accuracy: 0.7191 - val_loss: 1162.0582 - val_accuracy: 0.7239\nEpoch 79/200\nlearning rate scheduled to 0.0004520437808241695\n610/610 [==============================] - 34s 53ms/step - loss: 1160.7844 - accuracy: 0.7191 - val_loss: 1159.5454 - val_accuracy: 0.6806\nEpoch 80/200\nlearning rate scheduled to 0.0004475233418634161\n610/610 [==============================] - 34s 53ms/step - loss: 1158.2397 - accuracy: 0.7195 - val_loss: 1156.9979 - val_accuracy: 0.7035\nEpoch 81/200\nlearning rate scheduled to 0.0004430481101735495\n610/610 [==============================] - 34s 53ms/step - loss: 1155.7267 - accuracy: 0.7213 - val_loss: 1154.5065 - val_accuracy: 0.7036\nEpoch 82/200\nlearning rate scheduled to 0.0004386176247498952\n610/610 [==============================] - 34s 53ms/step - loss: 1153.2423 - accuracy: 0.7221 - val_loss: 1152.0035 - val_accuracy: 0.7237\nEpoch 83/200\nlearning rate scheduled to 0.00043423145340057087\n610/610 [==============================] - 34s 52ms/step - loss: 1150.7875 - accuracy: 0.7242 - val_loss: 1149.5648 - val_accuracy: 0.7212\nEpoch 84/200\nlearning rate scheduled to 0.0004298891351209022\n610/610 [==============================] - 34s 53ms/step - loss: 1148.3649 - accuracy: 0.7228 - val_loss: 1147.1592 - val_accuracy: 0.7212\nEpoch 85/200\nlearning rate scheduled to 0.00042559023771900686\n610/610 [==============================] - 34s 53ms/step - loss: 1145.9712 - accuracy: 0.7228 - val_loss: 1144.7997 - val_accuracy: 0.7078\nEpoch 86/200\nlearning rate scheduled to 0.0004213343290030025\n610/610 [==============================] - 34s 53ms/step - loss: 1143.6058 - accuracy: 0.7251 - val_loss: 1142.4534 - val_accuracy: 0.7053\nEpoch 87/200\nlearning rate scheduled to 0.00041712097678100687\n610/610 [==============================] - 34s 53ms/step - loss: 1141.2699 - accuracy: 0.7236 - val_loss: 1140.1464 - val_accuracy: 0.6913\nEpoch 88/200\nlearning rate scheduled to 0.00041294977767392994\n610/610 [==============================] - 34s 53ms/step - loss: 1138.9601 - accuracy: 0.7273 - val_loss: 1137.8104 - val_accuracy: 0.7237\nEpoch 89/200\nlearning rate scheduled to 0.0004088202706770971\n610/610 [==============================] - 34s 53ms/step - loss: 1136.6801 - accuracy: 0.7260 - val_loss: 1135.5422 - val_accuracy: 0.7274\nEpoch 90/200\nlearning rate scheduled to 0.00040473208122421056\n610/610 [==============================] - 34s 53ms/step - loss: 1134.4250 - accuracy: 0.7279 - val_loss: 1133.2970 - val_accuracy: 0.7262\nEpoch 91/200\nlearning rate scheduled to 0.0004006847483105957\n610/610 [==============================] - 34s 53ms/step - loss: 1132.1976 - accuracy: 0.7273 - val_loss: 1131.0891 - val_accuracy: 0.7275\nEpoch 92/200\nlearning rate scheduled to 0.0003966778973699547\n610/610 [==============================] - 34s 53ms/step - loss: 1129.9976 - accuracy: 0.7290 - val_loss: 1128.8976 - val_accuracy: 0.7312\nEpoch 93/200\nlearning rate scheduled to 0.0003927111250231974\n610/610 [==============================] - 34s 53ms/step - loss: 1127.8235 - accuracy: 0.7301 - val_loss: 1126.7474 - val_accuracy: 0.7240\nEpoch 94/200\nlearning rate scheduled to 0.0003887840278912336\n610/610 [==============================] - 34s 53ms/step - loss: 1125.6750 - accuracy: 0.7298 - val_loss: 1124.6229 - val_accuracy: 0.7184\nEpoch 95/200\nlearning rate scheduled to 0.000384896173782181\n610/610 [==============================] - 34s 53ms/step - loss: 1123.5522 - accuracy: 0.7309 - val_loss: 1122.5071 - val_accuracy: 0.7201\nEpoch 96/200\nlearning rate scheduled to 0.00038104721694253384\n610/610 [==============================] - 34s 53ms/step - loss: 1121.4554 - accuracy: 0.7309 - val_loss: 1120.4081 - val_accuracy: 0.7367\nEpoch 97/200\nlearning rate scheduled to 0.000377236753993202\n610/610 [==============================] - 34s 53ms/step - loss: 1119.3815 - accuracy: 0.7328 - val_loss: 1118.3536 - val_accuracy: 0.7299\nEpoch 98/200\nlearning rate scheduled to 0.0003734643815550953\n610/610 [==============================] - 34s 53ms/step - loss: 1117.3337 - accuracy: 0.7332 - val_loss: 1116.3107 - val_accuracy: 0.7349\nEpoch 99/200\nlearning rate scheduled to 0.0003697297250619158\n610/610 [==============================] - 34s 53ms/step - loss: 1115.3099 - accuracy: 0.7327 - val_loss: 1114.2981 - val_accuracy: 0.7389\nEpoch 100/200\nlearning rate scheduled to 0.0003660324387601577\n610/610 [==============================] - 34s 53ms/step - loss: 1113.3087 - accuracy: 0.7336 - val_loss: 1112.3165 - val_accuracy: 0.7280\nEpoch 101/200\nlearning rate scheduled to 0.00036237211927073074\n610/610 [==============================] - 34s 53ms/step - loss: 1111.3320 - accuracy: 0.7326 - val_loss: 1110.3597 - val_accuracy: 0.7194\nEpoch 102/200\nlearning rate scheduled to 0.0003587483920273371\n610/610 [==============================] - 34s 53ms/step - loss: 1109.3787 - accuracy: 0.7338 - val_loss: 1108.4075 - val_accuracy: 0.7336\nEpoch 103/200\nlearning rate scheduled to 0.0003551609112764709\n610/610 [==============================] - 34s 53ms/step - loss: 1107.4492 - accuracy: 0.7355 - val_loss: 1106.4810 - val_accuracy: 0.7383\nEpoch 104/200\nlearning rate scheduled to 0.0003516093024518341\n610/610 [==============================] - 34s 53ms/step - loss: 1105.5414 - accuracy: 0.7353 - val_loss: 1104.5933 - val_accuracy: 0.7344\nEpoch 105/200\nlearning rate scheduled to 0.0003480932197999209\n610/610 [==============================] - 34s 53ms/step - loss: 1103.6555 - accuracy: 0.7360 - val_loss: 1102.7113 - val_accuracy: 0.7387\nEpoch 106/200\nlearning rate scheduled to 0.0003446122887544334\n610/610 [==============================] - 34s 53ms/step - loss: 1101.7908 - accuracy: 0.7366 - val_loss: 1100.8590 - val_accuracy: 0.7396\nEpoch 107/200\nlearning rate scheduled to 0.00034116616356186566\n610/610 [==============================] - 34s 53ms/step - loss: 1099.9487 - accuracy: 0.7378 - val_loss: 1099.0408 - val_accuracy: 0.7295\nEpoch 108/200\nlearning rate scheduled to 0.00033775449846871195\n610/610 [==============================] - 34s 53ms/step - loss: 1098.1290 - accuracy: 0.7373 - val_loss: 1097.2279 - val_accuracy: 0.7274\nEpoch 109/200\nlearning rate scheduled to 0.0003343769477214664\n610/610 [==============================] - 34s 52ms/step - loss: 1096.3301 - accuracy: 0.7367 - val_loss: 1095.4465 - val_accuracy: 0.7277\nEpoch 110/200\nlearning rate scheduled to 0.0003310331655666232\n610/610 [==============================] - 34s 53ms/step - loss: 1094.5524 - accuracy: 0.7381 - val_loss: 1093.6656 - val_accuracy: 0.7372\nEpoch 111/200\nlearning rate scheduled to 0.00032772283506346864\n610/610 [==============================] - 34s 53ms/step - loss: 1092.7938 - accuracy: 0.7375 - val_loss: 1091.9346 - val_accuracy: 0.7275\nEpoch 112/200\nlearning rate scheduled to 0.00032444561045849693\n610/610 [==============================] - 34s 53ms/step - loss: 1091.0583 - accuracy: 0.7379 - val_loss: 1090.1898 - val_accuracy: 0.7407\nEpoch 113/200\nlearning rate scheduled to 0.00032120114599820226\n610/610 [==============================] - 34s 53ms/step - loss: 1089.3396 - accuracy: 0.7392 - val_loss: 1088.5018 - val_accuracy: 0.7264\nEpoch 114/200\nlearning rate scheduled to 0.00031798912474187093\n610/610 [==============================] - 34s 53ms/step - loss: 1087.6432 - accuracy: 0.7401 - val_loss: 1086.8303 - val_accuracy: 0.7216\nEpoch 115/200\nlearning rate scheduled to 0.0003148092297487892\n610/610 [==============================] - 34s 53ms/step - loss: 1085.9645 - accuracy: 0.7390 - val_loss: 1085.1272 - val_accuracy: 0.7392\nEpoch 116/200\nlearning rate scheduled to 0.0003116611440782435\n610/610 [==============================] - 34s 53ms/step - loss: 1084.3064 - accuracy: 0.7395 - val_loss: 1083.4957 - val_accuracy: 0.7287\nEpoch 117/200\nlearning rate scheduled to 0.000308544521976728\n610/610 [==============================] - 34s 53ms/step - loss: 1082.6653 - accuracy: 0.7410 - val_loss: 1081.8588 - val_accuracy: 0.7348\nEpoch 118/200\nlearning rate scheduled to 0.0003054590753163211\n610/610 [==============================] - 34s 53ms/step - loss: 1081.0452 - accuracy: 0.7404 - val_loss: 1080.2418 - val_accuracy: 0.7405\nEpoch 119/200\nlearning rate scheduled to 0.0003024044871563092\n610/610 [==============================] - 34s 53ms/step - loss: 1079.4430 - accuracy: 0.7424 - val_loss: 1078.6482 - val_accuracy: 0.7382\nEpoch 120/200\nlearning rate scheduled to 0.00029938044055597855\n610/610 [==============================] - 34s 53ms/step - loss: 1077.8610 - accuracy: 0.7409 - val_loss: 1077.0698 - val_accuracy: 0.7468\nEpoch 121/200\nlearning rate scheduled to 0.0002963866473874077\n610/610 [==============================] - 34s 53ms/step - loss: 1076.2957 - accuracy: 0.7420 - val_loss: 1075.5150 - val_accuracy: 0.7436\nEpoch 122/200\nlearning rate scheduled to 0.000293422790709883\n610/610 [==============================] - 34s 53ms/step - loss: 1074.7484 - accuracy: 0.7431 - val_loss: 1073.9817 - val_accuracy: 0.7364\nEpoch 123/200\nlearning rate scheduled to 0.00029048855358269063\n610/610 [==============================] - 34s 53ms/step - loss: 1073.2198 - accuracy: 0.7420 - val_loss: 1072.4856 - val_accuracy: 0.7174\nEpoch 124/200\nlearning rate scheduled to 0.0002875836766907014\n610/610 [==============================] - 34s 53ms/step - loss: 1071.7050 - accuracy: 0.7444 - val_loss: 1070.9489 - val_accuracy: 0.7479\nEpoch 125/200\nlearning rate scheduled to 0.0002847078430932015\n610/610 [==============================] - 34s 53ms/step - loss: 1070.2109 - accuracy: 0.7431 - val_loss: 1069.4609 - val_accuracy: 0.7464\nEpoch 126/200\nlearning rate scheduled to 0.0002818607646622695\n610/610 [==============================] - 34s 53ms/step - loss: 1068.7322 - accuracy: 0.7428 - val_loss: 1068.0593 - val_accuracy: 0.7038\nEpoch 127/200\nlearning rate scheduled to 0.00027904215326998385\n610/610 [==============================] - 34s 53ms/step - loss: 1067.2705 - accuracy: 0.7424 - val_loss: 1066.5375 - val_accuracy: 0.7479\nEpoch 128/200\nlearning rate scheduled to 0.00027625172078842296\n610/610 [==============================] - 34s 53ms/step - loss: 1065.8228 - accuracy: 0.7475 - val_loss: 1065.1166 - val_accuracy: 0.7333\nEpoch 129/200\nlearning rate scheduled to 0.0002734892079024576\n610/610 [==============================] - 34s 53ms/step - loss: 1064.3955 - accuracy: 0.7455 - val_loss: 1063.6876 - val_accuracy: 0.7431\nEpoch 130/200\nlearning rate scheduled to 0.0002707543264841661\n610/610 [==============================] - 34s 53ms/step - loss: 1062.9829 - accuracy: 0.7461 - val_loss: 1062.2931 - val_accuracy: 0.7359\nEpoch 131/200\nlearning rate scheduled to 0.000268046788405627\n610/610 [==============================] - 34s 53ms/step - loss: 1061.5881 - accuracy: 0.7452 - val_loss: 1060.8859 - val_accuracy: 0.7518\nEpoch 132/200\nlearning rate scheduled to 0.000265366334351711\n610/610 [==============================] - 34s 53ms/step - loss: 1060.2076 - accuracy: 0.7459 - val_loss: 1059.5204 - val_accuracy: 0.7477\nEpoch 133/200\nlearning rate scheduled to 0.00026271267619449646\n610/610 [==============================] - 34s 53ms/step - loss: 1058.8436 - accuracy: 0.7476 - val_loss: 1058.1669 - val_accuracy: 0.7450\nEpoch 134/200\nlearning rate scheduled to 0.00026008555461885406\n610/610 [==============================] - 34s 53ms/step - loss: 1057.4943 - accuracy: 0.7472 - val_loss: 1056.8220 - val_accuracy: 0.7456\nEpoch 135/200\nlearning rate scheduled to 0.00025748471030965445\n610/610 [==============================] - 34s 53ms/step - loss: 1056.1587 - accuracy: 0.7478 - val_loss: 1055.5031 - val_accuracy: 0.7399\nEpoch 136/200\nlearning rate scheduled to 0.0002549098551389761\n610/610 [==============================] - 34s 53ms/step - loss: 1054.8396 - accuracy: 0.7493 - val_loss: 1054.2216 - val_accuracy: 0.7267\nEpoch 137/200\nlearning rate scheduled to 0.00025236075860448183\n610/610 [==============================] - 34s 53ms/step - loss: 1053.5339 - accuracy: 0.7481 - val_loss: 1052.8923 - val_accuracy: 0.7405\nEpoch 138/200\nlearning rate scheduled to 0.0002498371613910422\n610/610 [==============================] - 34s 53ms/step - loss: 1052.2439 - accuracy: 0.7479 - val_loss: 1051.6013 - val_accuracy: 0.7456\nEpoch 139/200\nlearning rate scheduled to 0.0002473388041835278\n610/610 [==============================] - 34s 52ms/step - loss: 1050.9677 - accuracy: 0.7475 - val_loss: 1050.3424 - val_accuracy: 0.7422\nEpoch 140/200\nlearning rate scheduled to 0.0002448654276668094\n610/610 [==============================] - 34s 53ms/step - loss: 1049.7065 - accuracy: 0.7478 - val_loss: 1049.0748 - val_accuracy: 0.7519\nEpoch 141/200\nlearning rate scheduled to 0.00024241677252575755\n610/610 [==============================] - 34s 53ms/step - loss: 1048.4603 - accuracy: 0.7496 - val_loss: 1047.8376 - val_accuracy: 0.7504\nEpoch 142/200\nlearning rate scheduled to 0.00023999260825803502\n610/610 [==============================] - 34s 53ms/step - loss: 1047.2274 - accuracy: 0.7494 - val_loss: 1046.6101 - val_accuracy: 0.7550\nEpoch 143/200\nlearning rate scheduled to 0.00023759267554851249\n610/610 [==============================] - 34s 52ms/step - loss: 1046.0073 - accuracy: 0.7499 - val_loss: 1045.4062 - val_accuracy: 0.7474\nEpoch 144/200\nlearning rate scheduled to 0.0002352167438948527\n610/610 [==============================] - 34s 53ms/step - loss: 1044.8038 - accuracy: 0.7487 - val_loss: 1044.2070 - val_accuracy: 0.7459\nEpoch 145/200\nlearning rate scheduled to 0.00023286458279471843\n610/610 [==============================] - 34s 53ms/step - loss: 1043.6100 - accuracy: 0.7503 - val_loss: 1043.0165 - val_accuracy: 0.7498\nEpoch 146/200\nlearning rate scheduled to 0.00023053593293298035\n610/610 [==============================] - 34s 53ms/step - loss: 1042.4325 - accuracy: 0.7503 - val_loss: 1041.8472 - val_accuracy: 0.7475\nEpoch 147/200\nlearning rate scheduled to 0.0002282305782136973\n610/610 [==============================] - 34s 53ms/step - loss: 1041.2665 - accuracy: 0.7505 - val_loss: 1040.6809 - val_accuracy: 0.7554\nEpoch 148/200\nlearning rate scheduled to 0.00022594827372813598\n610/610 [==============================] - 34s 53ms/step - loss: 1040.1135 - accuracy: 0.7496 - val_loss: 1039.5353 - val_accuracy: 0.7513\nEpoch 149/200\nlearning rate scheduled to 0.00022368878897395915\n610/610 [==============================] - 34s 53ms/step - loss: 1038.9714 - accuracy: 0.7498 - val_loss: 1038.4152 - val_accuracy: 0.7402\nEpoch 150/200\nlearning rate scheduled to 0.00022145190785522573\n610/610 [==============================] - 34s 53ms/step - loss: 1037.8403 - accuracy: 0.7528 - val_loss: 1037.2798 - val_accuracy: 0.7519\nEpoch 151/200\nlearning rate scheduled to 0.00021923738546320237\n610/610 [==============================] - 34s 53ms/step - loss: 1036.7262 - accuracy: 0.7526 - val_loss: 1036.1714 - val_accuracy: 0.7502\nEpoch 152/200\nlearning rate scheduled to 0.00021704500570194797\n610/610 [==============================] - 34s 53ms/step - loss: 1035.6228 - accuracy: 0.7511 - val_loss: 1035.0665 - val_accuracy: 0.7561\nEpoch 153/200\nlearning rate scheduled to 0.00021487455247552135\n610/610 [==============================] - 34s 53ms/step - loss: 1034.5331 - accuracy: 0.7515 - val_loss: 1033.9873 - val_accuracy: 0.7548\nEpoch 154/200\nlearning rate scheduled to 0.00021272580968798138\n610/610 [==============================] - 34s 53ms/step - loss: 1033.4536 - accuracy: 0.7535 - val_loss: 1032.9137 - val_accuracy: 0.7568\nEpoch 155/200\nlearning rate scheduled to 0.00021059854683699086\n610/610 [==============================] - 34s 53ms/step - loss: 1032.3884 - accuracy: 0.7533 - val_loss: 1031.8571 - val_accuracy: 0.7528\nEpoch 156/200\nlearning rate scheduled to 0.0002084925622330047\n610/610 [==============================] - 34s 53ms/step - loss: 1031.3340 - accuracy: 0.7546 - val_loss: 1030.8104 - val_accuracy: 0.7509\nEpoch 157/200\nlearning rate scheduled to 0.0002064076397800818\n610/610 [==============================] - 34s 53ms/step - loss: 1030.2908 - accuracy: 0.7535 - val_loss: 1029.8015 - val_accuracy: 0.7355\nEpoch 158/200\nlearning rate scheduled to 0.000204343563382281\n610/610 [==============================] - 34s 53ms/step - loss: 1029.2609 - accuracy: 0.7525 - val_loss: 1028.7767 - val_accuracy: 0.7335\nEpoch 159/200\nlearning rate scheduled to 0.0002023001313500572\n610/610 [==============================] - 34s 53ms/step - loss: 1028.2422 - accuracy: 0.7522 - val_loss: 1027.7416 - val_accuracy: 0.7472\nEpoch 160/200\nlearning rate scheduled to 0.0002002771275874693\n610/610 [==============================] - 34s 53ms/step - loss: 1027.2316 - accuracy: 0.7534 - val_loss: 1026.7271 - val_accuracy: 0.7550\nEpoch 161/200\nlearning rate scheduled to 0.0001982743504049722\n610/610 [==============================] - 34s 53ms/step - loss: 1026.2318 - accuracy: 0.7543 - val_loss: 1025.7310 - val_accuracy: 0.7557\nEpoch 162/200\nlearning rate scheduled to 0.00019629161251941696\n610/610 [==============================] - 34s 53ms/step - loss: 1025.2448 - accuracy: 0.7535 - val_loss: 1024.7494 - val_accuracy: 0.7546\nEpoch 163/200\nlearning rate scheduled to 0.0001943286978348624\n610/610 [==============================] - 34s 53ms/step - loss: 1024.2665 - accuracy: 0.7537 - val_loss: 1023.7784 - val_accuracy: 0.7561\nEpoch 164/200\nlearning rate scheduled to 0.00019238540466176346\n610/610 [==============================] - 34s 53ms/step - loss: 1023.2990 - accuracy: 0.7549 - val_loss: 1022.8117 - val_accuracy: 0.7579\nEpoch 165/200\nlearning rate scheduled to 0.00019046154571697116\n610/610 [==============================] - 34s 53ms/step - loss: 1022.3438 - accuracy: 0.7552 - val_loss: 1021.8702 - val_accuracy: 0.7508\nEpoch 166/200\nlearning rate scheduled to 0.0001885569337173365\n610/610 [==============================] - 34s 53ms/step - loss: 1021.3966 - accuracy: 0.7561 - val_loss: 1020.9617 - val_accuracy: 0.7282\nEpoch 167/200\nlearning rate scheduled to 0.00018667136697331444\n610/610 [==============================] - 34s 53ms/step - loss: 1020.4631 - accuracy: 0.7552 - val_loss: 1019.9968 - val_accuracy: 0.7554\nEpoch 168/200\nlearning rate scheduled to 0.00018480465820175596\n610/610 [==============================] - 34s 53ms/step - loss: 1019.5379 - accuracy: 0.7558 - val_loss: 1019.0983 - val_accuracy: 0.7406\nEpoch 169/200\nlearning rate scheduled to 0.000182956605713116\n610/610 [==============================] - 34s 53ms/step - loss: 1018.6247 - accuracy: 0.7571 - val_loss: 1018.1698 - val_accuracy: 0.7543\nEpoch 170/200\nlearning rate scheduled to 0.00018112703663064166\n610/610 [==============================] - 34s 53ms/step - loss: 1017.7217 - accuracy: 0.7570 - val_loss: 1017.2754 - val_accuracy: 0.7507\nEpoch 171/200\nlearning rate scheduled to 0.00017931576367118395\n610/610 [==============================] - 34s 53ms/step - loss: 1016.8258 - accuracy: 0.7577 - val_loss: 1016.3994 - val_accuracy: 0.7449\nEpoch 172/200\nlearning rate scheduled to 0.0001775225995515939\n610/610 [==============================] - 34s 53ms/step - loss: 1015.9436 - accuracy: 0.7572 - val_loss: 1015.4929 - val_accuracy: 0.7680\nEpoch 173/200\nlearning rate scheduled to 0.00017574737139511854\n610/610 [==============================] - 34s 53ms/step - loss: 1015.0699 - accuracy: 0.7565 - val_loss: 1014.6274 - val_accuracy: 0.7630\nEpoch 174/200\nlearning rate scheduled to 0.00017398989191860892\n610/610 [==============================] - 34s 53ms/step - loss: 1014.2036 - accuracy: 0.7573 - val_loss: 1013.7682 - val_accuracy: 0.7591\nEpoch 175/200\nlearning rate scheduled to 0.00017224998824531213\n610/610 [==============================] - 34s 52ms/step - loss: 1013.3482 - accuracy: 0.7570 - val_loss: 1012.9226 - val_accuracy: 0.7591\nEpoch 176/200\nlearning rate scheduled to 0.00017052748749847522\n610/610 [==============================] - 34s 53ms/step - loss: 1012.5023 - accuracy: 0.7575 - val_loss: 1012.0834 - val_accuracy: 0.7568\nEpoch 177/200\nlearning rate scheduled to 0.00016882221680134535\n610/610 [==============================] - 34s 53ms/step - loss: 1011.6635 - accuracy: 0.7573 - val_loss: 1011.2475 - val_accuracy: 0.7584\nEpoch 178/200\nlearning rate scheduled to 0.00016713398887077346\n610/610 [==============================] - 34s 53ms/step - loss: 1010.8351 - accuracy: 0.7577 - val_loss: 1010.4235 - val_accuracy: 0.7555\nEpoch 179/200\nlearning rate scheduled to 0.00016546264523640276\n610/610 [==============================] - 34s 53ms/step - loss: 1010.0145 - accuracy: 0.7570 - val_loss: 1009.6021 - val_accuracy: 0.7599\nEpoch 180/200\nlearning rate scheduled to 0.00016380801302148028\n610/610 [==============================] - 34s 53ms/step - loss: 1009.2001 - accuracy: 0.7579 - val_loss: 1008.8018 - val_accuracy: 0.7553\nEpoch 181/200\nlearning rate scheduled to 0.00016216993375564926\n610/610 [==============================] - 34s 53ms/step - loss: 1008.3965 - accuracy: 0.7597 - val_loss: 1007.9979 - val_accuracy: 0.7609\nEpoch 182/200\nlearning rate scheduled to 0.00016054823456215672\n610/610 [==============================] - 34s 53ms/step - loss: 1007.6003 - accuracy: 0.7594 - val_loss: 1007.2138 - val_accuracy: 0.7556\nEpoch 183/200\nlearning rate scheduled to 0.00015894275697064586\n610/610 [==============================] - 34s 53ms/step - loss: 1006.8149 - accuracy: 0.7597 - val_loss: 1006.4240 - val_accuracy: 0.7596\nEpoch 184/200\nlearning rate scheduled to 0.00015735332810436374\n610/610 [==============================] - 34s 53ms/step - loss: 1006.0395 - accuracy: 0.7595 - val_loss: 1005.6462 - val_accuracy: 0.7618\nEpoch 185/200\nlearning rate scheduled to 0.00015577978949295356\n610/610 [==============================] - 34s 53ms/step - loss: 1005.2712 - accuracy: 0.7605 - val_loss: 1004.8880 - val_accuracy: 0.7586\nEpoch 186/200\nlearning rate scheduled to 0.00015422199707245453\n610/610 [==============================] - 34s 53ms/step - loss: 1004.5131 - accuracy: 0.7590 - val_loss: 1004.1346 - val_accuracy: 0.7590\nEpoch 187/200\nlearning rate scheduled to 0.00015267977796611375\n610/610 [==============================] - 34s 53ms/step - loss: 1003.7601 - accuracy: 0.7612 - val_loss: 1003.3827 - val_accuracy: 0.7628\nEpoch 188/200\nlearning rate scheduled to 0.00015115297370357439\n610/610 [==============================] - 34s 53ms/step - loss: 1003.0175 - accuracy: 0.7620 - val_loss: 1002.6480 - val_accuracy: 0.7652\nEpoch 189/200\nlearning rate scheduled to 0.00014964144022087568\n610/610 [==============================] - 34s 53ms/step - loss: 1002.2830 - accuracy: 0.7606 - val_loss: 1001.9189 - val_accuracy: 0.7602\nEpoch 190/200\nlearning rate scheduled to 0.00014814501904766075\n610/610 [==============================] - 34s 53ms/step - loss: 1001.5573 - accuracy: 0.7612 - val_loss: 1001.1948 - val_accuracy: 0.7585\nEpoch 191/200\nlearning rate scheduled to 0.00014666356611996888\n610/610 [==============================] - 34s 53ms/step - loss: 1000.8370 - accuracy: 0.7617 - val_loss: 1000.4746 - val_accuracy: 0.7649\nEpoch 192/200\nlearning rate scheduled to 0.00014519693737383933\n610/610 [==============================] - 34s 53ms/step - loss: 1000.1252 - accuracy: 0.7618 - val_loss: 999.7744 - val_accuracy: 0.7610\nEpoch 193/200\nlearning rate scheduled to 0.0001437449743389152\n610/610 [==============================] - 34s 53ms/step - loss: 999.4219 - accuracy: 0.7618 - val_loss: 999.0742 - val_accuracy: 0.7581\nEpoch 194/200\nlearning rate scheduled to 0.00014230751854483968\n610/610 [==============================] - 34s 53ms/step - loss: 998.7247 - accuracy: 0.7616 - val_loss: 998.3718 - val_accuracy: 0.7690\nEpoch 195/200\nlearning rate scheduled to 0.00014088444033404812\n610/610 [==============================] - 34s 53ms/step - loss: 998.0354 - accuracy: 0.7626 - val_loss: 997.7174 - val_accuracy: 0.7416\nEpoch 196/200\nlearning rate scheduled to 0.0001394755956425797\n610/610 [==============================] - 34s 53ms/step - loss: 997.3540 - accuracy: 0.7616 - val_loss: 997.0179 - val_accuracy: 0.7601\nEpoch 197/200\nlearning rate scheduled to 0.00013808084040647372\n610/610 [==============================] - 34s 53ms/step - loss: 996.6771 - accuracy: 0.7625 - val_loss: 996.3433 - val_accuracy: 0.7625\nEpoch 198/200\nlearning rate scheduled to 0.00013670003056176939\n610/610 [==============================] - 34s 53ms/step - loss: 996.0084 - accuracy: 0.7635 - val_loss: 995.6743 - val_accuracy: 0.7628\nEpoch 199/200\nlearning rate scheduled to 0.000135333036450902\n610/610 [==============================] - 34s 53ms/step - loss: 995.3480 - accuracy: 0.7618 - val_loss: 995.0160 - val_accuracy: 0.7641\nEpoch 200/200\nlearning rate scheduled to 0.00013397969960351474\n610/610 [==============================] - 34s 53ms/step - loss: 994.6895 - accuracy: 0.7632 - val_loss: 994.3594 - val_accuracy: 0.7658\n"
    }
   ],
   "source": [
    "history_original_siamese_model_5 = original_siamese_model_5.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                             model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "153/153 [==============================] - 5s 18ms/step - loss: 994.3560 - accuracy: 0.7681\n"
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model_5.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth Run - 150k Pairs Gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_150k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_150k_224_224/positive\"\n",
    "width, height, channels = 105, 105, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 105, 105, 1)]     0         \n_________________________________________________________________\nConv1 (Conv2D)               (None, 96, 96, 64)        6464      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 48, 48, 64)        0         \n_________________________________________________________________\nConv2 (Conv2D)               (None, 42, 42, 128)       401536    \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 21, 21, 128)       0         \n_________________________________________________________________\nConv3 (Conv2D)               (None, 18, 18, 128)       262272    \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 9, 9, 128)         0         \n_________________________________________________________________\nConv4 (Conv2D)               (None, 6, 6, 256)         524544    \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 9216)              0         \n_________________________________________________________________\nDense1 (Dense)               (None, 4096)              37752832  \n=================================================================\nTotal params: 38,947,648\nTrainable params: 38,947,648\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "original_model = get_original_model(height,width,channels)\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = original_model(left_input)\n",
    "encoded_r = original_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "original_siamese_model_6 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/gqcxwm0u\" target=\"_blank\">azure-bush-10</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"momentum\": 0.5,\n",
    "                         \"otimizer\": \"SGD\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 200,\n",
    "                         \"architecture\": \"Original - 150k - Grayscale\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "original_siamese_model_6.compile(loss=config.loss_function,\n",
    "                                 optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    new_lr =  lr * 0.99\n",
    "    print(f\"learning rate scheduled to {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Original_150k_Gray\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/200\nlearning rate scheduled to 0.0009900000470224768\n1014/1014 [==============================] - 63s 58ms/step - loss: 1504.8086 - accuracy: 0.5364 - val_loss: 1498.7686 - val_accuracy: 0.5598\nEpoch 2/200\nlearning rate scheduled to 0.000980100086890161\n1014/1014 [==============================] - 62s 57ms/step - loss: 1492.8362 - accuracy: 0.5633 - val_loss: 1486.9006 - val_accuracy: 0.5545\nEpoch 3/200\nlearning rate scheduled to 0.0009702991275116801\n1014/1014 [==============================] - 58s 54ms/step - loss: 1481.0712 - accuracy: 0.5507 - val_loss: 1475.2389 - val_accuracy: 0.5629\nEpoch 4/200\nlearning rate scheduled to 0.0009605961316265165\n1014/1014 [==============================] - 57s 53ms/step - loss: 1469.5118 - accuracy: 0.5697 - val_loss: 1463.7858 - val_accuracy: 0.5808\nEpoch 5/200\nlearning rate scheduled to 0.0009509901772253215\n1014/1014 [==============================] - 57s 53ms/step - loss: 1458.1604 - accuracy: 0.5860 - val_loss: 1452.5353 - val_accuracy: 0.5913\nEpoch 6/200\nlearning rate scheduled to 0.0009414802846731617\n1014/1014 [==============================] - 57s 53ms/step - loss: 1447.0048 - accuracy: 0.6073 - val_loss: 1441.4744 - val_accuracy: 0.6235\nEpoch 7/200\nlearning rate scheduled to 0.0009320654743351042\n1014/1014 [==============================] - 57s 53ms/step - loss: 1436.0367 - accuracy: 0.6344 - val_loss: 1430.6039 - val_accuracy: 0.6411\nEpoch 8/200\nlearning rate scheduled to 0.0009227448242017999\n1014/1014 [==============================] - 57s 53ms/step - loss: 1425.2653 - accuracy: 0.6482 - val_loss: 1419.9216 - val_accuracy: 0.6589\nEpoch 9/200\nlearning rate scheduled to 0.0009135173546383158\n1014/1014 [==============================] - 57s 53ms/step - loss: 1414.6776 - accuracy: 0.6609 - val_loss: 1409.4338 - val_accuracy: 0.6607\nEpoch 10/200\nlearning rate scheduled to 0.0009043822012608871\n1014/1014 [==============================] - 57s 53ms/step - loss: 1404.2814 - accuracy: 0.6668 - val_loss: 1399.1301 - val_accuracy: 0.6701\nEpoch 11/200\nlearning rate scheduled to 0.0008953383844345808\n1014/1014 [==============================] - 57s 53ms/step - loss: 1394.0664 - accuracy: 0.6695 - val_loss: 1388.9961 - val_accuracy: 0.6764\nEpoch 12/200\nlearning rate scheduled to 0.000886384982150048\n1014/1014 [==============================] - 57s 53ms/step - loss: 1384.0281 - accuracy: 0.6726 - val_loss: 1379.0590 - val_accuracy: 0.6698\nEpoch 13/200\nlearning rate scheduled to 0.0008775211300235242\n1014/1014 [==============================] - 57s 53ms/step - loss: 1374.1631 - accuracy: 0.6739 - val_loss: 1369.2682 - val_accuracy: 0.6789\nEpoch 14/200\nlearning rate scheduled to 0.0008687459060456604\n1014/1014 [==============================] - 57s 53ms/step - loss: 1364.4642 - accuracy: 0.6785 - val_loss: 1359.6539 - val_accuracy: 0.6832\nEpoch 15/200\nlearning rate scheduled to 0.0008600584458326921\n1014/1014 [==============================] - 57s 53ms/step - loss: 1354.9303 - accuracy: 0.6797 - val_loss: 1350.2062 - val_accuracy: 0.6813\nEpoch 16/200\nlearning rate scheduled to 0.0008514578850008547\n1014/1014 [==============================] - 57s 53ms/step - loss: 1345.5599 - accuracy: 0.6813 - val_loss: 1340.9081 - val_accuracy: 0.6827\nEpoch 17/200\nlearning rate scheduled to 0.0008429433015407995\n1014/1014 [==============================] - 57s 53ms/step - loss: 1336.3446 - accuracy: 0.6849 - val_loss: 1331.7800 - val_accuracy: 0.6869\nEpoch 18/200\nlearning rate scheduled to 0.0008345138886943459\n1014/1014 [==============================] - 57s 53ms/step - loss: 1327.2864 - accuracy: 0.6860 - val_loss: 1322.7888 - val_accuracy: 0.6912\nEpoch 19/200\nlearning rate scheduled to 0.0008261687244521454\n1014/1014 [==============================] - 57s 53ms/step - loss: 1318.3784 - accuracy: 0.6872 - val_loss: 1313.9568 - val_accuracy: 0.6900\nEpoch 20/200\nlearning rate scheduled to 0.0008179070596816018\n1014/1014 [==============================] - 63s 59ms/step - loss: 1309.6176 - accuracy: 0.6874 - val_loss: 1305.2686 - val_accuracy: 0.6939\nEpoch 21/200\nlearning rate scheduled to 0.0008097279723733664\n1014/1014 [==============================] - 63s 59ms/step - loss: 1301.0043 - accuracy: 0.6882 - val_loss: 1296.7292 - val_accuracy: 0.6894\nEpoch 22/200\nlearning rate scheduled to 0.000801630713394843\n1014/1014 [==============================] - 63s 59ms/step - loss: 1292.5312 - accuracy: 0.6894 - val_loss: 1288.3273 - val_accuracy: 0.6933\nEpoch 23/200\nlearning rate scheduled to 0.0007936144183622674\n1014/1014 [==============================] - 63s 59ms/step - loss: 1284.1967 - accuracy: 0.6912 - val_loss: 1280.0577 - val_accuracy: 0.6961\nEpoch 24/200\nlearning rate scheduled to 0.0007856782805174589\n1014/1014 [==============================] - 63s 59ms/step - loss: 1275.9982 - accuracy: 0.6916 - val_loss: 1271.9255 - val_accuracy: 0.6948\nEpoch 25/200\nlearning rate scheduled to 0.0007778214931022376\n1014/1014 [==============================] - 63s 59ms/step - loss: 1267.9347 - accuracy: 0.6941 - val_loss: 1263.9385 - val_accuracy: 0.6894\nEpoch 26/200\nlearning rate scheduled to 0.0007700433069840073\n1014/1014 [==============================] - 63s 59ms/step - loss: 1260.0028 - accuracy: 0.6953 - val_loss: 1256.0619 - val_accuracy: 0.7001\nEpoch 27/200\nlearning rate scheduled to 0.0007623428577790037\n1014/1014 [==============================] - 63s 59ms/step - loss: 1252.1978 - accuracy: 0.6949 - val_loss: 1248.3395 - val_accuracy: 0.6885\nEpoch 28/200\nlearning rate scheduled to 0.0007547194539802149\n1014/1014 [==============================] - 63s 59ms/step - loss: 1244.5189 - accuracy: 0.6952 - val_loss: 1240.7097 - val_accuracy: 0.6971\nEpoch 29/200\nlearning rate scheduled to 0.0007471722312038764\n1014/1014 [==============================] - 63s 59ms/step - loss: 1236.9617 - accuracy: 0.6974 - val_loss: 1233.2245 - val_accuracy: 0.6890\nEpoch 30/200\nlearning rate scheduled to 0.0007397004979429766\n1014/1014 [==============================] - 63s 59ms/step - loss: 1229.5275 - accuracy: 0.6971 - val_loss: 1225.8372 - val_accuracy: 0.7000\nEpoch 31/200\nlearning rate scheduled to 0.0007323035050649196\n1014/1014 [==============================] - 63s 59ms/step - loss: 1222.2123 - accuracy: 0.6973 - val_loss: 1218.5826 - val_accuracy: 0.6958\nEpoch 32/200\nlearning rate scheduled to 0.000724980445811525\n1014/1014 [==============================] - 63s 59ms/step - loss: 1215.0100 - accuracy: 0.6988 - val_loss: 1211.4360 - val_accuracy: 0.6998\nEpoch 33/200\nlearning rate scheduled to 0.0007177306286757812\n1014/1014 [==============================] - 63s 59ms/step - loss: 1207.9235 - accuracy: 0.7000 - val_loss: 1204.4054 - val_accuracy: 0.7041\nEpoch 34/200\nlearning rate scheduled to 0.0007105533045250923\n1014/1014 [==============================] - 63s 59ms/step - loss: 1200.9504 - accuracy: 0.6991 - val_loss: 1197.4828 - val_accuracy: 0.7069\nEpoch 35/200\nlearning rate scheduled to 0.0007034477818524464\n1014/1014 [==============================] - 63s 59ms/step - loss: 1194.0836 - accuracy: 0.7009 - val_loss: 1190.6744 - val_accuracy: 0.7039\nEpoch 36/200\nlearning rate scheduled to 0.000696413311525248\n1014/1014 [==============================] - 63s 59ms/step - loss: 1187.3252 - accuracy: 0.7008 - val_loss: 1183.9641 - val_accuracy: 0.7041\nEpoch 37/200\nlearning rate scheduled to 0.0006894492020364851\n1014/1014 [==============================] - 63s 59ms/step - loss: 1180.6719 - accuracy: 0.7024 - val_loss: 1177.3696 - val_accuracy: 0.7044\nEpoch 38/200\nlearning rate scheduled to 0.0006825547042535618\n1014/1014 [==============================] - 63s 59ms/step - loss: 1174.1234 - accuracy: 0.7025 - val_loss: 1170.8674 - val_accuracy: 0.7052\nEpoch 39/200\nlearning rate scheduled to 0.0006757291842950508\n1014/1014 [==============================] - 63s 59ms/step - loss: 1167.6736 - accuracy: 0.7038 - val_loss: 1164.4690 - val_accuracy: 0.7077\nEpoch 40/200\nlearning rate scheduled to 0.0006689718930283561\n1014/1014 [==============================] - 63s 59ms/step - loss: 1161.3248 - accuracy: 0.7053 - val_loss: 1158.1741 - val_accuracy: 0.7042\nEpoch 41/200\nlearning rate scheduled to 0.0006622821965720505\n1014/1014 [==============================] - 63s 59ms/step - loss: 1155.0719 - accuracy: 0.7057 - val_loss: 1151.9700 - val_accuracy: 0.7053\nEpoch 42/200\nlearning rate scheduled to 0.0006556594034191221\n1014/1014 [==============================] - 63s 59ms/step - loss: 1148.9167 - accuracy: 0.7056 - val_loss: 1145.8732 - val_accuracy: 0.6950\nEpoch 43/200\nlearning rate scheduled to 0.0006491028220625594\n1014/1014 [==============================] - 63s 59ms/step - loss: 1142.8560 - accuracy: 0.7060 - val_loss: 1139.8405 - val_accuracy: 0.7136\nEpoch 44/200\nlearning rate scheduled to 0.0006426118186209351\n1014/1014 [==============================] - 63s 59ms/step - loss: 1136.8849 - accuracy: 0.7080 - val_loss: 1133.9166 - val_accuracy: 0.7111\nEpoch 45/200\nlearning rate scheduled to 0.0006361857015872375\n1014/1014 [==============================] - 63s 58ms/step - loss: 1131.0057 - accuracy: 0.7086 - val_loss: 1128.0826 - val_accuracy: 0.7131\nEpoch 46/200\nlearning rate scheduled to 0.0006298238370800391\n1014/1014 [==============================] - 63s 59ms/step - loss: 1125.2144 - accuracy: 0.7090 - val_loss: 1122.3380 - val_accuracy: 0.7085\nEpoch 47/200\nlearning rate scheduled to 0.0006235255912179127\n1014/1014 [==============================] - 63s 59ms/step - loss: 1119.5112 - accuracy: 0.7101 - val_loss: 1116.6765 - val_accuracy: 0.7136\nEpoch 48/200\nlearning rate scheduled to 0.000617290330119431\n1014/1014 [==============================] - 63s 59ms/step - loss: 1113.8926 - accuracy: 0.7111 - val_loss: 1111.1046 - val_accuracy: 0.7116\nEpoch 49/200\nlearning rate scheduled to 0.0006111174199031666\n1014/1014 [==============================] - 63s 59ms/step - loss: 1108.3593 - accuracy: 0.7109 - val_loss: 1105.6099 - val_accuracy: 0.7115\nEpoch 50/200\nlearning rate scheduled to 0.0006050062266876921\n1014/1014 [==============================] - 63s 59ms/step - loss: 1102.9067 - accuracy: 0.7115 - val_loss: 1100.1970 - val_accuracy: 0.7177\nEpoch 51/200\nlearning rate scheduled to 0.0005989561742171646\n1014/1014 [==============================] - 63s 59ms/step - loss: 1097.5358 - accuracy: 0.7151 - val_loss: 1094.8674 - val_accuracy: 0.7166\nEpoch 52/200\nlearning rate scheduled to 0.0005929666286101564\n1014/1014 [==============================] - 63s 59ms/step - loss: 1092.2443 - accuracy: 0.7150 - val_loss: 1089.6217 - val_accuracy: 0.7113\nEpoch 53/200\nlearning rate scheduled to 0.0005870369559852406\n1014/1014 [==============================] - 63s 59ms/step - loss: 1087.0309 - accuracy: 0.7155 - val_loss: 1084.4421 - val_accuracy: 0.7173\nEpoch 54/200\nlearning rate scheduled to 0.000581166580086574\n1014/1014 [==============================] - 63s 59ms/step - loss: 1081.8925 - accuracy: 0.7174 - val_loss: 1079.3427 - val_accuracy: 0.7143\nEpoch 55/200\nlearning rate scheduled to 0.0005753549246583134\n1014/1014 [==============================] - 63s 59ms/step - loss: 1076.8317 - accuracy: 0.7177 - val_loss: 1074.3181 - val_accuracy: 0.7186\nEpoch 56/200\nlearning rate scheduled to 0.0005696013558190316\n1014/1014 [==============================] - 63s 59ms/step - loss: 1071.8442 - accuracy: 0.7195 - val_loss: 1069.3738 - val_accuracy: 0.7135\nEpoch 57/200\nlearning rate scheduled to 0.0005639053549384699\n1014/1014 [==============================] - 63s 59ms/step - loss: 1066.9290 - accuracy: 0.7193 - val_loss: 1064.4924 - val_accuracy: 0.7180\nEpoch 58/200\nlearning rate scheduled to 0.0005582662881352008\n1014/1014 [==============================] - 63s 59ms/step - loss: 1062.0856 - accuracy: 0.7198 - val_loss: 1059.6919 - val_accuracy: 0.7158\nEpoch 59/200\nlearning rate scheduled to 0.0005526836367789656\n1014/1014 [==============================] - 63s 59ms/step - loss: 1057.3115 - accuracy: 0.7212 - val_loss: 1054.9360 - val_accuracy: 0.7245\nEpoch 60/200\nlearning rate scheduled to 0.0005471568246139213\n1014/1014 [==============================] - 63s 59ms/step - loss: 1052.6060 - accuracy: 0.7220 - val_loss: 1050.2686 - val_accuracy: 0.7240\nEpoch 61/200\nlearning rate scheduled to 0.000541685275384225\n1014/1014 [==============================] - 63s 59ms/step - loss: 1047.9683 - accuracy: 0.7244 - val_loss: 1045.6622 - val_accuracy: 0.7264\nEpoch 62/200\nlearning rate scheduled to 0.0005362684128340334\n1014/1014 [==============================] - 63s 59ms/step - loss: 1043.3976 - accuracy: 0.7240 - val_loss: 1041.1202 - val_accuracy: 0.7287\nEpoch 63/200\nlearning rate scheduled to 0.0005309057183330878\n1014/1014 [==============================] - 63s 59ms/step - loss: 1038.8915 - accuracy: 0.7253 - val_loss: 1036.6516 - val_accuracy: 0.7276\nEpoch 64/200\nlearning rate scheduled to 0.0005255966732511297\n1014/1014 [==============================] - 63s 59ms/step - loss: 1034.4502 - accuracy: 0.7277 - val_loss: 1032.2417 - val_accuracy: 0.7283\nEpoch 65/200\nlearning rate scheduled to 0.0005203407013323158\n1014/1014 [==============================] - 63s 59ms/step - loss: 1030.0734 - accuracy: 0.7270 - val_loss: 1027.8938 - val_accuracy: 0.7325\nEpoch 66/200\nlearning rate scheduled to 0.0005151372839463875\n1014/1014 [==============================] - 63s 59ms/step - loss: 1025.7574 - accuracy: 0.7292 - val_loss: 1023.6140 - val_accuracy: 0.7304\nEpoch 67/200\nlearning rate scheduled to 0.0005099859024630859\n1014/1014 [==============================] - 63s 59ms/step - loss: 1021.5018 - accuracy: 0.7291 - val_loss: 1019.3856 - val_accuracy: 0.7314\nEpoch 68/200\nlearning rate scheduled to 0.0005048860382521525\n1014/1014 [==============================] - 63s 59ms/step - loss: 1017.3074 - accuracy: 0.7312 - val_loss: 1015.2281 - val_accuracy: 0.7281\nEpoch 69/200\nlearning rate scheduled to 0.0004998371726833284\n1014/1014 [==============================] - 63s 59ms/step - loss: 1013.1705 - accuracy: 0.7313 - val_loss: 1011.1146 - val_accuracy: 0.7342\nEpoch 70/200\nlearning rate scheduled to 0.0004948387871263549\n1014/1014 [==============================] - 63s 59ms/step - loss: 1009.0926 - accuracy: 0.7313 - val_loss: 1007.0660 - val_accuracy: 0.7350\nEpoch 71/200\nlearning rate scheduled to 0.0004898904205765575\n1014/1014 [==============================] - 63s 59ms/step - loss: 1005.0701 - accuracy: 0.7335 - val_loss: 1003.0724 - val_accuracy: 0.7337\nEpoch 72/200\nlearning rate scheduled to 0.00048499149677809326\n1014/1014 [==============================] - 63s 59ms/step - loss: 1001.1065 - accuracy: 0.7320 - val_loss: 999.1322 - val_accuracy: 0.7375\nEpoch 73/200\nlearning rate scheduled to 0.00048014158353907986\n1014/1014 [==============================] - 63s 59ms/step - loss: 997.1956 - accuracy: 0.7341 - val_loss: 995.2486 - val_accuracy: 0.7369\nEpoch 74/200\nlearning rate scheduled to 0.00047534016222925855\n1014/1014 [==============================] - 63s 59ms/step - loss: 993.3391 - accuracy: 0.7357 - val_loss: 991.4223 - val_accuracy: 0.7401\nEpoch 75/200\nlearning rate scheduled to 0.0004705867718439549\n1014/1014 [==============================] - 63s 59ms/step - loss: 989.5377 - accuracy: 0.7360 - val_loss: 987.6451 - val_accuracy: 0.7412\nEpoch 76/200\nlearning rate scheduled to 0.0004658808937529102\n1014/1014 [==============================] - 63s 59ms/step - loss: 985.7866 - accuracy: 0.7382 - val_loss: 983.9227 - val_accuracy: 0.7393\nEpoch 77/200\nlearning rate scheduled to 0.0004612220957642421\n1014/1014 [==============================] - 62s 57ms/step - loss: 982.0885 - accuracy: 0.7375 - val_loss: 980.2568 - val_accuracy: 0.7340\nEpoch 78/200\nlearning rate scheduled to 0.00045660988806048406\n1014/1014 [==============================] - 63s 59ms/step - loss: 978.4393 - accuracy: 0.7386 - val_loss: 976.6255 - val_accuracy: 0.7405\nEpoch 79/200\nlearning rate scheduled to 0.0004520437808241695\n1014/1014 [==============================] - 63s 58ms/step - loss: 974.8393 - accuracy: 0.7408 - val_loss: 973.0493 - val_accuracy: 0.7432\nEpoch 80/200\nlearning rate scheduled to 0.0004475233418634161\n1014/1014 [==============================] - 63s 59ms/step - loss: 971.2911 - accuracy: 0.7405 - val_loss: 969.5261 - val_accuracy: 0.7400\nEpoch 81/200\nlearning rate scheduled to 0.0004430481101735495\n1014/1014 [==============================] - 63s 59ms/step - loss: 967.7906 - accuracy: 0.7413 - val_loss: 966.0494 - val_accuracy: 0.7447\nEpoch 82/200\nlearning rate scheduled to 0.0004386176247498952\n1014/1014 [==============================] - 63s 59ms/step - loss: 964.3373 - accuracy: 0.7418 - val_loss: 962.6176 - val_accuracy: 0.7437\nEpoch 83/200\nlearning rate scheduled to 0.00043423145340057087\n1014/1014 [==============================] - 59s 54ms/step - loss: 960.9304 - accuracy: 0.7423 - val_loss: 959.2377 - val_accuracy: 0.7427\nEpoch 84/200\nlearning rate scheduled to 0.0004298891351209022\n1014/1014 [==============================] - 61s 57ms/step - loss: 957.5671 - accuracy: 0.7433 - val_loss: 955.8925 - val_accuracy: 0.7486\nEpoch 85/200\nlearning rate scheduled to 0.00042559023771900686\n1014/1014 [==============================] - 63s 59ms/step - loss: 954.2522 - accuracy: 0.7444 - val_loss: 952.6114 - val_accuracy: 0.7373\nEpoch 86/200\nlearning rate scheduled to 0.0004213343290030025\n1014/1014 [==============================] - 63s 59ms/step - loss: 950.9813 - accuracy: 0.7439 - val_loss: 949.3579 - val_accuracy: 0.7421\nEpoch 87/200\nlearning rate scheduled to 0.00041712097678100687\n1014/1014 [==============================] - 59s 55ms/step - loss: 947.7537 - accuracy: 0.7448 - val_loss: 946.1454 - val_accuracy: 0.7497\nEpoch 88/200\nlearning rate scheduled to 0.00041294977767392994\n1014/1014 [==============================] - 58s 54ms/step - loss: 944.5704 - accuracy: 0.7451 - val_loss: 942.9940 - val_accuracy: 0.7396\nEpoch 89/200\nlearning rate scheduled to 0.0004088202706770971\n1014/1014 [==============================] - 58s 54ms/step - loss: 941.4280 - accuracy: 0.7449 - val_loss: 939.8690 - val_accuracy: 0.7431\nEpoch 90/200\nlearning rate scheduled to 0.00040473208122421056\n1014/1014 [==============================] - 58s 54ms/step - loss: 938.3274 - accuracy: 0.7455 - val_loss: 936.7852 - val_accuracy: 0.7465\nEpoch 91/200\nlearning rate scheduled to 0.0004006847483105957\n1014/1014 [==============================] - 58s 54ms/step - loss: 935.2669 - accuracy: 0.7473 - val_loss: 933.7493 - val_accuracy: 0.7444\nEpoch 92/200\nlearning rate scheduled to 0.0003966778973699547\n1014/1014 [==============================] - 57s 53ms/step - loss: 932.2476 - accuracy: 0.7478 - val_loss: 930.7431 - val_accuracy: 0.7527\nEpoch 93/200\nlearning rate scheduled to 0.0003927111250231974\n1014/1014 [==============================] - 63s 59ms/step - loss: 929.2688 - accuracy: 0.7479 - val_loss: 927.7925 - val_accuracy: 0.7448\nEpoch 94/200\nlearning rate scheduled to 0.0003887840278912336\n1014/1014 [==============================] - 63s 58ms/step - loss: 926.3297 - accuracy: 0.7488 - val_loss: 924.8740 - val_accuracy: 0.7456\nEpoch 95/200\nlearning rate scheduled to 0.000384896173782181\n1014/1014 [==============================] - 60s 56ms/step - loss: 923.4279 - accuracy: 0.7491 - val_loss: 921.9869 - val_accuracy: 0.7497\nEpoch 96/200\nlearning rate scheduled to 0.00038104721694253384\n1014/1014 [==============================] - 57s 52ms/step - loss: 920.5658 - accuracy: 0.7489 - val_loss: 919.1443 - val_accuracy: 0.7523\nEpoch 97/200\nlearning rate scheduled to 0.000377236753993202\n1014/1014 [==============================] - 56s 52ms/step - loss: 917.7388 - accuracy: 0.7501 - val_loss: 916.3282 - val_accuracy: 0.7572\nEpoch 98/200\nlearning rate scheduled to 0.0003734643815550953\n1014/1014 [==============================] - 56s 52ms/step - loss: 914.9503 - accuracy: 0.7493 - val_loss: 913.5627 - val_accuracy: 0.7527\nEpoch 99/200\nlearning rate scheduled to 0.0003697297250619158\n1014/1014 [==============================] - 56s 52ms/step - loss: 912.1964 - accuracy: 0.7502 - val_loss: 910.8262 - val_accuracy: 0.7539\nEpoch 100/200\nlearning rate scheduled to 0.0003660324387601577\n1014/1014 [==============================] - 56s 52ms/step - loss: 909.4803 - accuracy: 0.7509 - val_loss: 908.1285 - val_accuracy: 0.7493\nEpoch 101/200\nlearning rate scheduled to 0.00036237211927073074\n1014/1014 [==============================] - 56s 52ms/step - loss: 906.7980 - accuracy: 0.7505 - val_loss: 905.4625 - val_accuracy: 0.7529\nEpoch 102/200\nlearning rate scheduled to 0.0003587483920273371\n1014/1014 [==============================] - 56s 52ms/step - loss: 904.1494 - accuracy: 0.7510 - val_loss: 902.8309 - val_accuracy: 0.7526\nEpoch 103/200\nlearning rate scheduled to 0.0003551609112764709\n1014/1014 [==============================] - 56s 52ms/step - loss: 901.5364 - accuracy: 0.7525 - val_loss: 900.2344 - val_accuracy: 0.7560\nEpoch 104/200\nlearning rate scheduled to 0.0003516093024518341\n1014/1014 [==============================] - 56s 52ms/step - loss: 898.9583 - accuracy: 0.7522 - val_loss: 897.6779 - val_accuracy: 0.7494\nEpoch 105/200\nlearning rate scheduled to 0.0003480932197999209\n1014/1014 [==============================] - 56s 52ms/step - loss: 896.4111 - accuracy: 0.7530 - val_loss: 895.1441 - val_accuracy: 0.7535\nEpoch 106/200\nlearning rate scheduled to 0.0003446122887544334\n1014/1014 [==============================] - 56s 52ms/step - loss: 893.8955 - accuracy: 0.7548 - val_loss: 892.6454 - val_accuracy: 0.7554\nEpoch 107/200\nlearning rate scheduled to 0.00034116616356186566\n1014/1014 [==============================] - 56s 52ms/step - loss: 891.4137 - accuracy: 0.7541 - val_loss: 890.1759 - val_accuracy: 0.7574\nEpoch 108/200\nlearning rate scheduled to 0.00033775449846871195\n1014/1014 [==============================] - 56s 52ms/step - loss: 888.9640 - accuracy: 0.7524 - val_loss: 887.7477 - val_accuracy: 0.7520\nEpoch 109/200\nlearning rate scheduled to 0.0003343769477214664\n1014/1014 [==============================] - 56s 52ms/step - loss: 886.5447 - accuracy: 0.7541 - val_loss: 885.3395 - val_accuracy: 0.7551\nEpoch 110/200\nlearning rate scheduled to 0.0003310331655666232\n1014/1014 [==============================] - 56s 52ms/step - loss: 884.1554 - accuracy: 0.7556 - val_loss: 882.9656 - val_accuracy: 0.7589\nEpoch 111/200\nlearning rate scheduled to 0.00032772283506346864\n1014/1014 [==============================] - 56s 52ms/step - loss: 881.7963 - accuracy: 0.7567 - val_loss: 880.6180 - val_accuracy: 0.7605\nEpoch 112/200\nlearning rate scheduled to 0.00032444561045849693\n1014/1014 [==============================] - 63s 59ms/step - loss: 879.4678 - accuracy: 0.7563 - val_loss: 878.3072 - val_accuracy: 0.7625\nEpoch 113/200\nlearning rate scheduled to 0.00032120114599820226\n1014/1014 [==============================] - 63s 59ms/step - loss: 877.1689 - accuracy: 0.7583 - val_loss: 876.0202 - val_accuracy: 0.7634\nEpoch 114/200\nlearning rate scheduled to 0.00031798912474187093\n1014/1014 [==============================] - 63s 59ms/step - loss: 874.9014 - accuracy: 0.7567 - val_loss: 873.7632 - val_accuracy: 0.7611\nEpoch 115/200\nlearning rate scheduled to 0.0003148092297487892\n1014/1014 [==============================] - 63s 59ms/step - loss: 872.6574 - accuracy: 0.7574 - val_loss: 871.5383 - val_accuracy: 0.7625\nEpoch 116/200\nlearning rate scheduled to 0.0003116611440782435\n1014/1014 [==============================] - 63s 59ms/step - loss: 870.4429 - accuracy: 0.7564 - val_loss: 869.3391 - val_accuracy: 0.7567\nEpoch 117/200\nlearning rate scheduled to 0.000308544521976728\n1014/1014 [==============================] - 63s 59ms/step - loss: 868.2563 - accuracy: 0.7575 - val_loss: 867.1639 - val_accuracy: 0.7650\nEpoch 118/200\nlearning rate scheduled to 0.0003054590753163211\n1014/1014 [==============================] - 63s 59ms/step - loss: 866.0967 - accuracy: 0.7578 - val_loss: 865.0209 - val_accuracy: 0.7597\nEpoch 119/200\nlearning rate scheduled to 0.0003024044871563092\n1014/1014 [==============================] - 59s 55ms/step - loss: 863.9652 - accuracy: 0.7590 - val_loss: 862.9053 - val_accuracy: 0.7609\nEpoch 120/200\nlearning rate scheduled to 0.00029938044055597855\n1014/1014 [==============================] - 57s 53ms/step - loss: 861.8604 - accuracy: 0.7591 - val_loss: 860.8132 - val_accuracy: 0.7574\nEpoch 121/200\nlearning rate scheduled to 0.0002963866473874077\n1014/1014 [==============================] - 56s 52ms/step - loss: 859.7822 - accuracy: 0.7588 - val_loss: 858.7468 - val_accuracy: 0.7604\nEpoch 122/200\nlearning rate scheduled to 0.000293422790709883\n1014/1014 [==============================] - 56s 52ms/step - loss: 857.7297 - accuracy: 0.7582 - val_loss: 856.7086 - val_accuracy: 0.7610\nEpoch 123/200\nlearning rate scheduled to 0.00029048855358269063\n1014/1014 [==============================] - 56s 52ms/step - loss: 855.6989 - accuracy: 0.7593 - val_loss: 854.6851 - val_accuracy: 0.7665\nEpoch 124/200\nlearning rate scheduled to 0.0002875836766907014\n1014/1014 [==============================] - 56s 52ms/step - loss: 853.6959 - accuracy: 0.7618 - val_loss: 852.6945 - val_accuracy: 0.7650\nEpoch 125/200\nlearning rate scheduled to 0.0002847078430932015\n1014/1014 [==============================] - 56s 52ms/step - loss: 851.7166 - accuracy: 0.7607 - val_loss: 850.7333 - val_accuracy: 0.7605\nEpoch 126/200\nlearning rate scheduled to 0.0002818607646622695\n1014/1014 [==============================] - 56s 52ms/step - loss: 849.7615 - accuracy: 0.7612 - val_loss: 848.7880 - val_accuracy: 0.7647\nEpoch 127/200\nlearning rate scheduled to 0.00027904215326998385\n1014/1014 [==============================] - 63s 59ms/step - loss: 847.8306 - accuracy: 0.7616 - val_loss: 846.8654 - val_accuracy: 0.7657\nEpoch 128/200\nlearning rate scheduled to 0.00027625172078842296\n1014/1014 [==============================] - 63s 59ms/step - loss: 845.9248 - accuracy: 0.7613 - val_loss: 844.9741 - val_accuracy: 0.7655\nEpoch 129/200\nlearning rate scheduled to 0.0002734892079024576\n1014/1014 [==============================] - 63s 59ms/step - loss: 844.0405 - accuracy: 0.7627 - val_loss: 843.1078 - val_accuracy: 0.7590\nEpoch 130/200\nlearning rate scheduled to 0.0002707543264841661\n1014/1014 [==============================] - 63s 59ms/step - loss: 842.1805 - accuracy: 0.7617 - val_loss: 841.2534 - val_accuracy: 0.7657\nEpoch 131/200\nlearning rate scheduled to 0.000268046788405627\n1014/1014 [==============================] - 63s 59ms/step - loss: 840.3439 - accuracy: 0.7618 - val_loss: 839.4292 - val_accuracy: 0.7641\nEpoch 132/200\nlearning rate scheduled to 0.000265366334351711\n1014/1014 [==============================] - 63s 59ms/step - loss: 838.5287 - accuracy: 0.7632 - val_loss: 837.6258 - val_accuracy: 0.7639\nEpoch 133/200\nlearning rate scheduled to 0.00026271267619449646\n1014/1014 [==============================] - 63s 59ms/step - loss: 836.7354 - accuracy: 0.7647 - val_loss: 835.8406 - val_accuracy: 0.7663\nEpoch 134/200\nlearning rate scheduled to 0.00026008555461885406\n1014/1014 [==============================] - 63s 59ms/step - loss: 834.9651 - accuracy: 0.7639 - val_loss: 834.0764 - val_accuracy: 0.7663\nEpoch 135/200\nlearning rate scheduled to 0.00025748471030965445\n1014/1014 [==============================] - 63s 59ms/step - loss: 833.2139 - accuracy: 0.7640 - val_loss: 832.3427 - val_accuracy: 0.7653\nEpoch 136/200\nlearning rate scheduled to 0.0002549098551389761\n1014/1014 [==============================] - 63s 59ms/step - loss: 831.4832 - accuracy: 0.7650 - val_loss: 830.6226 - val_accuracy: 0.7644\nEpoch 137/200\nlearning rate scheduled to 0.00025236075860448183\n1014/1014 [==============================] - 63s 59ms/step - loss: 829.7750 - accuracy: 0.7642 - val_loss: 828.9268 - val_accuracy: 0.7620\nEpoch 138/200\nlearning rate scheduled to 0.0002498371613910422\n1014/1014 [==============================] - 63s 59ms/step - loss: 828.0850 - accuracy: 0.7660 - val_loss: 827.2414 - val_accuracy: 0.7702\nEpoch 139/200\nlearning rate scheduled to 0.0002473388041835278\n1014/1014 [==============================] - 63s 59ms/step - loss: 826.4167 - accuracy: 0.7653 - val_loss: 825.5856 - val_accuracy: 0.7681\nEpoch 140/200\nlearning rate scheduled to 0.0002448654276668094\n1014/1014 [==============================] - 63s 59ms/step - loss: 824.7704 - accuracy: 0.7652 - val_loss: 823.9517 - val_accuracy: 0.7652\nEpoch 141/200\nlearning rate scheduled to 0.00024241677252575755\n1014/1014 [==============================] - 63s 59ms/step - loss: 823.1432 - accuracy: 0.7656 - val_loss: 822.3292 - val_accuracy: 0.7676\nEpoch 142/200\nlearning rate scheduled to 0.00023999260825803502\n1014/1014 [==============================] - 63s 59ms/step - loss: 821.5354 - accuracy: 0.7648 - val_loss: 820.7377 - val_accuracy: 0.7651\nEpoch 143/200\nlearning rate scheduled to 0.00023759267554851249\n1014/1014 [==============================] - 63s 59ms/step - loss: 819.9462 - accuracy: 0.7660 - val_loss: 819.1524 - val_accuracy: 0.7696\nEpoch 144/200\nlearning rate scheduled to 0.0002352167438948527\n1014/1014 [==============================] - 63s 59ms/step - loss: 818.3766 - accuracy: 0.7660 - val_loss: 817.5911 - val_accuracy: 0.7718\nEpoch 145/200\nlearning rate scheduled to 0.00023286458279471843\n1014/1014 [==============================] - 63s 59ms/step - loss: 816.8246 - accuracy: 0.7681 - val_loss: 816.0518 - val_accuracy: 0.7671\nEpoch 146/200\nlearning rate scheduled to 0.00023053593293298035\n1014/1014 [==============================] - 63s 59ms/step - loss: 815.2927 - accuracy: 0.7672 - val_loss: 814.5287 - val_accuracy: 0.7678\nEpoch 147/200\nlearning rate scheduled to 0.0002282305782136973\n1014/1014 [==============================] - 63s 59ms/step - loss: 813.7769 - accuracy: 0.7672 - val_loss: 813.0220 - val_accuracy: 0.7680\nEpoch 148/200\nlearning rate scheduled to 0.00022594827372813598\n1014/1014 [==============================] - 63s 59ms/step - loss: 812.2785 - accuracy: 0.7689 - val_loss: 811.5323 - val_accuracy: 0.7698\nEpoch 149/200\nlearning rate scheduled to 0.00022368878897395915\n1014/1014 [==============================] - 63s 59ms/step - loss: 810.7992 - accuracy: 0.7677 - val_loss: 810.0601 - val_accuracy: 0.7667\nEpoch 150/200\nlearning rate scheduled to 0.00022145190785522573\n1014/1014 [==============================] - 63s 59ms/step - loss: 809.3354 - accuracy: 0.7679 - val_loss: 808.6072 - val_accuracy: 0.7671\nEpoch 151/200\nlearning rate scheduled to 0.00021923738546320237\n1014/1014 [==============================] - 63s 59ms/step - loss: 807.8872 - accuracy: 0.7699 - val_loss: 807.1721 - val_accuracy: 0.7683\nEpoch 152/200\nlearning rate scheduled to 0.00021704500570194797\n1014/1014 [==============================] - 63s 59ms/step - loss: 806.4606 - accuracy: 0.7688 - val_loss: 805.7498 - val_accuracy: 0.7676\nEpoch 153/200\nlearning rate scheduled to 0.00021487455247552135\n1014/1014 [==============================] - 63s 59ms/step - loss: 805.0499 - accuracy: 0.7700 - val_loss: 804.3458 - val_accuracy: 0.7719\nEpoch 154/200\nlearning rate scheduled to 0.00021272580968798138\n1014/1014 [==============================] - 63s 59ms/step - loss: 803.6555 - accuracy: 0.7703 - val_loss: 802.9594 - val_accuracy: 0.7721\nEpoch 155/200\nlearning rate scheduled to 0.00021059854683699086\n1014/1014 [==============================] - 63s 59ms/step - loss: 802.2798 - accuracy: 0.7700 - val_loss: 801.5906 - val_accuracy: 0.7722\nEpoch 156/200\nlearning rate scheduled to 0.0002084925622330047\n1014/1014 [==============================] - 63s 59ms/step - loss: 800.9169 - accuracy: 0.7707 - val_loss: 800.2348 - val_accuracy: 0.7748\nEpoch 157/200\nlearning rate scheduled to 0.0002064076397800818\n1014/1014 [==============================] - 63s 59ms/step - loss: 799.5713 - accuracy: 0.7717 - val_loss: 798.9026 - val_accuracy: 0.7671\nEpoch 158/200\nlearning rate scheduled to 0.000204343563382281\n1014/1014 [==============================] - 63s 59ms/step - loss: 798.2429 - accuracy: 0.7708 - val_loss: 797.5773 - val_accuracy: 0.7728\nEpoch 159/200\nlearning rate scheduled to 0.0002023001313500572\n1014/1014 [==============================] - 63s 59ms/step - loss: 796.9260 - accuracy: 0.7711 - val_loss: 796.2744 - val_accuracy: 0.7688\nEpoch 160/200\nlearning rate scheduled to 0.0002002771275874693\n1014/1014 [==============================] - 63s 59ms/step - loss: 795.6274 - accuracy: 0.7721 - val_loss: 794.9778 - val_accuracy: 0.7743\nEpoch 161/200\nlearning rate scheduled to 0.0001982743504049722\n1014/1014 [==============================] - 63s 59ms/step - loss: 794.3429 - accuracy: 0.7718 - val_loss: 793.6996 - val_accuracy: 0.7733\nEpoch 162/200\nlearning rate scheduled to 0.00019629161251941696\n1014/1014 [==============================] - 63s 59ms/step - loss: 793.0701 - accuracy: 0.7733 - val_loss: 792.4397 - val_accuracy: 0.7747\nEpoch 163/200\nlearning rate scheduled to 0.0001943286978348624\n1014/1014 [==============================] - 63s 59ms/step - loss: 791.8177 - accuracy: 0.7711 - val_loss: 791.1873 - val_accuracy: 0.7763\nEpoch 164/200\nlearning rate scheduled to 0.00019238540466176346\n1014/1014 [==============================] - 63s 59ms/step - loss: 790.5750 - accuracy: 0.7715 - val_loss: 789.9536 - val_accuracy: 0.7745\nEpoch 165/200\nlearning rate scheduled to 0.00019046154571697116\n1014/1014 [==============================] - 63s 59ms/step - loss: 789.3463 - accuracy: 0.7727 - val_loss: 788.7399 - val_accuracy: 0.7718\nEpoch 166/200\nlearning rate scheduled to 0.0001885569337173365\n1014/1014 [==============================] - 63s 59ms/step - loss: 788.1326 - accuracy: 0.7738 - val_loss: 787.5304 - val_accuracy: 0.7694\nEpoch 167/200\nlearning rate scheduled to 0.00018667136697331444\n1014/1014 [==============================] - 63s 59ms/step - loss: 786.9349 - accuracy: 0.7734 - val_loss: 786.3403 - val_accuracy: 0.7746\nEpoch 168/200\nlearning rate scheduled to 0.00018480465820175596\n1014/1014 [==============================] - 63s 59ms/step - loss: 785.7520 - accuracy: 0.7736 - val_loss: 785.1694 - val_accuracy: 0.7677\nEpoch 169/200\nlearning rate scheduled to 0.000182956605713116\n1014/1014 [==============================] - 63s 59ms/step - loss: 784.5809 - accuracy: 0.7738 - val_loss: 784.0046 - val_accuracy: 0.7707\nEpoch 170/200\nlearning rate scheduled to 0.00018112703663064166\n1014/1014 [==============================] - 63s 59ms/step - loss: 783.4250 - accuracy: 0.7742 - val_loss: 782.8470 - val_accuracy: 0.7791\nEpoch 171/200\nlearning rate scheduled to 0.00017931576367118395\n1014/1014 [==============================] - 63s 59ms/step - loss: 782.2811 - accuracy: 0.7761 - val_loss: 781.7112 - val_accuracy: 0.7766\nEpoch 172/200\nlearning rate scheduled to 0.0001775225995515939\n1014/1014 [==============================] - 63s 59ms/step - loss: 781.1506 - accuracy: 0.7757 - val_loss: 780.5977 - val_accuracy: 0.7701\nEpoch 173/200\nlearning rate scheduled to 0.00017574737139511854\n1014/1014 [==============================] - 63s 59ms/step - loss: 780.0337 - accuracy: 0.7757 - val_loss: 779.4746 - val_accuracy: 0.7773\nEpoch 174/200\nlearning rate scheduled to 0.00017398989191860892\n1014/1014 [==============================] - 63s 59ms/step - loss: 778.9284 - accuracy: 0.7760 - val_loss: 778.3737 - val_accuracy: 0.7796\nEpoch 175/200\nlearning rate scheduled to 0.00017224998824531213\n1014/1014 [==============================] - 63s 59ms/step - loss: 777.8363 - accuracy: 0.7759 - val_loss: 777.2882 - val_accuracy: 0.7801\nEpoch 176/200\nlearning rate scheduled to 0.00017052748749847522\n1014/1014 [==============================] - 63s 59ms/step - loss: 776.7567 - accuracy: 0.7757 - val_loss: 776.2210 - val_accuracy: 0.7709\nEpoch 177/200\nlearning rate scheduled to 0.00016882221680134535\n1014/1014 [==============================] - 63s 59ms/step - loss: 775.6874 - accuracy: 0.7774 - val_loss: 775.1517 - val_accuracy: 0.7828\nEpoch 178/200\nlearning rate scheduled to 0.00016713398887077346\n1014/1014 [==============================] - 63s 59ms/step - loss: 774.6310 - accuracy: 0.7774 - val_loss: 774.1047 - val_accuracy: 0.7754\nEpoch 179/200\nlearning rate scheduled to 0.00016546264523640276\n1014/1014 [==============================] - 63s 59ms/step - loss: 773.5859 - accuracy: 0.7777 - val_loss: 773.0629 - val_accuracy: 0.7815\nEpoch 180/200\nlearning rate scheduled to 0.00016380801302148028\n1014/1014 [==============================] - 63s 59ms/step - loss: 772.5522 - accuracy: 0.7785 - val_loss: 772.0347 - val_accuracy: 0.7782\nEpoch 181/200\nlearning rate scheduled to 0.00016216993375564926\n1014/1014 [==============================] - 63s 59ms/step - loss: 771.5305 - accuracy: 0.7776 - val_loss: 771.0217 - val_accuracy: 0.7777\nEpoch 182/200\nlearning rate scheduled to 0.00016054823456215672\n1014/1014 [==============================] - 63s 59ms/step - loss: 770.5180 - accuracy: 0.7789 - val_loss: 770.0158 - val_accuracy: 0.7814\nEpoch 183/200\nlearning rate scheduled to 0.00015894275697064586\n1014/1014 [==============================] - 63s 59ms/step - loss: 769.5229 - accuracy: 0.7770 - val_loss: 769.0262 - val_accuracy: 0.7764\nEpoch 184/200\nlearning rate scheduled to 0.00015735332810436374\n1014/1014 [==============================] - 63s 59ms/step - loss: 768.5342 - accuracy: 0.7791 - val_loss: 768.0446 - val_accuracy: 0.7789\nEpoch 185/200\nlearning rate scheduled to 0.00015577978949295356\n1014/1014 [==============================] - 63s 59ms/step - loss: 767.5610 - accuracy: 0.7782 - val_loss: 767.0746 - val_accuracy: 0.7790\nEpoch 186/200\nlearning rate scheduled to 0.00015422199707245453\n1014/1014 [==============================] - 63s 59ms/step - loss: 766.5975 - accuracy: 0.7790 - val_loss: 766.1132 - val_accuracy: 0.7827\nEpoch 187/200\nlearning rate scheduled to 0.00015267977796611375\n1014/1014 [==============================] - 63s 59ms/step - loss: 765.6457 - accuracy: 0.7789 - val_loss: 765.1730 - val_accuracy: 0.7769\nEpoch 188/200\nlearning rate scheduled to 0.00015115297370357439\n1014/1014 [==============================] - 63s 59ms/step - loss: 764.7035 - accuracy: 0.7786 - val_loss: 764.2337 - val_accuracy: 0.7818\nEpoch 189/200\nlearning rate scheduled to 0.00014964144022087568\n1014/1014 [==============================] - 63s 59ms/step - loss: 763.7731 - accuracy: 0.7787 - val_loss: 763.3092 - val_accuracy: 0.7786\nEpoch 190/200\nlearning rate scheduled to 0.00014814501904766075\n1014/1014 [==============================] - 63s 59ms/step - loss: 762.8525 - accuracy: 0.7795 - val_loss: 762.3926 - val_accuracy: 0.7800\nEpoch 191/200\nlearning rate scheduled to 0.00014666356611996888\n1014/1014 [==============================] - 63s 59ms/step - loss: 761.9399 - accuracy: 0.7807 - val_loss: 761.4850 - val_accuracy: 0.7837\nEpoch 192/200\nlearning rate scheduled to 0.00014519693737383933\n1014/1014 [==============================] - 57s 53ms/step - loss: 761.0412 - accuracy: 0.7802 - val_loss: 760.5862 - val_accuracy: 0.7842\nEpoch 193/200\nlearning rate scheduled to 0.0001437449743389152\n1014/1014 [==============================] - 57s 53ms/step - loss: 760.1511 - accuracy: 0.7800 - val_loss: 759.7000 - val_accuracy: 0.7872\nEpoch 194/200\nlearning rate scheduled to 0.00014230751854483968\n1014/1014 [==============================] - 57s 53ms/step - loss: 759.2699 - accuracy: 0.7803 - val_loss: 758.8282 - val_accuracy: 0.7838\nEpoch 195/200\nlearning rate scheduled to 0.00014088444033404812\n1014/1014 [==============================] - 57s 53ms/step - loss: 758.3990 - accuracy: 0.7798 - val_loss: 757.9647 - val_accuracy: 0.7821\nEpoch 196/200\nlearning rate scheduled to 0.0001394755956425797\n1014/1014 [==============================] - 57s 53ms/step - loss: 757.5380 - accuracy: 0.7811 - val_loss: 757.1063 - val_accuracy: 0.7810\nEpoch 197/200\nlearning rate scheduled to 0.00013808084040647372\n1014/1014 [==============================] - 57s 53ms/step - loss: 756.6838 - accuracy: 0.7820 - val_loss: 756.2586 - val_accuracy: 0.7783\nEpoch 198/200\nlearning rate scheduled to 0.00013670003056176939\n1014/1014 [==============================] - 57s 53ms/step - loss: 755.8409 - accuracy: 0.7812 - val_loss: 755.4191 - val_accuracy: 0.7864\nEpoch 199/200\nlearning rate scheduled to 0.000135333036450902\n1014/1014 [==============================] - 57s 53ms/step - loss: 755.0052 - accuracy: 0.7828 - val_loss: 754.5991 - val_accuracy: 0.7768\nEpoch 200/200\nlearning rate scheduled to 0.00013397969960351474\n1014/1014 [==============================] - 59s 55ms/step - loss: 754.1797 - accuracy: 0.7818 - val_loss: 753.7651 - val_accuracy: 0.7837\n"
    }
   ],
   "source": [
    "history_original_siamese_model_6 = original_siamese_model_6.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                                                                                                                             model_checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "254/254 [==============================] - 8s 19ms/step - loss: 753.7661 - accuracy: 0.7866\n"
    }
   ],
   "source": [
    "loss, accuracy = original_siamese_model_6.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, original_siamese_model_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3090, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "mixed_precision.set_global_policy('mixed_float16')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def get_adapted_model(height, width, channels):\n",
    "\n",
    "    input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "    x = keras.layers.Conv2D(96, kernel_size=5, strides=2, activation='relu')(input)\n",
    "    x = keras.layers.MaxPooling2D(3, strides=2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(256, kernel_size=3, strides=1, padding=\"valid\", activation='relu')(x)\n",
    "    x = keras.layers.MaxPooling2D(3, strides=2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(384, kernel_size=3, strides=1, padding=\"valid\", activation='relu')(x)\n",
    "    x = keras.layers.Conv2D(384, kernel_size=3, strides=1, padding=\"valid\", activation='relu')(x)\n",
    "    x = keras.layers.Conv2D(256, kernel_size=3, strides=1, padding=\"valid\",  activation='relu')(x)\n",
    "    x = keras.layers.MaxPooling2D(3, strides=2)(x)\n",
    "\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(1024, activation='relu')(x)\n",
    "    output = keras.layers.Dense(1024, activation='relu')(x)\n",
    "\n",
    "    model = keras.models.Model(input, output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First Run - 30k Pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_30k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_30k_224_224/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 256\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "adapted_model = get_adapted_model(height,width,channels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 113, 113, 3)]     0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 55, 55, 96)        7296      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 27, 27, 96)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 25, 25, 256)       221440    \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 12, 12, 256)       0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 10, 10, 384)       885120    \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 8, 8, 384)         1327488   \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 6, 6, 256)         884992    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 2, 2, 256)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1024)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1024)              1049600   \n_________________________________________________________________\ndense_1 (Dense)              (None, 1024)              1049600   \n=================================================================\nTotal params: 5,425,536\nTrainable params: 5,425,536\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "adapted_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = adapted_model(left_input)\n",
    "encoded_r = adapted_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "adapted_siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/jmqrfpy6\" target=\"_blank\">likely-butterfly-35</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"momentum\": 0.9,\n",
    "                         \"weight_decay\": 0.0005,\n",
    "                         \"otimizer\": \"SGDW\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 150,\n",
    "                         \"architecture\": \"Adapted - 30k\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Adapted_30k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=5, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "adapted_siamese_model.compile(loss=config.loss_function,\n",
    "                               optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/150\n188/188 [==============================] - 15s 49ms/step - loss: 0.6763 - accuracy: 0.5548 - val_loss: 0.6513 - val_accuracy: 0.6459\nEpoch 2/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.6297 - accuracy: 0.6505 - val_loss: 0.6287 - val_accuracy: 0.6478\nEpoch 3/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.5947 - accuracy: 0.6785 - val_loss: 0.5679 - val_accuracy: 0.7034\nEpoch 4/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.5649 - accuracy: 0.7073 - val_loss: 0.6107 - val_accuracy: 0.6682\nEpoch 5/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.5299 - accuracy: 0.7363 - val_loss: 0.4959 - val_accuracy: 0.7515\nEpoch 6/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.5098 - accuracy: 0.7543 - val_loss: 0.4750 - val_accuracy: 0.7781\nEpoch 7/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.4700 - accuracy: 0.7805 - val_loss: 0.4515 - val_accuracy: 0.7883\nEpoch 8/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.4595 - accuracy: 0.7842 - val_loss: 0.4433 - val_accuracy: 0.7898\nEpoch 9/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.4816 - accuracy: 0.7733 - val_loss: 0.4524 - val_accuracy: 0.7998\nEpoch 10/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.4400 - accuracy: 0.7976 - val_loss: 0.4764 - val_accuracy: 0.7664\nEpoch 11/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.4447 - accuracy: 0.7922 - val_loss: 0.4268 - val_accuracy: 0.7977\nEpoch 12/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.4386 - accuracy: 0.7978 - val_loss: 0.4233 - val_accuracy: 0.8026\nEpoch 13/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.4437 - accuracy: 0.7910 - val_loss: 0.5496 - val_accuracy: 0.7349\nEpoch 14/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.4343 - accuracy: 0.8009 - val_loss: 0.4329 - val_accuracy: 0.8003\nEpoch 15/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.4257 - accuracy: 0.8021 - val_loss: 0.4083 - val_accuracy: 0.8160\nEpoch 16/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.4200 - accuracy: 0.8055 - val_loss: 0.4114 - val_accuracy: 0.8132\nEpoch 17/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.4045 - accuracy: 0.8158 - val_loss: 0.4159 - val_accuracy: 0.8056\nEpoch 18/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.3931 - accuracy: 0.8229 - val_loss: 0.3723 - val_accuracy: 0.8345\nEpoch 19/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.3825 - accuracy: 0.8278 - val_loss: 0.3737 - val_accuracy: 0.8331\nEpoch 20/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.3749 - accuracy: 0.8331 - val_loss: 0.3613 - val_accuracy: 0.8494\nEpoch 21/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.3783 - accuracy: 0.8325 - val_loss: 0.3516 - val_accuracy: 0.8445\nEpoch 22/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.3697 - accuracy: 0.8355 - val_loss: 0.3613 - val_accuracy: 0.8388\nEpoch 23/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.3655 - accuracy: 0.8386 - val_loss: 0.3815 - val_accuracy: 0.8260\nEpoch 24/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.3694 - accuracy: 0.8366 - val_loss: 0.3556 - val_accuracy: 0.8456\nEpoch 25/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.3583 - accuracy: 0.8432 - val_loss: 0.3340 - val_accuracy: 0.8575\nEpoch 26/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.3523 - accuracy: 0.8453 - val_loss: 0.3249 - val_accuracy: 0.8595\nEpoch 27/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.3499 - accuracy: 0.8439 - val_loss: 0.3327 - val_accuracy: 0.8588\nEpoch 28/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.3514 - accuracy: 0.8467 - val_loss: 0.3577 - val_accuracy: 0.8458\nEpoch 29/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.3422 - accuracy: 0.8510 - val_loss: 0.3689 - val_accuracy: 0.8371\nEpoch 30/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.3491 - accuracy: 0.8455 - val_loss: 0.3343 - val_accuracy: 0.8531\nEpoch 31/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.3329 - accuracy: 0.8517 - val_loss: 0.3317 - val_accuracy: 0.8556\n\nEpoch 00031: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\nEpoch 32/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.3162 - accuracy: 0.8659 - val_loss: 0.3044 - val_accuracy: 0.8709\nEpoch 33/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2974 - accuracy: 0.8762 - val_loss: 0.2923 - val_accuracy: 0.8791\nEpoch 34/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2927 - accuracy: 0.8776 - val_loss: 0.2922 - val_accuracy: 0.8763\nEpoch 35/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2886 - accuracy: 0.8790 - val_loss: 0.2791 - val_accuracy: 0.8878\nEpoch 36/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2870 - accuracy: 0.8797 - val_loss: 0.2767 - val_accuracy: 0.8861\nEpoch 37/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2828 - accuracy: 0.8818 - val_loss: 0.2842 - val_accuracy: 0.8797\nEpoch 38/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2832 - accuracy: 0.8801 - val_loss: 0.2672 - val_accuracy: 0.8884\nEpoch 39/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2793 - accuracy: 0.8821 - val_loss: 0.2745 - val_accuracy: 0.8840\nEpoch 40/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2787 - accuracy: 0.8836 - val_loss: 0.2816 - val_accuracy: 0.8825\nEpoch 41/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2702 - accuracy: 0.8888 - val_loss: 0.2634 - val_accuracy: 0.8901\nEpoch 42/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2711 - accuracy: 0.8881 - val_loss: 0.2574 - val_accuracy: 0.8935\nEpoch 43/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2704 - accuracy: 0.8876 - val_loss: 0.2700 - val_accuracy: 0.8863\nEpoch 44/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2665 - accuracy: 0.8875 - val_loss: 0.2746 - val_accuracy: 0.8856\nEpoch 45/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2669 - accuracy: 0.8887 - val_loss: 0.2711 - val_accuracy: 0.8890\nEpoch 46/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2594 - accuracy: 0.8939 - val_loss: 0.2662 - val_accuracy: 0.8895\nEpoch 47/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2603 - accuracy: 0.8911 - val_loss: 0.2564 - val_accuracy: 0.8944\nEpoch 48/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2609 - accuracy: 0.8904 - val_loss: 0.2606 - val_accuracy: 0.8927\nEpoch 49/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2552 - accuracy: 0.8937 - val_loss: 0.2544 - val_accuracy: 0.8956\nEpoch 50/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2565 - accuracy: 0.8935 - val_loss: 0.2474 - val_accuracy: 0.8988\nEpoch 51/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2540 - accuracy: 0.8954 - val_loss: 0.2351 - val_accuracy: 0.9069\nEpoch 52/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2494 - accuracy: 0.8968 - val_loss: 0.2397 - val_accuracy: 0.9014\nEpoch 53/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2447 - accuracy: 0.8991 - val_loss: 0.2367 - val_accuracy: 0.9005\nEpoch 54/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2416 - accuracy: 0.9002 - val_loss: 0.2298 - val_accuracy: 0.9103\nEpoch 55/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2424 - accuracy: 0.8998 - val_loss: 0.2252 - val_accuracy: 0.9093\nEpoch 56/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2388 - accuracy: 0.9019 - val_loss: 0.2265 - val_accuracy: 0.9078\nEpoch 57/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2341 - accuracy: 0.9043 - val_loss: 0.2233 - val_accuracy: 0.9067\nEpoch 58/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2333 - accuracy: 0.9042 - val_loss: 0.2350 - val_accuracy: 0.9037\nEpoch 59/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2319 - accuracy: 0.9041 - val_loss: 0.2234 - val_accuracy: 0.9080\nEpoch 60/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2273 - accuracy: 0.9061 - val_loss: 0.2388 - val_accuracy: 0.9010\nEpoch 61/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2269 - accuracy: 0.9053 - val_loss: 0.2211 - val_accuracy: 0.9101\nEpoch 62/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2216 - accuracy: 0.9104 - val_loss: 0.2168 - val_accuracy: 0.9112\nEpoch 63/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2193 - accuracy: 0.9111 - val_loss: 0.2089 - val_accuracy: 0.9133\nEpoch 64/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2174 - accuracy: 0.9111 - val_loss: 0.2077 - val_accuracy: 0.9148\nEpoch 65/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2164 - accuracy: 0.9116 - val_loss: 0.2063 - val_accuracy: 0.9163\nEpoch 66/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.2105 - accuracy: 0.9147 - val_loss: 0.2092 - val_accuracy: 0.9127\nEpoch 67/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2144 - accuracy: 0.9119 - val_loss: 0.2020 - val_accuracy: 0.9202\nEpoch 68/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2042 - accuracy: 0.9175 - val_loss: 0.1978 - val_accuracy: 0.9221\nEpoch 69/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.2027 - accuracy: 0.9188 - val_loss: 0.1925 - val_accuracy: 0.9240\nEpoch 70/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.1973 - accuracy: 0.9188 - val_loss: 0.1952 - val_accuracy: 0.9184\nEpoch 71/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1975 - accuracy: 0.9205 - val_loss: 0.1880 - val_accuracy: 0.9253\nEpoch 72/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.1892 - accuracy: 0.9253 - val_loss: 0.1837 - val_accuracy: 0.9310\nEpoch 73/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1903 - accuracy: 0.9250 - val_loss: 0.1770 - val_accuracy: 0.9331\nEpoch 74/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.1857 - accuracy: 0.9263 - val_loss: 0.1709 - val_accuracy: 0.9350\nEpoch 75/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1773 - accuracy: 0.9301 - val_loss: 0.1707 - val_accuracy: 0.9317\nEpoch 76/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.1746 - accuracy: 0.9324 - val_loss: 0.1664 - val_accuracy: 0.9350\nEpoch 77/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1715 - accuracy: 0.9339 - val_loss: 0.1545 - val_accuracy: 0.9436\nEpoch 78/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.1689 - accuracy: 0.9339 - val_loss: 0.1667 - val_accuracy: 0.9350\nEpoch 79/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1636 - accuracy: 0.9381 - val_loss: 0.1489 - val_accuracy: 0.9457\nEpoch 80/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1606 - accuracy: 0.9391 - val_loss: 0.1619 - val_accuracy: 0.9374\nEpoch 81/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.1589 - accuracy: 0.9394 - val_loss: 0.1369 - val_accuracy: 0.9512\nEpoch 82/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.1532 - accuracy: 0.9403 - val_loss: 0.1359 - val_accuracy: 0.9508\nEpoch 83/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1479 - accuracy: 0.9438 - val_loss: 0.1377 - val_accuracy: 0.9474\nEpoch 84/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.1423 - accuracy: 0.9466 - val_loss: 0.1313 - val_accuracy: 0.9572\nEpoch 85/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1383 - accuracy: 0.9493 - val_loss: 0.1352 - val_accuracy: 0.9529\nEpoch 86/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1367 - accuracy: 0.9505 - val_loss: 0.1279 - val_accuracy: 0.9531\nEpoch 87/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1352 - accuracy: 0.9495 - val_loss: 0.1222 - val_accuracy: 0.9580\nEpoch 88/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.1242 - accuracy: 0.9541 - val_loss: 0.1141 - val_accuracy: 0.9621\nEpoch 89/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1217 - accuracy: 0.9574 - val_loss: 0.1122 - val_accuracy: 0.9619\nEpoch 90/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.1226 - accuracy: 0.9547 - val_loss: 0.1139 - val_accuracy: 0.9600\nEpoch 91/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1131 - accuracy: 0.9609 - val_loss: 0.1022 - val_accuracy: 0.9674\nEpoch 92/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.1161 - accuracy: 0.9588 - val_loss: 0.0895 - val_accuracy: 0.9736\nEpoch 93/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.1041 - accuracy: 0.9653 - val_loss: 0.0908 - val_accuracy: 0.9751\nEpoch 94/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0988 - accuracy: 0.9663 - val_loss: 0.0898 - val_accuracy: 0.9740\nEpoch 95/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0990 - accuracy: 0.9663 - val_loss: 0.0826 - val_accuracy: 0.9766\nEpoch 96/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0892 - accuracy: 0.9719 - val_loss: 0.0751 - val_accuracy: 0.9804\nEpoch 97/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0880 - accuracy: 0.9716 - val_loss: 0.0707 - val_accuracy: 0.9819\nEpoch 98/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0777 - accuracy: 0.9767 - val_loss: 0.0708 - val_accuracy: 0.9806\nEpoch 99/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0764 - accuracy: 0.9773 - val_loss: 0.0580 - val_accuracy: 0.9881\nEpoch 100/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0689 - accuracy: 0.9807 - val_loss: 0.0677 - val_accuracy: 0.9796\nEpoch 101/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0805 - accuracy: 0.9720 - val_loss: 0.0803 - val_accuracy: 0.9762\nEpoch 102/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0719 - accuracy: 0.9780 - val_loss: 0.0496 - val_accuracy: 0.9917\nEpoch 103/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0590 - accuracy: 0.9840 - val_loss: 0.0494 - val_accuracy: 0.9885\nEpoch 104/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0565 - accuracy: 0.9853 - val_loss: 0.0607 - val_accuracy: 0.9789\nEpoch 105/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0520 - accuracy: 0.9861 - val_loss: 0.0423 - val_accuracy: 0.9917\nEpoch 106/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0471 - accuracy: 0.9891 - val_loss: 0.0486 - val_accuracy: 0.9872\nEpoch 107/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0550 - accuracy: 0.9847 - val_loss: 0.0488 - val_accuracy: 0.9887\nEpoch 108/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0434 - accuracy: 0.9893 - val_loss: 0.0431 - val_accuracy: 0.9906\nEpoch 109/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0419 - accuracy: 0.9901 - val_loss: 0.0315 - val_accuracy: 0.9943\nEpoch 110/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0339 - accuracy: 0.9940 - val_loss: 0.0285 - val_accuracy: 0.9957\nEpoch 111/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0279 - accuracy: 0.9965 - val_loss: 0.0298 - val_accuracy: 0.9934\nEpoch 112/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0248 - accuracy: 0.9972 - val_loss: 0.0202 - val_accuracy: 0.9992\nEpoch 113/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0236 - accuracy: 0.9971 - val_loss: 0.0194 - val_accuracy: 0.9987\nEpoch 114/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0201 - accuracy: 0.9985 - val_loss: 0.0187 - val_accuracy: 0.9987\nEpoch 115/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0180 - accuracy: 0.9987 - val_loss: 0.0151 - val_accuracy: 0.9998\nEpoch 116/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0165 - accuracy: 0.9993 - val_loss: 0.0126 - val_accuracy: 1.0000\nEpoch 117/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0137 - accuracy: 0.9996 - val_loss: 0.0122 - val_accuracy: 0.9996\nEpoch 118/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0122 - accuracy: 0.9997 - val_loss: 0.0158 - val_accuracy: 0.9983\nEpoch 119/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0106 - accuracy: 0.9999 - val_loss: 0.0087 - val_accuracy: 1.0000\nEpoch 120/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0090 - accuracy: 0.9999 - val_loss: 0.0076 - val_accuracy: 1.0000\nEpoch 121/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0086 - accuracy: 0.9999 - val_loss: 0.0078 - val_accuracy: 0.9998\nEpoch 122/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0073 - accuracy: 0.9999 - val_loss: 0.0067 - val_accuracy: 1.0000\nEpoch 123/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\nEpoch 124/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\nEpoch 125/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\nEpoch 126/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\nEpoch 127/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\nEpoch 128/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\nEpoch 129/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\nEpoch 130/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\nEpoch 131/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\nEpoch 132/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\nEpoch 133/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\nEpoch 134/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\nEpoch 135/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\nEpoch 136/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\nEpoch 137/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\nEpoch 138/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\nEpoch 139/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\nEpoch 140/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\nEpoch 141/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\nEpoch 142/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\nEpoch 143/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\nEpoch 144/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\nEpoch 145/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\nEpoch 146/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\nEpoch 147/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\nEpoch 148/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\nEpoch 149/150\n188/188 [==============================] - 10s 46ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\nEpoch 150/150\n188/188 [==============================] - 10s 45ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n"
    }
   ],
   "source": [
    "history_adapted_siamese_model = adapted_siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[model_checkpoint_callback, WandbCallback(), reduce_lr])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "47/47 [==============================] - 2s 15ms/step - loss: 0.0016 - accuracy: 1.0000\n"
    }
   ],
   "source": [
    "loss, accuracy = adapted_siamese_model.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, adapted_siamese_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Second Run - 90k Pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_90k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_90k_224_224/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 256\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "adapted_model = get_adapted_model(height,width,channels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_9 (InputLayer)         [(None, 113, 113, 3)]     0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 55, 55, 96)        7296      \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 27, 27, 96)        0         \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 25, 25, 256)       221440    \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 12, 12, 256)       0         \n_________________________________________________________________\nconv2d_12 (Conv2D)           (None, 10, 10, 384)       885120    \n_________________________________________________________________\nconv2d_13 (Conv2D)           (None, 8, 8, 384)         1327488   \n_________________________________________________________________\nconv2d_14 (Conv2D)           (None, 6, 6, 256)         884992    \n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 2, 2, 256)         0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 1024)              0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 1024)              1049600   \n_________________________________________________________________\ndense_8 (Dense)              (None, 1024)              1049600   \n=================================================================\nTotal params: 5,425,536\nTrainable params: 5,425,536\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "adapted_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = adapted_model(left_input)\n",
    "encoded_r = adapted_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "adapted_siamese_model_2 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/29yuoyn6\" target=\"_blank\">pretty-pond-37</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"momentum\": 0.9,\n",
    "                         \"weight_decay\": 0.0005,\n",
    "                         \"otimizer\": \"SGDW\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 150,\n",
    "                         \"architecture\": \"Adapted - 90k\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Adapted_90k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                                                 patience=5, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "adapted_siamese_model_2.compile(loss=config.loss_function,\n",
    "                              optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/150\n566/566 [==============================] - 34s 48ms/step - loss: 0.6273 - accuracy: 0.6376 - val_loss: 0.5553 - val_accuracy: 0.7143\nEpoch 2/150\n566/566 [==============================] - 31s 47ms/step - loss: 0.5280 - accuracy: 0.7400 - val_loss: 0.4869 - val_accuracy: 0.7668\nEpoch 3/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.4671 - accuracy: 0.7814 - val_loss: 0.4576 - val_accuracy: 0.7926\nEpoch 4/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.4440 - accuracy: 0.7930 - val_loss: 0.4209 - val_accuracy: 0.8089\nEpoch 5/150\n566/566 [==============================] - 30s 47ms/step - loss: 0.4172 - accuracy: 0.8091 - val_loss: 0.4264 - val_accuracy: 0.8061\nEpoch 6/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.4006 - accuracy: 0.8191 - val_loss: 0.3807 - val_accuracy: 0.8349\nEpoch 7/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3854 - accuracy: 0.8292 - val_loss: 0.3889 - val_accuracy: 0.8223\nEpoch 8/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3768 - accuracy: 0.8333 - val_loss: 0.3732 - val_accuracy: 0.8346\nEpoch 9/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3747 - accuracy: 0.8352 - val_loss: 0.3733 - val_accuracy: 0.8343\nEpoch 10/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3665 - accuracy: 0.8382 - val_loss: 0.3468 - val_accuracy: 0.8503\nEpoch 11/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3557 - accuracy: 0.8447 - val_loss: 0.3381 - val_accuracy: 0.8555\nEpoch 12/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3493 - accuracy: 0.8473 - val_loss: 0.3526 - val_accuracy: 0.8468\nEpoch 13/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3426 - accuracy: 0.8523 - val_loss: 0.3267 - val_accuracy: 0.8600\nEpoch 14/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3377 - accuracy: 0.8539 - val_loss: 0.3536 - val_accuracy: 0.8459\nEpoch 15/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3389 - accuracy: 0.8534 - val_loss: 0.3299 - val_accuracy: 0.8575\nEpoch 16/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3328 - accuracy: 0.8560 - val_loss: 0.3677 - val_accuracy: 0.8360\nEpoch 17/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3299 - accuracy: 0.8581 - val_loss: 0.3018 - val_accuracy: 0.8705\nEpoch 18/150\n566/566 [==============================] - 30s 46ms/step - loss: 0.3225 - accuracy: 0.8621 - val_loss: 0.3064 - val_accuracy: 0.8713\nEpoch 19/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.3242 - accuracy: 0.8604 - val_loss: 0.3166 - val_accuracy: 0.8633\nEpoch 20/150\n566/566 [==============================] - 29s 46ms/step - loss: 0.3126 - accuracy: 0.8671 - val_loss: 0.3128 - val_accuracy: 0.8668\nEpoch 21/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.3108 - accuracy: 0.8693 - val_loss: 0.3029 - val_accuracy: 0.8711\nEpoch 22/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.3033 - accuracy: 0.8725 - val_loss: 0.2998 - val_accuracy: 0.8720\nEpoch 23/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.3001 - accuracy: 0.8732 - val_loss: 0.2851 - val_accuracy: 0.8812\nEpoch 24/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.2901 - accuracy: 0.8801 - val_loss: 0.2847 - val_accuracy: 0.8817\nEpoch 25/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.2851 - accuracy: 0.8803 - val_loss: 0.2676 - val_accuracy: 0.8880\nEpoch 26/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.2847 - accuracy: 0.8818 - val_loss: 0.2543 - val_accuracy: 0.8980\nEpoch 27/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.2789 - accuracy: 0.8828 - val_loss: 0.2718 - val_accuracy: 0.8830\nEpoch 28/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.2727 - accuracy: 0.8860 - val_loss: 0.3083 - val_accuracy: 0.8695\nEpoch 29/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.2707 - accuracy: 0.8867 - val_loss: 0.2549 - val_accuracy: 0.8953\nEpoch 30/150\n566/566 [==============================] - 29s 46ms/step - loss: 0.2622 - accuracy: 0.8922 - val_loss: 0.2347 - val_accuracy: 0.9076\nEpoch 31/150\n566/566 [==============================] - 29s 46ms/step - loss: 0.2595 - accuracy: 0.8931 - val_loss: 0.2568 - val_accuracy: 0.8943\nEpoch 32/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.2565 - accuracy: 0.8947 - val_loss: 0.2546 - val_accuracy: 0.8947\nEpoch 33/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.2492 - accuracy: 0.8983 - val_loss: 0.2600 - val_accuracy: 0.8921\nEpoch 34/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.2442 - accuracy: 0.8991 - val_loss: 0.2458 - val_accuracy: 0.8974\nEpoch 35/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.2397 - accuracy: 0.9015 - val_loss: 0.2515 - val_accuracy: 0.8949\n\nEpoch 00035: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\nEpoch 36/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1949 - accuracy: 0.9236 - val_loss: 0.1707 - val_accuracy: 0.9359\nEpoch 37/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.1772 - accuracy: 0.9328 - val_loss: 0.1711 - val_accuracy: 0.9367\nEpoch 38/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1716 - accuracy: 0.9350 - val_loss: 0.1632 - val_accuracy: 0.9383\nEpoch 39/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1660 - accuracy: 0.9377 - val_loss: 0.1572 - val_accuracy: 0.9410\nEpoch 40/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.1611 - accuracy: 0.9396 - val_loss: 0.1515 - val_accuracy: 0.9436\nEpoch 41/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1576 - accuracy: 0.9404 - val_loss: 0.1508 - val_accuracy: 0.9449\nEpoch 42/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.1527 - accuracy: 0.9429 - val_loss: 0.1420 - val_accuracy: 0.9480\nEpoch 43/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1492 - accuracy: 0.9444 - val_loss: 0.1418 - val_accuracy: 0.9477\nEpoch 44/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.1470 - accuracy: 0.9454 - val_loss: 0.1374 - val_accuracy: 0.9503\nEpoch 45/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1417 - accuracy: 0.9479 - val_loss: 0.1336 - val_accuracy: 0.9520\nEpoch 46/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.1386 - accuracy: 0.9495 - val_loss: 0.1276 - val_accuracy: 0.9545\nEpoch 47/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1343 - accuracy: 0.9513 - val_loss: 0.1270 - val_accuracy: 0.9542\nEpoch 48/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1318 - accuracy: 0.9520 - val_loss: 0.1234 - val_accuracy: 0.9560\nEpoch 49/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1286 - accuracy: 0.9524 - val_loss: 0.1199 - val_accuracy: 0.9569\nEpoch 50/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1255 - accuracy: 0.9540 - val_loss: 0.1147 - val_accuracy: 0.9588\nEpoch 51/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1199 - accuracy: 0.9570 - val_loss: 0.1121 - val_accuracy: 0.9604\nEpoch 52/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1181 - accuracy: 0.9575 - val_loss: 0.1096 - val_accuracy: 0.9623\nEpoch 53/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1126 - accuracy: 0.9602 - val_loss: 0.1041 - val_accuracy: 0.9659\nEpoch 54/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1108 - accuracy: 0.9605 - val_loss: 0.1021 - val_accuracy: 0.9652\nEpoch 55/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1065 - accuracy: 0.9624 - val_loss: 0.0977 - val_accuracy: 0.9673\nEpoch 56/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.1013 - accuracy: 0.9651 - val_loss: 0.0938 - val_accuracy: 0.9686\nEpoch 57/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0980 - accuracy: 0.9661 - val_loss: 0.0912 - val_accuracy: 0.9707\nEpoch 58/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0930 - accuracy: 0.9681 - val_loss: 0.0862 - val_accuracy: 0.9730\nEpoch 59/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0909 - accuracy: 0.9689 - val_loss: 0.0824 - val_accuracy: 0.9732\nEpoch 60/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.0851 - accuracy: 0.9716 - val_loss: 0.0785 - val_accuracy: 0.9750\nEpoch 61/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0824 - accuracy: 0.9723 - val_loss: 0.0772 - val_accuracy: 0.9750\nEpoch 62/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0778 - accuracy: 0.9750 - val_loss: 0.0704 - val_accuracy: 0.9779\nEpoch 63/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0740 - accuracy: 0.9764 - val_loss: 0.0637 - val_accuracy: 0.9824\nEpoch 64/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0694 - accuracy: 0.9788 - val_loss: 0.0587 - val_accuracy: 0.9833\nEpoch 65/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0656 - accuracy: 0.9797 - val_loss: 0.0565 - val_accuracy: 0.9851\nEpoch 66/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0617 - accuracy: 0.9815 - val_loss: 0.0561 - val_accuracy: 0.9844\nEpoch 67/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0583 - accuracy: 0.9828 - val_loss: 0.0534 - val_accuracy: 0.9842\nEpoch 68/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0546 - accuracy: 0.9841 - val_loss: 0.0470 - val_accuracy: 0.9889\nEpoch 69/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0510 - accuracy: 0.9857 - val_loss: 0.0432 - val_accuracy: 0.9892\nEpoch 70/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0472 - accuracy: 0.9876 - val_loss: 0.0387 - val_accuracy: 0.9921\nEpoch 71/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.0427 - accuracy: 0.9892 - val_loss: 0.0357 - val_accuracy: 0.9928\nEpoch 72/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0394 - accuracy: 0.9906 - val_loss: 0.0323 - val_accuracy: 0.9941\nEpoch 73/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0374 - accuracy: 0.9916 - val_loss: 0.0405 - val_accuracy: 0.9881\nEpoch 74/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0338 - accuracy: 0.9925 - val_loss: 0.0278 - val_accuracy: 0.9960\nEpoch 75/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0308 - accuracy: 0.9937 - val_loss: 0.0256 - val_accuracy: 0.9959\nEpoch 76/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0275 - accuracy: 0.9951 - val_loss: 0.0233 - val_accuracy: 0.9971\nEpoch 77/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0259 - accuracy: 0.9955 - val_loss: 0.0228 - val_accuracy: 0.9969\nEpoch 78/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0228 - accuracy: 0.9963 - val_loss: 0.0210 - val_accuracy: 0.9972\nEpoch 79/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0213 - accuracy: 0.9969 - val_loss: 0.0175 - val_accuracy: 0.9982\nEpoch 80/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0182 - accuracy: 0.9979 - val_loss: 0.0151 - val_accuracy: 0.9991\nEpoch 81/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0159 - accuracy: 0.9987 - val_loss: 0.0141 - val_accuracy: 0.9989\nEpoch 82/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.0143 - accuracy: 0.9989 - val_loss: 0.0127 - val_accuracy: 0.9994\nEpoch 83/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0123 - accuracy: 0.9993 - val_loss: 0.0104 - val_accuracy: 0.9997\nEpoch 84/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.0107 - accuracy: 0.9995 - val_loss: 0.0096 - val_accuracy: 0.9997\nEpoch 85/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0100 - accuracy: 0.9995 - val_loss: 0.0080 - val_accuracy: 0.9999\nEpoch 86/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0086 - accuracy: 0.9998 - val_loss: 0.0081 - val_accuracy: 0.9997\nEpoch 87/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0075 - accuracy: 0.9999 - val_loss: 0.0066 - val_accuracy: 0.9999\nEpoch 88/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0069 - accuracy: 0.9998 - val_loss: 0.0064 - val_accuracy: 0.9999\nEpoch 89/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0062 - accuracy: 0.9999 - val_loss: 0.0057 - val_accuracy: 0.9999\nEpoch 90/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9999\nEpoch 91/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\nEpoch 92/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\nEpoch 93/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\nEpoch 94/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9999\nEpoch 95/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\nEpoch 96/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\nEpoch 97/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\nEpoch 98/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\nEpoch 99/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\nEpoch 100/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\nEpoch 101/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\nEpoch 102/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 0.9999\nEpoch 103/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\nEpoch 104/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\nEpoch 105/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\nEpoch 106/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\nEpoch 107/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\nEpoch 108/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 0.9999\nEpoch 109/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\nEpoch 110/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\nEpoch 111/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\nEpoch 112/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\nEpoch 113/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\nEpoch 114/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 0.9999\nEpoch 115/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 0.9999\nEpoch 116/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\nEpoch 117/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\nEpoch 118/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\nEpoch 119/150\n566/566 [==============================] - 29s 44ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\nEpoch 120/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\nEpoch 121/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.8358e-04 - val_accuracy: 1.0000\nEpoch 122/150\n566/566 [==============================] - 29s 45ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.7321e-04 - val_accuracy: 1.0000\nEpoch 123/150\n566/566 [==============================] - 28s 44ms/step - loss: 9.8348e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9999\nEpoch 124/150\n566/566 [==============================] - 29s 45ms/step - loss: 9.3605e-04 - accuracy: 1.0000 - val_loss: 9.0145e-04 - val_accuracy: 1.0000\nEpoch 125/150\n566/566 [==============================] - 28s 44ms/step - loss: 9.1133e-04 - accuracy: 1.0000 - val_loss: 8.6447e-04 - val_accuracy: 1.0000\nEpoch 126/150\n566/566 [==============================] - 28s 44ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\nEpoch 127/150\n566/566 [==============================] - 29s 45ms/step - loss: 9.5298e-04 - accuracy: 1.0000 - val_loss: 9.2597e-04 - val_accuracy: 1.0000\nEpoch 128/150\n566/566 [==============================] - 29s 44ms/step - loss: 9.4258e-04 - accuracy: 1.0000 - val_loss: 7.8632e-04 - val_accuracy: 1.0000\nEpoch 129/150\n566/566 [==============================] - 29s 45ms/step - loss: 8.2875e-04 - accuracy: 1.0000 - val_loss: 7.5390e-04 - val_accuracy: 1.0000\nEpoch 130/150\n566/566 [==============================] - 28s 44ms/step - loss: 9.6015e-04 - accuracy: 1.0000 - val_loss: 9.1449e-04 - val_accuracy: 1.0000\nEpoch 131/150\n566/566 [==============================] - 29s 45ms/step - loss: 8.8438e-04 - accuracy: 1.0000 - val_loss: 7.9481e-04 - val_accuracy: 1.0000\nEpoch 132/150\n566/566 [==============================] - 28s 44ms/step - loss: 8.4781e-04 - accuracy: 1.0000 - val_loss: 7.8083e-04 - val_accuracy: 1.0000\nEpoch 133/150\n566/566 [==============================] - 29s 45ms/step - loss: 9.5539e-04 - accuracy: 1.0000 - val_loss: 7.2724e-04 - val_accuracy: 1.0000\nEpoch 134/150\n566/566 [==============================] - 28s 44ms/step - loss: 8.3737e-04 - accuracy: 1.0000 - val_loss: 6.9492e-04 - val_accuracy: 1.0000\n\nEpoch 00134: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\nEpoch 135/150\n566/566 [==============================] - 29s 45ms/step - loss: 7.4292e-04 - accuracy: 1.0000 - val_loss: 6.7646e-04 - val_accuracy: 1.0000\nEpoch 136/150\n566/566 [==============================] - 29s 45ms/step - loss: 7.1562e-04 - accuracy: 1.0000 - val_loss: 6.5147e-04 - val_accuracy: 1.0000\nEpoch 137/150\n566/566 [==============================] - 29s 45ms/step - loss: 7.0515e-04 - accuracy: 1.0000 - val_loss: 6.8970e-04 - val_accuracy: 1.0000\nEpoch 138/150\n566/566 [==============================] - 29s 45ms/step - loss: 6.9948e-04 - accuracy: 1.0000 - val_loss: 6.5955e-04 - val_accuracy: 1.0000\nEpoch 139/150\n566/566 [==============================] - 29s 45ms/step - loss: 6.7373e-04 - accuracy: 1.0000 - val_loss: 6.6643e-04 - val_accuracy: 1.0000\nEpoch 140/150\n566/566 [==============================] - 29s 45ms/step - loss: 6.7866e-04 - accuracy: 1.0000 - val_loss: 6.8534e-04 - val_accuracy: 1.0000\nEpoch 141/150\n566/566 [==============================] - 29s 45ms/step - loss: 6.7519e-04 - accuracy: 1.0000 - val_loss: 6.6364e-04 - val_accuracy: 1.0000\n\nEpoch 00141: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\nEpoch 142/150\n566/566 [==============================] - 29s 45ms/step - loss: 6.8337e-04 - accuracy: 1.0000 - val_loss: 6.5186e-04 - val_accuracy: 1.0000\nEpoch 143/150\n566/566 [==============================] - 29s 45ms/step - loss: 6.8185e-04 - accuracy: 1.0000 - val_loss: 7.2886e-04 - val_accuracy: 0.9999\nEpoch 144/150\n566/566 [==============================] - 29s 45ms/step - loss: 6.8429e-04 - accuracy: 1.0000 - val_loss: 6.7368e-04 - val_accuracy: 1.0000\nEpoch 145/150\n566/566 [==============================] - 29s 45ms/step - loss: 6.6822e-04 - accuracy: 1.0000 - val_loss: 6.4922e-04 - val_accuracy: 1.0000\nEpoch 146/150\n566/566 [==============================] - 28s 44ms/step - loss: 6.7457e-04 - accuracy: 1.0000 - val_loss: 6.4491e-04 - val_accuracy: 1.0000\n\nEpoch 00146: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\nEpoch 147/150\n566/566 [==============================] - 29s 45ms/step - loss: 6.7589e-04 - accuracy: 1.0000 - val_loss: 6.5213e-04 - val_accuracy: 1.0000\nEpoch 148/150\n566/566 [==============================] - 29s 44ms/step - loss: 6.7591e-04 - accuracy: 1.0000 - val_loss: 6.4458e-04 - val_accuracy: 1.0000\nEpoch 149/150\n566/566 [==============================] - 29s 45ms/step - loss: 6.6808e-04 - accuracy: 1.0000 - val_loss: 6.7629e-04 - val_accuracy: 1.0000\nEpoch 150/150\n566/566 [==============================] - 28s 44ms/step - loss: 6.7920e-04 - accuracy: 1.0000 - val_loss: 7.0611e-04 - val_accuracy: 1.0000\n"
    }
   ],
   "source": [
    "history_adapted_siamese_model_2 = adapted_siamese_model_2.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[model_checkpoint_callback, WandbCallback(), reduce_lr])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "142/142 [==============================] - 6s 15ms/step - loss: 7.1380e-04 - accuracy: 0.9999\n"
    }
   ],
   "source": [
    "loss, accuracy = adapted_siamese_model_2.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, adapted_siamese_model_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Thirt Run - 150k Pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_150k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_150k_224_224/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 256\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "adapted_model = get_adapted_model(height,width,channels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_7\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_12 (InputLayer)        [(None, 113, 113, 3)]     0         \n_________________________________________________________________\nconv2d_15 (Conv2D)           (None, 55, 55, 96)        7296      \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 27, 27, 96)        0         \n_________________________________________________________________\nconv2d_16 (Conv2D)           (None, 25, 25, 256)       221440    \n_________________________________________________________________\nmax_pooling2d_10 (MaxPooling (None, 12, 12, 256)       0         \n_________________________________________________________________\nconv2d_17 (Conv2D)           (None, 10, 10, 384)       885120    \n_________________________________________________________________\nconv2d_18 (Conv2D)           (None, 8, 8, 384)         1327488   \n_________________________________________________________________\nconv2d_19 (Conv2D)           (None, 6, 6, 256)         884992    \n_________________________________________________________________\nmax_pooling2d_11 (MaxPooling (None, 2, 2, 256)         0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 1024)              0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 1024)              1049600   \n_________________________________________________________________\ndense_11 (Dense)             (None, 1024)              1049600   \n=================================================================\nTotal params: 5,425,536\nTrainable params: 5,425,536\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "adapted_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = adapted_model(left_input)\n",
    "encoded_r = adapted_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "adapted_siamese_model_3 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1c3s3u8z\" target=\"_blank\">fine-dust-38</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"momentum\": 0.9,\n",
    "                         \"weight_decay\": 0.0005,\n",
    "                         \"otimizer\": \"SGDW\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 150,\n",
    "                         \"architecture\": \"Adapted - 150k\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Adapted_150k\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                                                 patience=5, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "adapted_siamese_model_3.compile(loss=config.loss_function,\n",
    "                                optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/150\n942/942 [==============================] - 53s 47ms/step - loss: 0.5931 - accuracy: 0.6747 - val_loss: 0.4841 - val_accuracy: 0.7780\nEpoch 2/150\n942/942 [==============================] - 50s 46ms/step - loss: 0.4699 - accuracy: 0.7790 - val_loss: 0.5124 - val_accuracy: 0.7554\nEpoch 3/150\n942/942 [==============================] - 50s 46ms/step - loss: 0.4243 - accuracy: 0.8053 - val_loss: 0.4127 - val_accuracy: 0.8153\nEpoch 4/150\n942/942 [==============================] - 49s 45ms/step - loss: 0.4053 - accuracy: 0.8154 - val_loss: 0.3818 - val_accuracy: 0.8325\nEpoch 5/150\n942/942 [==============================] - 50s 46ms/step - loss: 0.3758 - accuracy: 0.8333 - val_loss: 0.3777 - val_accuracy: 0.8336\nEpoch 6/150\n942/942 [==============================] - 49s 45ms/step - loss: 0.3657 - accuracy: 0.8387 - val_loss: 0.3677 - val_accuracy: 0.8369\nEpoch 7/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.3578 - accuracy: 0.8430 - val_loss: 0.3521 - val_accuracy: 0.8475\nEpoch 8/150\n942/942 [==============================] - 49s 45ms/step - loss: 0.3464 - accuracy: 0.8496 - val_loss: 0.3438 - val_accuracy: 0.8496\nEpoch 9/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.3383 - accuracy: 0.8545 - val_loss: 0.3239 - val_accuracy: 0.8603\nEpoch 10/150\n942/942 [==============================] - 49s 45ms/step - loss: 0.3288 - accuracy: 0.8600 - val_loss: 0.3175 - val_accuracy: 0.8650\nEpoch 11/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.3186 - accuracy: 0.8651 - val_loss: 0.3435 - val_accuracy: 0.8505\nEpoch 12/150\n942/942 [==============================] - 49s 45ms/step - loss: 0.3127 - accuracy: 0.8666 - val_loss: 0.3166 - val_accuracy: 0.8636\nEpoch 13/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.3049 - accuracy: 0.8713 - val_loss: 0.3333 - val_accuracy: 0.8556\nEpoch 14/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.2997 - accuracy: 0.8742 - val_loss: 0.2746 - val_accuracy: 0.8877\nEpoch 15/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.2894 - accuracy: 0.8786 - val_loss: 0.2711 - val_accuracy: 0.8871\nEpoch 16/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.2832 - accuracy: 0.8826 - val_loss: 0.2708 - val_accuracy: 0.8855\nEpoch 17/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.2771 - accuracy: 0.8846 - val_loss: 0.2796 - val_accuracy: 0.8849\nEpoch 18/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.2689 - accuracy: 0.8891 - val_loss: 0.2687 - val_accuracy: 0.8906\nEpoch 19/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.2615 - accuracy: 0.8925 - val_loss: 0.2512 - val_accuracy: 0.8979\nEpoch 20/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.2569 - accuracy: 0.8962 - val_loss: 0.2543 - val_accuracy: 0.8978\nEpoch 21/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.2508 - accuracy: 0.8978 - val_loss: 0.2592 - val_accuracy: 0.8939\nEpoch 22/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.2488 - accuracy: 0.8987 - val_loss: 0.2401 - val_accuracy: 0.9034\nEpoch 23/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.2395 - accuracy: 0.9027 - val_loss: 0.2286 - val_accuracy: 0.9069\nEpoch 24/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.2349 - accuracy: 0.9054 - val_loss: 0.2075 - val_accuracy: 0.9208\nEpoch 25/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.2248 - accuracy: 0.9100 - val_loss: 0.2334 - val_accuracy: 0.9066\nEpoch 26/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.2204 - accuracy: 0.9122 - val_loss: 0.2298 - val_accuracy: 0.9065\nEpoch 27/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.2160 - accuracy: 0.9138 - val_loss: 0.2044 - val_accuracy: 0.9187\nEpoch 28/150\n942/942 [==============================] - 49s 45ms/step - loss: 0.2084 - accuracy: 0.9164 - val_loss: 0.1888 - val_accuracy: 0.9283\nEpoch 29/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.1988 - accuracy: 0.9209 - val_loss: 0.2092 - val_accuracy: 0.9141\nEpoch 30/150\n942/942 [==============================] - 49s 45ms/step - loss: 0.1993 - accuracy: 0.9206 - val_loss: 0.1935 - val_accuracy: 0.9215\nEpoch 31/150\n942/942 [==============================] - 49s 45ms/step - loss: 0.1897 - accuracy: 0.9250 - val_loss: 0.1724 - val_accuracy: 0.9337\nEpoch 32/150\n942/942 [==============================] - 49s 45ms/step - loss: 0.1823 - accuracy: 0.9284 - val_loss: 0.1811 - val_accuracy: 0.9272\nEpoch 33/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.1783 - accuracy: 0.9296 - val_loss: 0.1558 - val_accuracy: 0.9390\nEpoch 34/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.1705 - accuracy: 0.9327 - val_loss: 0.1837 - val_accuracy: 0.9252\nEpoch 35/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.1621 - accuracy: 0.9366 - val_loss: 0.1589 - val_accuracy: 0.9365\nEpoch 36/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.1534 - accuracy: 0.9400 - val_loss: 0.1571 - val_accuracy: 0.9372\nEpoch 37/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.1545 - accuracy: 0.9386 - val_loss: 0.1415 - val_accuracy: 0.9450\nEpoch 38/150\n942/942 [==============================] - 49s 46ms/step - loss: 0.1405 - accuracy: 0.9450 - val_loss: 0.1282 - val_accuracy: 0.9495\nEpoch 39/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.1370 - accuracy: 0.9459 - val_loss: 0.1349 - val_accuracy: 0.9464\nEpoch 40/150\n942/942 [==============================] - 49s 45ms/step - loss: 0.1340 - accuracy: 0.9472 - val_loss: 0.1120 - val_accuracy: 0.9573\nEpoch 41/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.1201 - accuracy: 0.9531 - val_loss: 0.1098 - val_accuracy: 0.9575\nEpoch 42/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.1159 - accuracy: 0.9544 - val_loss: 0.1609 - val_accuracy: 0.9352\nEpoch 43/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.1102 - accuracy: 0.9562 - val_loss: 0.0952 - val_accuracy: 0.9624\nEpoch 44/150\n942/942 [==============================] - 49s 45ms/step - loss: 0.1078 - accuracy: 0.9581 - val_loss: 0.1024 - val_accuracy: 0.9594\nEpoch 45/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.1018 - accuracy: 0.9598 - val_loss: 0.1052 - val_accuracy: 0.9572\nEpoch 46/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0960 - accuracy: 0.9627 - val_loss: 0.0602 - val_accuracy: 0.9789\nEpoch 47/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0832 - accuracy: 0.9675 - val_loss: 0.0935 - val_accuracy: 0.9632\nEpoch 48/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0805 - accuracy: 0.9691 - val_loss: 0.0637 - val_accuracy: 0.9747\nEpoch 49/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0797 - accuracy: 0.9692 - val_loss: 0.0602 - val_accuracy: 0.9773\nEpoch 50/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0713 - accuracy: 0.9723 - val_loss: 0.0659 - val_accuracy: 0.9747\nEpoch 51/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0664 - accuracy: 0.9749 - val_loss: 0.0550 - val_accuracy: 0.9797\nEpoch 52/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0604 - accuracy: 0.9771 - val_loss: 0.0398 - val_accuracy: 0.9850\nEpoch 53/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0586 - accuracy: 0.9781 - val_loss: 0.0477 - val_accuracy: 0.9815\nEpoch 54/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0600 - accuracy: 0.9770 - val_loss: 0.0372 - val_accuracy: 0.9867\nEpoch 55/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0500 - accuracy: 0.9813 - val_loss: 0.0483 - val_accuracy: 0.9822\nEpoch 56/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0472 - accuracy: 0.9816 - val_loss: 0.0434 - val_accuracy: 0.9838\nEpoch 57/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0547 - accuracy: 0.9789 - val_loss: 0.0390 - val_accuracy: 0.9861\nEpoch 58/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0479 - accuracy: 0.9819 - val_loss: 0.0324 - val_accuracy: 0.9886\nEpoch 59/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0442 - accuracy: 0.9833 - val_loss: 0.0276 - val_accuracy: 0.9907\nEpoch 60/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0415 - accuracy: 0.9848 - val_loss: 0.0346 - val_accuracy: 0.9872\nEpoch 61/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0380 - accuracy: 0.9859 - val_loss: 0.0277 - val_accuracy: 0.9906\nEpoch 62/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0364 - accuracy: 0.9866 - val_loss: 0.0286 - val_accuracy: 0.9892\nEpoch 63/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0385 - accuracy: 0.9860 - val_loss: 0.0337 - val_accuracy: 0.9870\nEpoch 64/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0362 - accuracy: 0.9866 - val_loss: 0.0451 - val_accuracy: 0.9826\n\nEpoch 00064: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\nEpoch 65/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0043 - val_accuracy: 0.9996\nEpoch 66/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0029 - val_accuracy: 0.9999\nEpoch 67/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.0019 - val_accuracy: 1.0000\nEpoch 68/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\nEpoch 69/150\n942/942 [==============================] - 48s 45ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\nEpoch 70/150\n942/942 [==============================] - 48s 44ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\nEpoch 71/150\n942/942 [==============================] - 48s 45ms/step - loss: 9.9193e-04 - accuracy: 1.0000 - val_loss: 8.5870e-04 - val_accuracy: 1.0000\nEpoch 72/150\n942/942 [==============================] - 48s 45ms/step - loss: 8.5379e-04 - accuracy: 1.0000 - val_loss: 7.9025e-04 - val_accuracy: 1.0000\nEpoch 73/150\n942/942 [==============================] - 48s 45ms/step - loss: 7.6003e-04 - accuracy: 1.0000 - val_loss: 6.8667e-04 - val_accuracy: 1.0000\nEpoch 74/150\n942/942 [==============================] - 48s 45ms/step - loss: 6.8764e-04 - accuracy: 1.0000 - val_loss: 6.2327e-04 - val_accuracy: 1.0000\nEpoch 75/150\n942/942 [==============================] - 48s 45ms/step - loss: 6.1949e-04 - accuracy: 1.0000 - val_loss: 5.7519e-04 - val_accuracy: 1.0000\nEpoch 76/150\n942/942 [==============================] - 48s 45ms/step - loss: 5.7026e-04 - accuracy: 1.0000 - val_loss: 5.2793e-04 - val_accuracy: 1.0000\nEpoch 77/150\n942/942 [==============================] - 48s 45ms/step - loss: 5.2505e-04 - accuracy: 1.0000 - val_loss: 4.7289e-04 - val_accuracy: 1.0000\nEpoch 78/150\n942/942 [==============================] - 48s 45ms/step - loss: 4.8798e-04 - accuracy: 1.0000 - val_loss: 4.4692e-04 - val_accuracy: 1.0000\nEpoch 79/150\n942/942 [==============================] - 48s 45ms/step - loss: 4.4752e-04 - accuracy: 1.0000 - val_loss: 4.1450e-04 - val_accuracy: 1.0000\nEpoch 80/150\n942/942 [==============================] - 48s 45ms/step - loss: 4.1646e-04 - accuracy: 1.0000 - val_loss: 3.8516e-04 - val_accuracy: 1.0000\nEpoch 81/150\n942/942 [==============================] - 48s 45ms/step - loss: 3.9687e-04 - accuracy: 1.0000 - val_loss: 3.7851e-04 - val_accuracy: 1.0000\nEpoch 82/150\n942/942 [==============================] - 48s 45ms/step - loss: 3.7022e-04 - accuracy: 1.0000 - val_loss: 3.6330e-04 - val_accuracy: 1.0000\nEpoch 83/150\n942/942 [==============================] - 48s 45ms/step - loss: 3.4974e-04 - accuracy: 1.0000 - val_loss: 3.3435e-04 - val_accuracy: 1.0000\nEpoch 84/150\n942/942 [==============================] - 48s 44ms/step - loss: 3.2925e-04 - accuracy: 1.0000 - val_loss: 3.1420e-04 - val_accuracy: 1.0000\nEpoch 85/150\n942/942 [==============================] - 48s 45ms/step - loss: 3.1577e-04 - accuracy: 1.0000 - val_loss: 2.9958e-04 - val_accuracy: 1.0000\nEpoch 86/150\n942/942 [==============================] - 48s 45ms/step - loss: 3.0345e-04 - accuracy: 1.0000 - val_loss: 2.7287e-04 - val_accuracy: 1.0000\nEpoch 87/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.8717e-04 - accuracy: 1.0000 - val_loss: 2.7983e-04 - val_accuracy: 1.0000\n\nEpoch 00087: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\nEpoch 88/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.7485e-04 - accuracy: 1.0000 - val_loss: 2.6860e-04 - val_accuracy: 1.0000\nEpoch 89/150\n942/942 [==============================] - 48s 44ms/step - loss: 2.7400e-04 - accuracy: 1.0000 - val_loss: 2.6362e-04 - val_accuracy: 1.0000\nEpoch 90/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.7147e-04 - accuracy: 1.0000 - val_loss: 2.7209e-04 - val_accuracy: 1.0000\nEpoch 91/150\n942/942 [==============================] - 48s 44ms/step - loss: 2.7023e-04 - accuracy: 1.0000 - val_loss: 2.6526e-04 - val_accuracy: 1.0000\nEpoch 92/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6924e-04 - accuracy: 1.0000 - val_loss: 2.5636e-04 - val_accuracy: 1.0000\nEpoch 93/150\n942/942 [==============================] - 48s 44ms/step - loss: 2.6868e-04 - accuracy: 1.0000 - val_loss: 2.5871e-04 - val_accuracy: 1.0000\nEpoch 94/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6288e-04 - accuracy: 1.0000 - val_loss: 2.7182e-04 - val_accuracy: 1.0000\nEpoch 95/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6611e-04 - accuracy: 1.0000 - val_loss: 2.5801e-04 - val_accuracy: 1.0000\nEpoch 96/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6388e-04 - accuracy: 1.0000 - val_loss: 2.5474e-04 - val_accuracy: 1.0000\nEpoch 97/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6456e-04 - accuracy: 1.0000 - val_loss: 2.7090e-04 - val_accuracy: 1.0000\n\nEpoch 00097: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\nEpoch 98/150\n942/942 [==============================] - 48s 44ms/step - loss: 2.6014e-04 - accuracy: 1.0000 - val_loss: 2.6214e-04 - val_accuracy: 1.0000\nEpoch 99/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6171e-04 - accuracy: 1.0000 - val_loss: 2.4581e-04 - val_accuracy: 1.0000\nEpoch 100/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6206e-04 - accuracy: 1.0000 - val_loss: 2.5116e-04 - val_accuracy: 1.0000\nEpoch 101/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6174e-04 - accuracy: 1.0000 - val_loss: 2.6635e-04 - val_accuracy: 1.0000\nEpoch 102/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6019e-04 - accuracy: 1.0000 - val_loss: 2.6203e-04 - val_accuracy: 1.0000\n\nEpoch 00102: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\nEpoch 103/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6241e-04 - accuracy: 1.0000 - val_loss: 2.5552e-04 - val_accuracy: 1.0000\nEpoch 104/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6475e-04 - accuracy: 1.0000 - val_loss: 2.6482e-04 - val_accuracy: 1.0000\nEpoch 105/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6029e-04 - accuracy: 1.0000 - val_loss: 2.6941e-04 - val_accuracy: 1.0000\nEpoch 106/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6237e-04 - accuracy: 1.0000 - val_loss: 2.6067e-04 - val_accuracy: 1.0000\nEpoch 107/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6359e-04 - accuracy: 1.0000 - val_loss: 2.6324e-04 - val_accuracy: 1.0000\n\nEpoch 00107: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\nEpoch 108/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6388e-04 - accuracy: 1.0000 - val_loss: 2.6585e-04 - val_accuracy: 1.0000\nEpoch 109/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6208e-04 - accuracy: 1.0000 - val_loss: 2.7341e-04 - val_accuracy: 1.0000\nEpoch 110/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6080e-04 - accuracy: 1.0000 - val_loss: 2.6026e-04 - val_accuracy: 1.0000\nEpoch 111/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6268e-04 - accuracy: 1.0000 - val_loss: 2.6092e-04 - val_accuracy: 1.0000\nEpoch 112/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6283e-04 - accuracy: 1.0000 - val_loss: 2.5171e-04 - val_accuracy: 1.0000\n\nEpoch 00112: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\nEpoch 113/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.5828e-04 - accuracy: 1.0000 - val_loss: 2.5946e-04 - val_accuracy: 1.0000\nEpoch 114/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6356e-04 - accuracy: 1.0000 - val_loss: 2.6111e-04 - val_accuracy: 1.0000\nEpoch 115/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6033e-04 - accuracy: 1.0000 - val_loss: 2.5861e-04 - val_accuracy: 1.0000\nEpoch 116/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6008e-04 - accuracy: 1.0000 - val_loss: 2.5923e-04 - val_accuracy: 1.0000\nEpoch 117/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6286e-04 - accuracy: 1.0000 - val_loss: 2.6606e-04 - val_accuracy: 1.0000\n\nEpoch 00117: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\nEpoch 118/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.5857e-04 - accuracy: 1.0000 - val_loss: 2.6782e-04 - val_accuracy: 1.0000\nEpoch 119/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6278e-04 - accuracy: 1.0000 - val_loss: 2.6891e-04 - val_accuracy: 1.0000\nEpoch 120/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6118e-04 - accuracy: 1.0000 - val_loss: 2.6644e-04 - val_accuracy: 1.0000\nEpoch 121/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6467e-04 - accuracy: 1.0000 - val_loss: 2.6547e-04 - val_accuracy: 1.0000\nEpoch 122/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6036e-04 - accuracy: 1.0000 - val_loss: 2.6941e-04 - val_accuracy: 1.0000\n\nEpoch 00122: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\nEpoch 123/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6133e-04 - accuracy: 1.0000 - val_loss: 2.5822e-04 - val_accuracy: 1.0000\nEpoch 124/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6417e-04 - accuracy: 1.0000 - val_loss: 2.6155e-04 - val_accuracy: 1.0000\nEpoch 125/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6254e-04 - accuracy: 1.0000 - val_loss: 2.6149e-04 - val_accuracy: 1.0000\nEpoch 126/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6468e-04 - accuracy: 1.0000 - val_loss: 2.5761e-04 - val_accuracy: 1.0000\nEpoch 127/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.5904e-04 - accuracy: 1.0000 - val_loss: 2.5300e-04 - val_accuracy: 1.0000\n\nEpoch 00127: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\nEpoch 128/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6366e-04 - accuracy: 1.0000 - val_loss: 2.5396e-04 - val_accuracy: 1.0000\nEpoch 129/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6166e-04 - accuracy: 1.0000 - val_loss: 2.5133e-04 - val_accuracy: 1.0000\nEpoch 130/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.5774e-04 - accuracy: 1.0000 - val_loss: 2.8063e-04 - val_accuracy: 1.0000\nEpoch 131/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6221e-04 - accuracy: 1.0000 - val_loss: 2.6468e-04 - val_accuracy: 1.0000\nEpoch 132/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6092e-04 - accuracy: 1.0000 - val_loss: 2.7031e-04 - val_accuracy: 1.0000\n\nEpoch 00132: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\nEpoch 133/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.5594e-04 - accuracy: 1.0000 - val_loss: 2.5661e-04 - val_accuracy: 1.0000\nEpoch 134/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6124e-04 - accuracy: 1.0000 - val_loss: 2.4898e-04 - val_accuracy: 1.0000\nEpoch 135/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6249e-04 - accuracy: 1.0000 - val_loss: 2.6098e-04 - val_accuracy: 1.0000\nEpoch 136/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6224e-04 - accuracy: 1.0000 - val_loss: 2.6101e-04 - val_accuracy: 1.0000\nEpoch 137/150\n942/942 [==============================] - 49s 46ms/step - loss: 2.6159e-04 - accuracy: 1.0000 - val_loss: 2.7029e-04 - val_accuracy: 1.0000\n\nEpoch 00137: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\nEpoch 138/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6000e-04 - accuracy: 1.0000 - val_loss: 2.6294e-04 - val_accuracy: 1.0000\nEpoch 139/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6257e-04 - accuracy: 1.0000 - val_loss: 2.6888e-04 - val_accuracy: 1.0000\nEpoch 140/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6443e-04 - accuracy: 1.0000 - val_loss: 2.6654e-04 - val_accuracy: 1.0000\nEpoch 141/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6162e-04 - accuracy: 1.0000 - val_loss: 2.6534e-04 - val_accuracy: 1.0000\nEpoch 142/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6101e-04 - accuracy: 1.0000 - val_loss: 2.6081e-04 - val_accuracy: 1.0000\n\nEpoch 00142: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\nEpoch 143/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6395e-04 - accuracy: 1.0000 - val_loss: 2.5863e-04 - val_accuracy: 1.0000\nEpoch 144/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6129e-04 - accuracy: 1.0000 - val_loss: 2.6261e-04 - val_accuracy: 1.0000\nEpoch 145/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.5973e-04 - accuracy: 1.0000 - val_loss: 2.6962e-04 - val_accuracy: 1.0000\nEpoch 146/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6141e-04 - accuracy: 1.0000 - val_loss: 2.5315e-04 - val_accuracy: 1.0000\nEpoch 147/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6037e-04 - accuracy: 1.0000 - val_loss: 2.6421e-04 - val_accuracy: 1.0000\n\nEpoch 00147: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\nEpoch 148/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6177e-04 - accuracy: 1.0000 - val_loss: 2.6507e-04 - val_accuracy: 1.0000\nEpoch 149/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6342e-04 - accuracy: 1.0000 - val_loss: 2.5933e-04 - val_accuracy: 1.0000\nEpoch 150/150\n942/942 [==============================] - 48s 45ms/step - loss: 2.6258e-04 - accuracy: 1.0000 - val_loss: 2.7133e-04 - val_accuracy: 1.0000\n"
    }
   ],
   "source": [
    "history_adapted_siamese_model_3 = adapted_siamese_model_3.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[model_checkpoint_callback, WandbCallback(), reduce_lr])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "236/236 [==============================] - 9s 15ms/step - loss: 2.6775e-04 - accuracy: 1.0000\n"
    }
   ],
   "source": [
    "loss, accuracy = adapted_siamese_model_3.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, adapted_siamese_model_3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Forth Run - 30k pairs gray"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_30k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_30k_224_224/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 256\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "adapted_model = get_adapted_model(height,width,channels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 113, 113, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 55, 55, 96)        2496      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 25, 25, 256)       221440    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 256)         884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "=================================================================\n",
      "Total params: 5,420,736\n",
      "Trainable params: 5,420,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adapted_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = adapted_model(left_input)\n",
    "encoded_r = adapted_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "adapted_siamese_model_4 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/l1n29x37\" target=\"_blank\">playful-wood-39</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"momentum\": 0.9,\n",
    "                         \"weight_decay\": 0.0005,\n",
    "                         \"otimizer\": \"SGDW\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 150,\n",
    "                         \"architecture\": \"Adapted - 30k - Grayscale\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Adapted_30k_Gray\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                                                 patience=5, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "adapted_siamese_model_4.compile(loss=config.loss_function,\n",
    "                                optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "188/188 [==============================] - 12s 42ms/step - loss: 0.6856 - accuracy: 0.5652 - val_loss: 0.6586 - val_accuracy: 0.6088\n",
      "Epoch 2/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.6382 - accuracy: 0.6433 - val_loss: 0.6179 - val_accuracy: 0.6637\n",
      "Epoch 3/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.6094 - accuracy: 0.6641 - val_loss: 0.6096 - val_accuracy: 0.6614\n",
      "Epoch 4/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5959 - accuracy: 0.6775 - val_loss: 0.5853 - val_accuracy: 0.6733\n",
      "Epoch 5/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5885 - accuracy: 0.6817 - val_loss: 0.5849 - val_accuracy: 0.6921\n",
      "Epoch 6/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5817 - accuracy: 0.6884 - val_loss: 0.5759 - val_accuracy: 0.6899\n",
      "Epoch 7/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5775 - accuracy: 0.6888 - val_loss: 0.5722 - val_accuracy: 0.6965\n",
      "Epoch 8/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5682 - accuracy: 0.6968 - val_loss: 0.5677 - val_accuracy: 0.6948\n",
      "Epoch 9/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5573 - accuracy: 0.7086 - val_loss: 0.5435 - val_accuracy: 0.7187\n",
      "Epoch 10/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5527 - accuracy: 0.7093 - val_loss: 0.5336 - val_accuracy: 0.7174\n",
      "Epoch 11/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5392 - accuracy: 0.7222 - val_loss: 0.5265 - val_accuracy: 0.7306\n",
      "Epoch 12/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5344 - accuracy: 0.7271 - val_loss: 0.5347 - val_accuracy: 0.7172\n",
      "Epoch 13/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5311 - accuracy: 0.7269 - val_loss: 0.5202 - val_accuracy: 0.7325\n",
      "Epoch 14/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5264 - accuracy: 0.7303 - val_loss: 0.5175 - val_accuracy: 0.7345\n",
      "Epoch 15/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5219 - accuracy: 0.7327 - val_loss: 0.5040 - val_accuracy: 0.7430\n",
      "Epoch 16/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5116 - accuracy: 0.7382 - val_loss: 0.4969 - val_accuracy: 0.7481\n",
      "Epoch 17/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5066 - accuracy: 0.7446 - val_loss: 0.4968 - val_accuracy: 0.7517\n",
      "Epoch 18/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.5030 - accuracy: 0.7423 - val_loss: 0.4946 - val_accuracy: 0.7543\n",
      "Epoch 19/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4977 - accuracy: 0.7484 - val_loss: 0.4883 - val_accuracy: 0.7534\n",
      "Epoch 20/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4962 - accuracy: 0.7483 - val_loss: 0.4823 - val_accuracy: 0.7547\n",
      "Epoch 21/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4878 - accuracy: 0.7558 - val_loss: 0.4812 - val_accuracy: 0.7536\n",
      "Epoch 22/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4788 - accuracy: 0.7583 - val_loss: 0.4727 - val_accuracy: 0.7623\n",
      "Epoch 23/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4782 - accuracy: 0.7599 - val_loss: 0.4474 - val_accuracy: 0.7845\n",
      "Epoch 24/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4770 - accuracy: 0.7625 - val_loss: 0.4666 - val_accuracy: 0.7653\n",
      "Epoch 25/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4680 - accuracy: 0.7678 - val_loss: 0.4466 - val_accuracy: 0.7853\n",
      "Epoch 26/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4621 - accuracy: 0.7704 - val_loss: 0.4516 - val_accuracy: 0.7807\n",
      "Epoch 27/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4601 - accuracy: 0.7730 - val_loss: 0.4685 - val_accuracy: 0.7721\n",
      "Epoch 28/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4533 - accuracy: 0.7757 - val_loss: 0.4246 - val_accuracy: 0.7983\n",
      "Epoch 29/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4425 - accuracy: 0.7833 - val_loss: 0.4152 - val_accuracy: 0.7990\n",
      "Epoch 30/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4383 - accuracy: 0.7854 - val_loss: 0.4016 - val_accuracy: 0.8058\n",
      "Epoch 31/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4312 - accuracy: 0.7921 - val_loss: 0.3995 - val_accuracy: 0.8124\n",
      "Epoch 32/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4244 - accuracy: 0.7981 - val_loss: 0.4127 - val_accuracy: 0.8024\n",
      "Epoch 33/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4154 - accuracy: 0.8023 - val_loss: 0.3863 - val_accuracy: 0.8213\n",
      "Epoch 34/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.4097 - accuracy: 0.8033 - val_loss: 0.3805 - val_accuracy: 0.8239\n",
      "Epoch 35/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.3973 - accuracy: 0.8114 - val_loss: 0.3839 - val_accuracy: 0.8301\n",
      "Epoch 36/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.3801 - accuracy: 0.8198 - val_loss: 0.3537 - val_accuracy: 0.8379\n",
      "Epoch 37/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.3774 - accuracy: 0.8247 - val_loss: 0.3532 - val_accuracy: 0.8403\n",
      "Epoch 38/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.3591 - accuracy: 0.8357 - val_loss: 0.3334 - val_accuracy: 0.8479\n",
      "Epoch 39/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.3655 - accuracy: 0.8298 - val_loss: 0.3372 - val_accuracy: 0.8546\n",
      "Epoch 40/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.3470 - accuracy: 0.8419 - val_loss: 0.3029 - val_accuracy: 0.8722\n",
      "Epoch 41/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.3235 - accuracy: 0.8531 - val_loss: 0.2973 - val_accuracy: 0.8673\n",
      "Epoch 42/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.3204 - accuracy: 0.8577 - val_loss: 0.2927 - val_accuracy: 0.8626\n",
      "Epoch 43/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.3015 - accuracy: 0.8662 - val_loss: 0.2562 - val_accuracy: 0.8899\n",
      "Epoch 44/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.2960 - accuracy: 0.8700 - val_loss: 0.2401 - val_accuracy: 0.9101\n",
      "Epoch 45/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.2773 - accuracy: 0.8785 - val_loss: 0.2549 - val_accuracy: 0.8937\n",
      "Epoch 46/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.2663 - accuracy: 0.8866 - val_loss: 0.2255 - val_accuracy: 0.9052\n",
      "Epoch 47/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.2378 - accuracy: 0.8974 - val_loss: 0.2349 - val_accuracy: 0.8995\n",
      "Epoch 48/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.2315 - accuracy: 0.9019 - val_loss: 0.1992 - val_accuracy: 0.9223\n",
      "Epoch 49/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.2215 - accuracy: 0.9080 - val_loss: 0.1787 - val_accuracy: 0.9312\n",
      "Epoch 50/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.2092 - accuracy: 0.9132 - val_loss: 0.2359 - val_accuracy: 0.8978\n",
      "Epoch 51/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1907 - accuracy: 0.9228 - val_loss: 0.1768 - val_accuracy: 0.9265\n",
      "Epoch 52/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1983 - accuracy: 0.9205 - val_loss: 0.1897 - val_accuracy: 0.9216\n",
      "Epoch 53/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1734 - accuracy: 0.9278 - val_loss: 0.1408 - val_accuracy: 0.9483\n",
      "Epoch 54/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1609 - accuracy: 0.9356 - val_loss: 0.1686 - val_accuracy: 0.9319\n",
      "Epoch 55/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1539 - accuracy: 0.9398 - val_loss: 0.1269 - val_accuracy: 0.9478\n",
      "Epoch 56/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1359 - accuracy: 0.9472 - val_loss: 0.1449 - val_accuracy: 0.9434\n",
      "Epoch 57/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1446 - accuracy: 0.9437 - val_loss: 0.1049 - val_accuracy: 0.9612\n",
      "Epoch 58/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1187 - accuracy: 0.9548 - val_loss: 0.0838 - val_accuracy: 0.9695\n",
      "Epoch 59/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1182 - accuracy: 0.9530 - val_loss: 0.1165 - val_accuracy: 0.9555\n",
      "Epoch 60/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1194 - accuracy: 0.9538 - val_loss: 0.1135 - val_accuracy: 0.9602\n",
      "Epoch 61/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1002 - accuracy: 0.9626 - val_loss: 0.1292 - val_accuracy: 0.9487\n",
      "Epoch 62/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1064 - accuracy: 0.9585 - val_loss: 0.0624 - val_accuracy: 0.9774\n",
      "Epoch 63/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1026 - accuracy: 0.9614 - val_loss: 0.0858 - val_accuracy: 0.9668\n",
      "Epoch 64/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0969 - accuracy: 0.9619 - val_loss: 0.0593 - val_accuracy: 0.9783\n",
      "Epoch 65/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.1079 - accuracy: 0.9589 - val_loss: 0.0861 - val_accuracy: 0.9698\n",
      "Epoch 66/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0902 - accuracy: 0.9663 - val_loss: 0.0614 - val_accuracy: 0.9753\n",
      "Epoch 67/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0955 - accuracy: 0.9633 - val_loss: 0.1699 - val_accuracy: 0.9367\n",
      "Epoch 68/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0781 - accuracy: 0.9716 - val_loss: 0.0563 - val_accuracy: 0.9806\n",
      "Epoch 69/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0673 - accuracy: 0.9758 - val_loss: 0.0530 - val_accuracy: 0.9821\n",
      "Epoch 70/150\n",
      "188/188 [==============================] - 8s 39ms/step - loss: 0.0894 - accuracy: 0.9685 - val_loss: 0.0726 - val_accuracy: 0.9747\n",
      "Epoch 71/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0694 - accuracy: 0.9749 - val_loss: 0.0829 - val_accuracy: 0.9702\n",
      "Epoch 72/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0626 - accuracy: 0.9779 - val_loss: 0.0761 - val_accuracy: 0.9740\n",
      "Epoch 73/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0770 - accuracy: 0.9701 - val_loss: 0.0670 - val_accuracy: 0.9753\n",
      "Epoch 74/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0581 - accuracy: 0.9793 - val_loss: 0.0513 - val_accuracy: 0.9810\n",
      "Epoch 75/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0812 - accuracy: 0.9712 - val_loss: 0.0659 - val_accuracy: 0.9761\n",
      "Epoch 76/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0552 - accuracy: 0.9795 - val_loss: 0.0405 - val_accuracy: 0.9855\n",
      "Epoch 77/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0590 - accuracy: 0.9778 - val_loss: 0.0334 - val_accuracy: 0.9872\n",
      "Epoch 78/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0584 - accuracy: 0.9785 - val_loss: 0.0518 - val_accuracy: 0.9825\n",
      "Epoch 79/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0485 - accuracy: 0.9829 - val_loss: 0.0253 - val_accuracy: 0.9910\n",
      "Epoch 80/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0434 - accuracy: 0.9841 - val_loss: 0.0174 - val_accuracy: 0.9932\n",
      "Epoch 81/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0668 - accuracy: 0.9763 - val_loss: 0.0676 - val_accuracy: 0.9783\n",
      "Epoch 82/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0647 - accuracy: 0.9769 - val_loss: 0.0392 - val_accuracy: 0.9851\n",
      "Epoch 83/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0411 - accuracy: 0.9855 - val_loss: 0.0412 - val_accuracy: 0.9864\n",
      "Epoch 84/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0601 - accuracy: 0.9787 - val_loss: 0.0275 - val_accuracy: 0.9896\n",
      "Epoch 85/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0351 - accuracy: 0.9883 - val_loss: 0.0437 - val_accuracy: 0.9860\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 86/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0250 - accuracy: 0.9922 - val_loss: 0.0050 - val_accuracy: 0.9992\n",
      "Epoch 87/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
      "Epoch 88/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 9.9890e-04 - accuracy: 1.0000 - val_loss: 8.8576e-04 - val_accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 8.5737e-04 - accuracy: 1.0000 - val_loss: 7.0888e-04 - val_accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 7.4941e-04 - accuracy: 1.0000 - val_loss: 7.3746e-04 - val_accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 6.9013e-04 - accuracy: 1.0000 - val_loss: 6.6767e-04 - val_accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 6.1102e-04 - accuracy: 1.0000 - val_loss: 5.9111e-04 - val_accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 5.7356e-04 - accuracy: 1.0000 - val_loss: 5.1351e-04 - val_accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 5.2539e-04 - accuracy: 1.0000 - val_loss: 5.0617e-04 - val_accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 5.0024e-04 - accuracy: 1.0000 - val_loss: 4.6907e-04 - val_accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 4.6436e-04 - accuracy: 1.0000 - val_loss: 4.4593e-04 - val_accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 4.2166e-04 - accuracy: 1.0000 - val_loss: 4.3298e-04 - val_accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 4.0647e-04 - accuracy: 1.0000 - val_loss: 3.6209e-04 - val_accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.9205e-04 - accuracy: 1.0000 - val_loss: 3.7352e-04 - val_accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.6063e-04 - accuracy: 1.0000 - val_loss: 3.4127e-04 - val_accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.4698e-04 - accuracy: 1.0000 - val_loss: 3.6518e-04 - val_accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.3359e-04 - accuracy: 1.0000 - val_loss: 3.2080e-04 - val_accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.1271e-04 - accuracy: 1.0000 - val_loss: 2.9757e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 107/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0870e-04 - accuracy: 1.0000 - val_loss: 3.1159e-04 - val_accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0151e-04 - accuracy: 1.0000 - val_loss: 2.8230e-04 - val_accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0574e-04 - accuracy: 1.0000 - val_loss: 3.0089e-04 - val_accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9808e-04 - accuracy: 1.0000 - val_loss: 2.9844e-04 - val_accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9470e-04 - accuracy: 1.0000 - val_loss: 2.9444e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 112/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0305e-04 - accuracy: 1.0000 - val_loss: 2.7527e-04 - val_accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9809e-04 - accuracy: 1.0000 - val_loss: 2.9536e-04 - val_accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9704e-04 - accuracy: 1.0000 - val_loss: 2.9740e-04 - val_accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0011e-04 - accuracy: 1.0000 - val_loss: 3.0874e-04 - val_accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9674e-04 - accuracy: 1.0000 - val_loss: 3.0009e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 117/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0058e-04 - accuracy: 1.0000 - val_loss: 3.2745e-04 - val_accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0343e-04 - accuracy: 1.0000 - val_loss: 2.8706e-04 - val_accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9684e-04 - accuracy: 1.0000 - val_loss: 3.0263e-04 - val_accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9963e-04 - accuracy: 1.0000 - val_loss: 3.1344e-04 - val_accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9680e-04 - accuracy: 1.0000 - val_loss: 3.0253e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "Epoch 122/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0042e-04 - accuracy: 1.0000 - val_loss: 3.0252e-04 - val_accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9868e-04 - accuracy: 1.0000 - val_loss: 3.0482e-04 - val_accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9622e-04 - accuracy: 1.0000 - val_loss: 2.7420e-04 - val_accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0140e-04 - accuracy: 1.0000 - val_loss: 2.9110e-04 - val_accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0227e-04 - accuracy: 1.0000 - val_loss: 3.1892e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "Epoch 127/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9950e-04 - accuracy: 1.0000 - val_loss: 3.0264e-04 - val_accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9789e-04 - accuracy: 1.0000 - val_loss: 3.0518e-04 - val_accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0189e-04 - accuracy: 1.0000 - val_loss: 3.0435e-04 - val_accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9960e-04 - accuracy: 1.0000 - val_loss: 3.1154e-04 - val_accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0038e-04 - accuracy: 1.0000 - val_loss: 2.7138e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "Epoch 132/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9839e-04 - accuracy: 1.0000 - val_loss: 3.3766e-04 - val_accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9519e-04 - accuracy: 1.0000 - val_loss: 2.8619e-04 - val_accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9903e-04 - accuracy: 1.0000 - val_loss: 2.9209e-04 - val_accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0689e-04 - accuracy: 1.0000 - val_loss: 3.0065e-04 - val_accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9732e-04 - accuracy: 1.0000 - val_loss: 2.9287e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "Epoch 137/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9964e-04 - accuracy: 1.0000 - val_loss: 3.1034e-04 - val_accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0168e-04 - accuracy: 1.0000 - val_loss: 3.0038e-04 - val_accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0148e-04 - accuracy: 1.0000 - val_loss: 3.0162e-04 - val_accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0423e-04 - accuracy: 1.0000 - val_loss: 3.2370e-04 - val_accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9764e-04 - accuracy: 1.0000 - val_loss: 3.0056e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\n",
      "Epoch 142/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0221e-04 - accuracy: 1.0000 - val_loss: 2.8976e-04 - val_accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0275e-04 - accuracy: 1.0000 - val_loss: 3.0338e-04 - val_accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0454e-04 - accuracy: 1.0000 - val_loss: 3.0849e-04 - val_accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9942e-04 - accuracy: 1.0000 - val_loss: 2.9926e-04 - val_accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0029e-04 - accuracy: 1.0000 - val_loss: 2.7358e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00146: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 147/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0114e-04 - accuracy: 1.0000 - val_loss: 3.0270e-04 - val_accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0370e-04 - accuracy: 1.0000 - val_loss: 2.8844e-04 - val_accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 3.0170e-04 - accuracy: 1.0000 - val_loss: 2.9092e-04 - val_accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "188/188 [==============================] - 8s 38ms/step - loss: 2.9514e-04 - accuracy: 1.0000 - val_loss: 3.0091e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history_adapted_siamese_model_4 = adapted_siamese_model_4.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[model_checkpoint_callback, WandbCallback(), reduce_lr])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 10ms/step - loss: 3.0935e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = adapted_siamese_model_4.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, adapted_siamese_model_4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fifth Run - 90k Pairs gray"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_90k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_90k_224_224/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 256\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "adapted_model = get_adapted_model(height,width,channels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 113, 113, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 55, 55, 96)        2496      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 25, 25, 256)       221440    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 10, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 6, 6, 256)         884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "=================================================================\n",
      "Total params: 5,420,736\n",
      "Trainable params: 5,420,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adapted_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = adapted_model(left_input)\n",
    "encoded_r = adapted_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "adapted_siamese_model_5 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3mwag0un\" target=\"_blank\">rose-resonance-40</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"momentum\": 0.9,\n",
    "                         \"weight_decay\": 0.0005,\n",
    "                         \"otimizer\": \"SGDW\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 150,\n",
    "                         \"architecture\": \"Adapted - 90k - Grayscale\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Adapted_90k_Gray\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                                                 patience=5, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "adapted_siamese_model_5.compile(loss=config.loss_function,\n",
    "                                optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "566/566 [==============================] - 25s 39ms/step - loss: 0.6450 - accuracy: 0.6156 - val_loss: 0.5977 - val_accuracy: 0.6813\n",
      "Epoch 2/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 0.5958 - accuracy: 0.6771 - val_loss: 0.5780 - val_accuracy: 0.6915\n",
      "Epoch 3/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 0.5786 - accuracy: 0.6886 - val_loss: 0.5694 - val_accuracy: 0.6902\n",
      "Epoch 4/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 0.5570 - accuracy: 0.7063 - val_loss: 0.5591 - val_accuracy: 0.7059\n",
      "Epoch 5/150\n",
      "566/566 [==============================] - 25s 41ms/step - loss: 0.5383 - accuracy: 0.7230 - val_loss: 0.5343 - val_accuracy: 0.7245\n",
      "Epoch 6/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.5229 - accuracy: 0.7320 - val_loss: 0.5523 - val_accuracy: 0.7165\n",
      "Epoch 7/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 0.5127 - accuracy: 0.7375 - val_loss: 0.5059 - val_accuracy: 0.7413\n",
      "Epoch 8/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.5046 - accuracy: 0.7429 - val_loss: 0.4936 - val_accuracy: 0.7526\n",
      "Epoch 9/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4985 - accuracy: 0.7465 - val_loss: 0.5157 - val_accuracy: 0.7392\n",
      "Epoch 10/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4944 - accuracy: 0.7487 - val_loss: 0.4896 - val_accuracy: 0.7544\n",
      "Epoch 11/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4839 - accuracy: 0.7574 - val_loss: 0.4744 - val_accuracy: 0.7609\n",
      "Epoch 12/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4795 - accuracy: 0.7596 - val_loss: 0.4747 - val_accuracy: 0.7629\n",
      "Epoch 13/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4738 - accuracy: 0.7636 - val_loss: 0.4579 - val_accuracy: 0.7721\n",
      "Epoch 14/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4648 - accuracy: 0.7709 - val_loss: 0.4457 - val_accuracy: 0.7804\n",
      "Epoch 15/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4613 - accuracy: 0.7720 - val_loss: 0.4730 - val_accuracy: 0.7608\n",
      "Epoch 16/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4534 - accuracy: 0.7767 - val_loss: 0.4676 - val_accuracy: 0.7678\n",
      "Epoch 17/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4428 - accuracy: 0.7838 - val_loss: 0.4259 - val_accuracy: 0.8004\n",
      "Epoch 18/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4380 - accuracy: 0.7875 - val_loss: 0.4437 - val_accuracy: 0.7865\n",
      "Epoch 19/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4337 - accuracy: 0.7906 - val_loss: 0.4254 - val_accuracy: 0.7987\n",
      "Epoch 20/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 0.4213 - accuracy: 0.7979 - val_loss: 0.4380 - val_accuracy: 0.7897\n",
      "Epoch 21/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4161 - accuracy: 0.8017 - val_loss: 0.4077 - val_accuracy: 0.8089\n",
      "Epoch 22/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.4102 - accuracy: 0.8058 - val_loss: 0.3788 - val_accuracy: 0.8218\n",
      "Epoch 23/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.3959 - accuracy: 0.8141 - val_loss: 0.3592 - val_accuracy: 0.8361\n",
      "Epoch 24/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.3872 - accuracy: 0.8184 - val_loss: 0.3681 - val_accuracy: 0.8310\n",
      "Epoch 25/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.3781 - accuracy: 0.8225 - val_loss: 0.3501 - val_accuracy: 0.8437\n",
      "Epoch 26/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.3711 - accuracy: 0.8278 - val_loss: 0.3496 - val_accuracy: 0.8421\n",
      "Epoch 27/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.3563 - accuracy: 0.8348 - val_loss: 0.3174 - val_accuracy: 0.8596\n",
      "Epoch 28/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.3450 - accuracy: 0.8417 - val_loss: 0.3024 - val_accuracy: 0.8679\n",
      "Epoch 29/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.3339 - accuracy: 0.8477 - val_loss: 0.3000 - val_accuracy: 0.8698\n",
      "Epoch 30/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.3203 - accuracy: 0.8547 - val_loss: 0.2898 - val_accuracy: 0.8742\n",
      "Epoch 31/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.3076 - accuracy: 0.8613 - val_loss: 0.2690 - val_accuracy: 0.8844\n",
      "Epoch 32/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.2951 - accuracy: 0.8687 - val_loss: 0.2680 - val_accuracy: 0.8830\n",
      "Epoch 33/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.2800 - accuracy: 0.8773 - val_loss: 0.2573 - val_accuracy: 0.8876\n",
      "Epoch 34/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.2686 - accuracy: 0.8823 - val_loss: 0.2250 - val_accuracy: 0.9084\n",
      "Epoch 35/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.2489 - accuracy: 0.8925 - val_loss: 0.2213 - val_accuracy: 0.9083\n",
      "Epoch 36/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.2357 - accuracy: 0.8984 - val_loss: 0.2025 - val_accuracy: 0.9166\n",
      "Epoch 37/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.2200 - accuracy: 0.9053 - val_loss: 0.1862 - val_accuracy: 0.9224\n",
      "Epoch 38/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.2061 - accuracy: 0.9137 - val_loss: 0.1715 - val_accuracy: 0.9329\n",
      "Epoch 39/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.1932 - accuracy: 0.9201 - val_loss: 0.1801 - val_accuracy: 0.9271\n",
      "Epoch 40/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.1748 - accuracy: 0.9279 - val_loss: 0.1401 - val_accuracy: 0.9455\n",
      "Epoch 41/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.1706 - accuracy: 0.9293 - val_loss: 0.1193 - val_accuracy: 0.9548\n",
      "Epoch 42/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.1540 - accuracy: 0.9369 - val_loss: 0.1307 - val_accuracy: 0.9481\n",
      "Epoch 43/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.1509 - accuracy: 0.9393 - val_loss: 0.1099 - val_accuracy: 0.9578\n",
      "Epoch 44/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.1344 - accuracy: 0.9465 - val_loss: 0.1284 - val_accuracy: 0.9507\n",
      "Epoch 45/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.1247 - accuracy: 0.9511 - val_loss: 0.1026 - val_accuracy: 0.9608\n",
      "Epoch 46/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.1188 - accuracy: 0.9538 - val_loss: 0.0926 - val_accuracy: 0.9642\n",
      "Epoch 47/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.1116 - accuracy: 0.9559 - val_loss: 0.1092 - val_accuracy: 0.9571\n",
      "Epoch 48/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.1015 - accuracy: 0.9618 - val_loss: 0.1001 - val_accuracy: 0.9595\n",
      "Epoch 49/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.1057 - accuracy: 0.9587 - val_loss: 0.0733 - val_accuracy: 0.9744\n",
      "Epoch 50/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0892 - accuracy: 0.9657 - val_loss: 0.0854 - val_accuracy: 0.9669\n",
      "Epoch 51/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0922 - accuracy: 0.9651 - val_loss: 0.0626 - val_accuracy: 0.9771\n",
      "Epoch 52/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0786 - accuracy: 0.9704 - val_loss: 0.0714 - val_accuracy: 0.9732\n",
      "Epoch 53/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0816 - accuracy: 0.9694 - val_loss: 0.0465 - val_accuracy: 0.9836\n",
      "Epoch 54/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0750 - accuracy: 0.9713 - val_loss: 0.0426 - val_accuracy: 0.9859\n",
      "Epoch 55/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 0.0696 - accuracy: 0.9734 - val_loss: 0.0557 - val_accuracy: 0.9789\n",
      "Epoch 56/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0673 - accuracy: 0.9747 - val_loss: 0.0515 - val_accuracy: 0.9821\n",
      "Epoch 57/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0698 - accuracy: 0.9744 - val_loss: 0.0482 - val_accuracy: 0.9829\n",
      "Epoch 58/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0644 - accuracy: 0.9767 - val_loss: 0.0418 - val_accuracy: 0.9852\n",
      "Epoch 59/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0586 - accuracy: 0.9783 - val_loss: 0.0544 - val_accuracy: 0.9794\n",
      "Epoch 60/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0580 - accuracy: 0.9780 - val_loss: 0.0370 - val_accuracy: 0.9860\n",
      "Epoch 61/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0460 - accuracy: 0.9834 - val_loss: 0.0623 - val_accuracy: 0.9770\n",
      "Epoch 62/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0599 - accuracy: 0.9780 - val_loss: 0.0410 - val_accuracy: 0.9846\n",
      "Epoch 63/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0533 - accuracy: 0.9810 - val_loss: 0.0319 - val_accuracy: 0.9887\n",
      "Epoch 64/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0366 - accuracy: 0.9871 - val_loss: 0.0551 - val_accuracy: 0.9799\n",
      "Epoch 65/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0580 - accuracy: 0.9785 - val_loss: 0.0435 - val_accuracy: 0.9836\n",
      "Epoch 66/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 0.0478 - accuracy: 0.9830 - val_loss: 0.0462 - val_accuracy: 0.9830\n",
      "Epoch 67/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0410 - accuracy: 0.9847 - val_loss: 0.0230 - val_accuracy: 0.9913\n",
      "Epoch 68/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0359 - accuracy: 0.9871 - val_loss: 0.0569 - val_accuracy: 0.9811\n",
      "Epoch 69/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 0.0477 - accuracy: 0.9825 - val_loss: 0.0342 - val_accuracy: 0.9882\n",
      "Epoch 70/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0376 - accuracy: 0.9867 - val_loss: 0.0391 - val_accuracy: 0.9854\n",
      "Epoch 71/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 0.0419 - accuracy: 0.9853 - val_loss: 0.0505 - val_accuracy: 0.9825\n",
      "Epoch 72/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0448 - accuracy: 0.9844 - val_loss: 0.0584 - val_accuracy: 0.9797\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 73/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0169 - accuracy: 0.9940 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "Epoch 74/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0022 - val_accuracy: 0.9999\n",
      "Epoch 75/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9999\n",
      "Epoch 77/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 8.9497e-04 - val_accuracy: 0.9999\n",
      "Epoch 78/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 8.3654e-04 - accuracy: 1.0000 - val_loss: 6.9913e-04 - val_accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 7.8770e-04 - accuracy: 1.0000 - val_loss: 6.1369e-04 - val_accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 6.7173e-04 - accuracy: 1.0000 - val_loss: 5.8059e-04 - val_accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 6.1269e-04 - accuracy: 1.0000 - val_loss: 5.1154e-04 - val_accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 5.6231e-04 - accuracy: 1.0000 - val_loss: 4.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 4.5803e-04 - accuracy: 1.0000 - val_loss: 4.3986e-04 - val_accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 4.9150e-04 - accuracy: 1.0000 - val_loss: 3.9100e-04 - val_accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 4.2586e-04 - accuracy: 1.0000 - val_loss: 3.8383e-04 - val_accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 4.2236e-04 - accuracy: 1.0000 - val_loss: 3.5027e-04 - val_accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 4.0266e-04 - accuracy: 1.0000 - val_loss: 3.4588e-04 - val_accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 3.8477e-04 - accuracy: 1.0000 - val_loss: 4.4603e-04 - val_accuracy: 0.9999\n",
      "Epoch 89/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 3.4345e-04 - accuracy: 1.0000 - val_loss: 4.0355e-04 - val_accuracy: 0.9999\n",
      "Epoch 90/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 3.4305e-04 - accuracy: 1.0000 - val_loss: 2.6396e-04 - val_accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 3.3936e-04 - accuracy: 1.0000 - val_loss: 2.6168e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 92/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.9521e-04 - accuracy: 1.0000 - val_loss: 2.5671e-04 - val_accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.8977e-04 - accuracy: 1.0000 - val_loss: 2.6386e-04 - val_accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.8431e-04 - accuracy: 1.0000 - val_loss: 2.4857e-04 - val_accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.8072e-04 - accuracy: 1.0000 - val_loss: 3.4516e-04 - val_accuracy: 0.9999\n",
      "Epoch 96/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7904e-04 - accuracy: 1.0000 - val_loss: 2.8642e-04 - val_accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7723e-04 - accuracy: 1.0000 - val_loss: 3.1168e-04 - val_accuracy: 0.9999\n",
      "Epoch 98/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.5453e-04 - accuracy: 1.0000 - val_loss: 2.4789e-04 - val_accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7257e-04 - accuracy: 1.0000 - val_loss: 3.0846e-04 - val_accuracy: 0.9999\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 100/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7012e-04 - accuracy: 1.0000 - val_loss: 2.8186e-04 - val_accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7057e-04 - accuracy: 1.0000 - val_loss: 2.4511e-04 - val_accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7276e-04 - accuracy: 1.0000 - val_loss: 2.4048e-04 - val_accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6837e-04 - accuracy: 1.0000 - val_loss: 2.4315e-04 - val_accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.5532e-04 - accuracy: 1.0000 - val_loss: 2.5829e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 105/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6903e-04 - accuracy: 1.0000 - val_loss: 2.5548e-04 - val_accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.5981e-04 - accuracy: 1.0000 - val_loss: 2.4075e-04 - val_accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6094e-04 - accuracy: 1.0000 - val_loss: 2.8377e-04 - val_accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6386e-04 - accuracy: 1.0000 - val_loss: 2.3797e-04 - val_accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7023e-04 - accuracy: 1.0000 - val_loss: 2.3828e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "Epoch 110/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6889e-04 - accuracy: 1.0000 - val_loss: 2.6167e-04 - val_accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7047e-04 - accuracy: 1.0000 - val_loss: 2.4126e-04 - val_accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 2.6939e-04 - accuracy: 1.0000 - val_loss: 3.1868e-04 - val_accuracy: 0.9999\n",
      "Epoch 113/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6946e-04 - accuracy: 1.0000 - val_loss: 2.8488e-04 - val_accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.5103e-04 - accuracy: 1.0000 - val_loss: 2.4590e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "Epoch 115/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7138e-04 - accuracy: 1.0000 - val_loss: 2.5060e-04 - val_accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6723e-04 - accuracy: 1.0000 - val_loss: 2.7554e-04 - val_accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7016e-04 - accuracy: 1.0000 - val_loss: 3.2322e-04 - val_accuracy: 0.9999\n",
      "Epoch 118/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6773e-04 - accuracy: 1.0000 - val_loss: 2.4472e-04 - val_accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6509e-04 - accuracy: 1.0000 - val_loss: 2.5240e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "Epoch 120/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6688e-04 - accuracy: 1.0000 - val_loss: 2.7122e-04 - val_accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6966e-04 - accuracy: 1.0000 - val_loss: 2.7270e-04 - val_accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6966e-04 - accuracy: 1.0000 - val_loss: 2.7758e-04 - val_accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6204e-04 - accuracy: 1.0000 - val_loss: 2.8429e-04 - val_accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7094e-04 - accuracy: 1.0000 - val_loss: 2.4978e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "Epoch 125/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6924e-04 - accuracy: 1.0000 - val_loss: 2.7088e-04 - val_accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.5598e-04 - accuracy: 1.0000 - val_loss: 2.4382e-04 - val_accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 2.7005e-04 - accuracy: 1.0000 - val_loss: 2.3252e-04 - val_accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7354e-04 - accuracy: 1.0000 - val_loss: 2.4396e-04 - val_accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6550e-04 - accuracy: 1.0000 - val_loss: 2.6114e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\n",
      "Epoch 130/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.5266e-04 - accuracy: 1.0000 - val_loss: 3.1535e-04 - val_accuracy: 0.9999\n",
      "Epoch 131/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 2.6691e-04 - accuracy: 1.0000 - val_loss: 3.3444e-04 - val_accuracy: 0.9999\n",
      "Epoch 132/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 2.5566e-04 - accuracy: 1.0000 - val_loss: 2.5549e-04 - val_accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7153e-04 - accuracy: 1.0000 - val_loss: 2.5034e-04 - val_accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6849e-04 - accuracy: 1.0000 - val_loss: 2.4828e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00134: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 135/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6999e-04 - accuracy: 1.0000 - val_loss: 2.3677e-04 - val_accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.4759e-04 - accuracy: 1.0000 - val_loss: 2.3923e-04 - val_accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6750e-04 - accuracy: 1.0000 - val_loss: 2.7289e-04 - val_accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7055e-04 - accuracy: 1.0000 - val_loss: 3.1088e-04 - val_accuracy: 0.9999\n",
      "Epoch 139/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 2.6932e-04 - accuracy: 1.0000 - val_loss: 2.5074e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "Epoch 140/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.5108e-04 - accuracy: 1.0000 - val_loss: 3.2281e-04 - val_accuracy: 0.9999\n",
      "Epoch 141/150\n",
      "566/566 [==============================] - 23s 38ms/step - loss: 2.7038e-04 - accuracy: 1.0000 - val_loss: 2.4719e-04 - val_accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6984e-04 - accuracy: 1.0000 - val_loss: 2.5649e-04 - val_accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6483e-04 - accuracy: 1.0000 - val_loss: 3.1018e-04 - val_accuracy: 0.9999\n",
      "Epoch 144/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.5324e-04 - accuracy: 1.0000 - val_loss: 2.4473e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "Epoch 145/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6637e-04 - accuracy: 1.0000 - val_loss: 2.3931e-04 - val_accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6920e-04 - accuracy: 1.0000 - val_loss: 3.1772e-04 - val_accuracy: 0.9999\n",
      "Epoch 147/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.5063e-04 - accuracy: 1.0000 - val_loss: 2.7277e-04 - val_accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.7009e-04 - accuracy: 1.0000 - val_loss: 2.5164e-04 - val_accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6916e-04 - accuracy: 1.0000 - val_loss: 3.1988e-04 - val_accuracy: 0.9999\n",
      "\n",
      "Epoch 00149: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "Epoch 150/150\n",
      "566/566 [==============================] - 24s 38ms/step - loss: 2.6888e-04 - accuracy: 1.0000 - val_loss: 2.4999e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history_adapted_siamese_model_5 = adapted_siamese_model_5.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[model_checkpoint_callback, WandbCallback(), reduce_lr])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 4s 10ms/step - loss: 2.4646e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = adapted_siamese_model_5.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, adapted_siamese_model_5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sixth Run - 150k Gray"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_150k_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_150k_224_224/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 256\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "adapted_model = get_adapted_model(height,width,channels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 113, 113, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 55, 55, 96)        2496      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 25, 25, 256)       221440    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 10, 10, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 6, 6, 256)         884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              1049600   \n",
      "=================================================================\n",
      "Total params: 5,420,736\n",
      "Trainable params: 5,420,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adapted_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "left_input = keras.layers.Input(shape=(height, width, channels))\n",
    "right_input = keras.layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = adapted_model(left_input)\n",
    "encoded_r = adapted_model(right_input)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "merge_layer = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "prediction = keras.layers.Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "\n",
    "adapted_siamese_model_6 = keras.models.Model([left_input, right_input], outputs=prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2pyeptc0\" target=\"_blank\">dutiful-snowball-41</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"momentum\": 0.9,\n",
    "                         \"weight_decay\": 0.0005,\n",
    "                         \"otimizer\": \"SGDW\",\n",
    "                         \"loss_function\": \"binary_crossentropy\",\n",
    "                         \"epochs\": 150,\n",
    "                         \"architecture\": \"Adapted - 150k - Grayscale\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"Architecture_1_Checkpoints/Adapted_150k_Gray\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                                                 patience=5, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "adapted_siamese_model_6.compile(loss=config.loss_function,\n",
    "                                optimizer=keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=config.momentum), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "942/942 [==============================] - 41s 39ms/step - loss: 0.6255 - accuracy: 0.6493 - val_loss: 0.5861 - val_accuracy: 0.6780\n",
      "Epoch 2/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.5772 - accuracy: 0.6891 - val_loss: 0.5594 - val_accuracy: 0.7042\n",
      "Epoch 3/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.5493 - accuracy: 0.7124 - val_loss: 0.5333 - val_accuracy: 0.7240\n",
      "Epoch 4/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.5280 - accuracy: 0.7290 - val_loss: 0.5062 - val_accuracy: 0.7443\n",
      "Epoch 5/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.5123 - accuracy: 0.7389 - val_loss: 0.5190 - val_accuracy: 0.7311\n",
      "Epoch 6/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.5046 - accuracy: 0.7415 - val_loss: 0.5006 - val_accuracy: 0.7481\n",
      "Epoch 7/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.4957 - accuracy: 0.7479 - val_loss: 0.4830 - val_accuracy: 0.7572\n",
      "Epoch 8/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.4864 - accuracy: 0.7533 - val_loss: 0.4710 - val_accuracy: 0.7682\n",
      "Epoch 9/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.4753 - accuracy: 0.7605 - val_loss: 0.4694 - val_accuracy: 0.7662\n",
      "Epoch 10/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.4713 - accuracy: 0.7643 - val_loss: 0.4597 - val_accuracy: 0.7731\n",
      "Epoch 11/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.4604 - accuracy: 0.7723 - val_loss: 0.4498 - val_accuracy: 0.7786\n",
      "Epoch 12/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.4477 - accuracy: 0.7810 - val_loss: 0.4432 - val_accuracy: 0.7833\n",
      "Epoch 13/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.4417 - accuracy: 0.7856 - val_loss: 0.4222 - val_accuracy: 0.7982\n",
      "Epoch 14/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.4343 - accuracy: 0.7915 - val_loss: 0.4185 - val_accuracy: 0.8004\n",
      "Epoch 15/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.4219 - accuracy: 0.7969 - val_loss: 0.4167 - val_accuracy: 0.8006\n",
      "Epoch 16/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.4108 - accuracy: 0.8053 - val_loss: 0.3974 - val_accuracy: 0.8157\n",
      "Epoch 17/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.4013 - accuracy: 0.8089 - val_loss: 0.3691 - val_accuracy: 0.8302\n",
      "Epoch 18/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.3905 - accuracy: 0.8169 - val_loss: 0.3853 - val_accuracy: 0.8199\n",
      "Epoch 19/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.3795 - accuracy: 0.8234 - val_loss: 0.3561 - val_accuracy: 0.8367\n",
      "Epoch 20/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.3654 - accuracy: 0.8317 - val_loss: 0.3473 - val_accuracy: 0.8399\n",
      "Epoch 21/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.3582 - accuracy: 0.8352 - val_loss: 0.3298 - val_accuracy: 0.8523\n",
      "Epoch 22/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.3474 - accuracy: 0.8419 - val_loss: 0.3110 - val_accuracy: 0.8638\n",
      "Epoch 23/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.3315 - accuracy: 0.8504 - val_loss: 0.3076 - val_accuracy: 0.8612\n",
      "Epoch 24/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.3188 - accuracy: 0.8583 - val_loss: 0.2880 - val_accuracy: 0.8743\n",
      "Epoch 25/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.3062 - accuracy: 0.8636 - val_loss: 0.2798 - val_accuracy: 0.8790\n",
      "Epoch 26/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.2949 - accuracy: 0.8691 - val_loss: 0.2475 - val_accuracy: 0.8943\n",
      "Epoch 27/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.2768 - accuracy: 0.8788 - val_loss: 0.2378 - val_accuracy: 0.9008\n",
      "Epoch 28/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.2623 - accuracy: 0.8865 - val_loss: 0.2165 - val_accuracy: 0.9111\n",
      "Epoch 29/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.2507 - accuracy: 0.8916 - val_loss: 0.2112 - val_accuracy: 0.9146\n",
      "Epoch 30/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.2343 - accuracy: 0.8994 - val_loss: 0.2093 - val_accuracy: 0.9146\n",
      "Epoch 31/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.2166 - accuracy: 0.9087 - val_loss: 0.2013 - val_accuracy: 0.9184\n",
      "Epoch 32/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.2041 - accuracy: 0.9147 - val_loss: 0.1738 - val_accuracy: 0.9289\n",
      "Epoch 33/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.1915 - accuracy: 0.9203 - val_loss: 0.1629 - val_accuracy: 0.9357\n",
      "Epoch 34/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.1791 - accuracy: 0.9252 - val_loss: 0.1398 - val_accuracy: 0.9451\n",
      "Epoch 35/150\n",
      "942/942 [==============================] - 40s 39ms/step - loss: 0.1642 - accuracy: 0.9314 - val_loss: 0.1407 - val_accuracy: 0.9446\n",
      "Epoch 36/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.1521 - accuracy: 0.9375 - val_loss: 0.1230 - val_accuracy: 0.9507\n",
      "Epoch 37/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.1421 - accuracy: 0.9426 - val_loss: 0.1132 - val_accuracy: 0.9545\n",
      "Epoch 38/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.1323 - accuracy: 0.9468 - val_loss: 0.1124 - val_accuracy: 0.9551\n",
      "Epoch 39/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.1236 - accuracy: 0.9508 - val_loss: 0.1112 - val_accuracy: 0.9575\n",
      "Epoch 40/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.1168 - accuracy: 0.9536 - val_loss: 0.1256 - val_accuracy: 0.9504\n",
      "Epoch 41/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.1021 - accuracy: 0.9607 - val_loss: 0.0977 - val_accuracy: 0.9622\n",
      "Epoch 42/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0966 - accuracy: 0.9620 - val_loss: 0.1322 - val_accuracy: 0.9492\n",
      "Epoch 43/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0947 - accuracy: 0.9630 - val_loss: 0.0754 - val_accuracy: 0.9715\n",
      "Epoch 44/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0876 - accuracy: 0.9663 - val_loss: 0.0675 - val_accuracy: 0.9760\n",
      "Epoch 45/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.0792 - accuracy: 0.9696 - val_loss: 0.0650 - val_accuracy: 0.9748\n",
      "Epoch 46/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0757 - accuracy: 0.9715 - val_loss: 0.0477 - val_accuracy: 0.9832\n",
      "Epoch 47/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0725 - accuracy: 0.9726 - val_loss: 0.0662 - val_accuracy: 0.9744\n",
      "Epoch 48/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.0682 - accuracy: 0.9739 - val_loss: 0.0679 - val_accuracy: 0.9745\n",
      "Epoch 49/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.0651 - accuracy: 0.9756 - val_loss: 0.0472 - val_accuracy: 0.9829\n",
      "Epoch 50/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.0585 - accuracy: 0.9779 - val_loss: 0.0668 - val_accuracy: 0.9749\n",
      "Epoch 51/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0601 - accuracy: 0.9775 - val_loss: 0.0561 - val_accuracy: 0.9785\n",
      "Epoch 52/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0557 - accuracy: 0.9795 - val_loss: 0.0623 - val_accuracy: 0.9778\n",
      "Epoch 53/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0540 - accuracy: 0.9799 - val_loss: 0.0434 - val_accuracy: 0.9842\n",
      "Epoch 54/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.0489 - accuracy: 0.9820 - val_loss: 0.0473 - val_accuracy: 0.9821\n",
      "Epoch 55/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0503 - accuracy: 0.9817 - val_loss: 0.0348 - val_accuracy: 0.9871\n",
      "Epoch 56/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0481 - accuracy: 0.9820 - val_loss: 0.0355 - val_accuracy: 0.9860\n",
      "Epoch 57/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0468 - accuracy: 0.9823 - val_loss: 0.0475 - val_accuracy: 0.9822\n",
      "Epoch 58/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0453 - accuracy: 0.9833 - val_loss: 0.0484 - val_accuracy: 0.9829\n",
      "Epoch 59/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0431 - accuracy: 0.9841 - val_loss: 0.0336 - val_accuracy: 0.9882\n",
      "Epoch 60/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0432 - accuracy: 0.9841 - val_loss: 0.0377 - val_accuracy: 0.9861\n",
      "Epoch 61/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 0.0394 - accuracy: 0.9859 - val_loss: 0.0202 - val_accuracy: 0.9934\n",
      "Epoch 62/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0325 - accuracy: 0.9882 - val_loss: 0.0373 - val_accuracy: 0.9865\n",
      "Epoch 63/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0370 - accuracy: 0.9864 - val_loss: 0.0506 - val_accuracy: 0.9820\n",
      "Epoch 64/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0362 - accuracy: 0.9868 - val_loss: 0.0349 - val_accuracy: 0.9872\n",
      "Epoch 65/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0355 - accuracy: 0.9870 - val_loss: 0.0273 - val_accuracy: 0.9903\n",
      "Epoch 66/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0319 - accuracy: 0.9881 - val_loss: 0.0221 - val_accuracy: 0.9929\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 67/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0111 - accuracy: 0.9963 - val_loss: 0.0026 - val_accuracy: 0.9997\n",
      "Epoch 68/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.0016 - val_accuracy: 0.9999\n",
      "Epoch 69/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 9.9786e-04 - val_accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 9.3154e-04 - accuracy: 1.0000 - val_loss: 7.6664e-04 - val_accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 7.5531e-04 - accuracy: 1.0000 - val_loss: 6.6755e-04 - val_accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 6.2830e-04 - accuracy: 1.0000 - val_loss: 5.5012e-04 - val_accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 5.3449e-04 - accuracy: 1.0000 - val_loss: 4.9419e-04 - val_accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 4.7971e-04 - accuracy: 1.0000 - val_loss: 4.4214e-04 - val_accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 4.2891e-04 - accuracy: 1.0000 - val_loss: 3.9707e-04 - val_accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 3.8529e-04 - accuracy: 1.0000 - val_loss: 3.4758e-04 - val_accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 3.4520e-04 - accuracy: 1.0000 - val_loss: 3.3364e-04 - val_accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 3.2232e-04 - accuracy: 1.0000 - val_loss: 2.9564e-04 - val_accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.9807e-04 - accuracy: 1.0000 - val_loss: 2.7313e-04 - val_accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.7403e-04 - accuracy: 1.0000 - val_loss: 2.5346e-04 - val_accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.5882e-04 - accuracy: 1.0000 - val_loss: 2.3930e-04 - val_accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.4270e-04 - accuracy: 1.0000 - val_loss: 2.3958e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 83/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.3088e-04 - accuracy: 1.0000 - val_loss: 2.2997e-04 - val_accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2864e-04 - accuracy: 1.0000 - val_loss: 2.2543e-04 - val_accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2805e-04 - accuracy: 1.0000 - val_loss: 2.2502e-04 - val_accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2411e-04 - accuracy: 1.0000 - val_loss: 2.2485e-04 - val_accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2564e-04 - accuracy: 1.0000 - val_loss: 2.3299e-04 - val_accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2389e-04 - accuracy: 1.0000 - val_loss: 2.2191e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 89/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2243e-04 - accuracy: 1.0000 - val_loss: 2.0780e-04 - val_accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.1953e-04 - accuracy: 1.0000 - val_loss: 2.2781e-04 - val_accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2407e-04 - accuracy: 1.0000 - val_loss: 2.3035e-04 - val_accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2331e-04 - accuracy: 1.0000 - val_loss: 2.1503e-04 - val_accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2192e-04 - accuracy: 1.0000 - val_loss: 2.2625e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 94/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2504e-04 - accuracy: 1.0000 - val_loss: 2.1729e-04 - val_accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2280e-04 - accuracy: 1.0000 - val_loss: 2.3245e-04 - val_accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2243e-04 - accuracy: 1.0000 - val_loss: 2.3082e-04 - val_accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2470e-04 - accuracy: 1.0000 - val_loss: 2.1740e-04 - val_accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2468e-04 - accuracy: 1.0000 - val_loss: 2.1669e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "Epoch 99/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.1916e-04 - accuracy: 1.0000 - val_loss: 2.2967e-04 - val_accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2176e-04 - accuracy: 1.0000 - val_loss: 2.1533e-04 - val_accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2163e-04 - accuracy: 1.0000 - val_loss: 2.2088e-04 - val_accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2381e-04 - accuracy: 1.0000 - val_loss: 2.2123e-04 - val_accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2200e-04 - accuracy: 1.0000 - val_loss: 2.2362e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "Epoch 104/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2416e-04 - accuracy: 1.0000 - val_loss: 2.2821e-04 - val_accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2084e-04 - accuracy: 1.0000 - val_loss: 2.2161e-04 - val_accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2138e-04 - accuracy: 1.0000 - val_loss: 2.2419e-04 - val_accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2352e-04 - accuracy: 1.0000 - val_loss: 2.2713e-04 - val_accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2255e-04 - accuracy: 1.0000 - val_loss: 2.2158e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "Epoch 109/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2592e-04 - accuracy: 1.0000 - val_loss: 2.1945e-04 - val_accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2428e-04 - accuracy: 1.0000 - val_loss: 2.2531e-04 - val_accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2081e-04 - accuracy: 1.0000 - val_loss: 2.2867e-04 - val_accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2161e-04 - accuracy: 1.0000 - val_loss: 2.2328e-04 - val_accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2226e-04 - accuracy: 1.0000 - val_loss: 2.1801e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "Epoch 114/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2318e-04 - accuracy: 1.0000 - val_loss: 2.1117e-04 - val_accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2357e-04 - accuracy: 1.0000 - val_loss: 2.2880e-04 - val_accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2328e-04 - accuracy: 1.0000 - val_loss: 2.1555e-04 - val_accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2307e-04 - accuracy: 1.0000 - val_loss: 2.1982e-04 - val_accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2176e-04 - accuracy: 1.0000 - val_loss: 2.2153e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\n",
      "Epoch 119/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2614e-04 - accuracy: 1.0000 - val_loss: 2.2330e-04 - val_accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2180e-04 - accuracy: 1.0000 - val_loss: 2.1254e-04 - val_accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2465e-04 - accuracy: 1.0000 - val_loss: 2.3186e-04 - val_accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2250e-04 - accuracy: 1.0000 - val_loss: 2.1513e-04 - val_accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2221e-04 - accuracy: 1.0000 - val_loss: 2.2378e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 124/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2089e-04 - accuracy: 1.0000 - val_loss: 2.2018e-04 - val_accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2102e-04 - accuracy: 1.0000 - val_loss: 2.2925e-04 - val_accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.1992e-04 - accuracy: 1.0000 - val_loss: 2.3568e-04 - val_accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2286e-04 - accuracy: 1.0000 - val_loss: 2.3199e-04 - val_accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2232e-04 - accuracy: 1.0000 - val_loss: 2.2307e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "Epoch 129/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2362e-04 - accuracy: 1.0000 - val_loss: 2.2935e-04 - val_accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2197e-04 - accuracy: 1.0000 - val_loss: 2.1719e-04 - val_accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2323e-04 - accuracy: 1.0000 - val_loss: 2.2470e-04 - val_accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2279e-04 - accuracy: 1.0000 - val_loss: 2.2571e-04 - val_accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2265e-04 - accuracy: 1.0000 - val_loss: 2.2354e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "Epoch 134/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2639e-04 - accuracy: 1.0000 - val_loss: 2.1074e-04 - val_accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2291e-04 - accuracy: 1.0000 - val_loss: 2.2474e-04 - val_accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2297e-04 - accuracy: 1.0000 - val_loss: 2.1602e-04 - val_accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2295e-04 - accuracy: 1.0000 - val_loss: 2.1773e-04 - val_accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2090e-04 - accuracy: 1.0000 - val_loss: 2.2290e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "Epoch 139/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2088e-04 - accuracy: 1.0000 - val_loss: 2.3855e-04 - val_accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2467e-04 - accuracy: 1.0000 - val_loss: 2.2245e-04 - val_accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2273e-04 - accuracy: 1.0000 - val_loss: 2.3580e-04 - val_accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2356e-04 - accuracy: 1.0000 - val_loss: 2.2296e-04 - val_accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2168e-04 - accuracy: 1.0000 - val_loss: 2.2006e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
      "Epoch 144/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2259e-04 - accuracy: 1.0000 - val_loss: 2.2791e-04 - val_accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2133e-04 - accuracy: 1.0000 - val_loss: 2.1867e-04 - val_accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "942/942 [==============================] - 39s 38ms/step - loss: 2.2155e-04 - accuracy: 1.0000 - val_loss: 2.1903e-04 - val_accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2310e-04 - accuracy: 1.0000 - val_loss: 2.2614e-04 - val_accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2154e-04 - accuracy: 1.0000 - val_loss: 2.2320e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00148: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
      "Epoch 149/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2224e-04 - accuracy: 1.0000 - val_loss: 2.1627e-04 - val_accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "942/942 [==============================] - 40s 38ms/step - loss: 2.2655e-04 - accuracy: 1.0000 - val_loss: 2.2016e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history_adapted_siamese_model_6 = adapted_siamese_model_6.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[model_checkpoint_callback, WandbCallback(), reduce_lr])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 6s 11ms/step - loss: 2.0901e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = adapted_siamese_model_6.evaluate(val_dataset)\n",
    "wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/test_pairs_224_224/anchor\"\n",
    "positive_images_path = \"npz_datasets/test_pairs_224_224/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_path, positive_images_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, adapted_siamese_model_6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
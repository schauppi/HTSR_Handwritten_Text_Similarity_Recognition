{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from helper_functions import get_classification_report\n",
    "from helper_functions import create_tf_data_datasets_contrastive\n",
    "from helper_functions import create_tf_data_testset_contrastive\n",
    "from helper_functions import euclidean_distance\n",
    "from helper_functions import manhattan_distance\n",
    "from helper_functions import contrastive_loss\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class ConvNeXt_Block(layers.Layer):\n",
    "    r\"\"\" ConvNeXt Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = layers.DepthwiseConv2D(kernel_size=7, padding='same')  # depthwise conv\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.pwconv1 = layers.Dense(4 * dim)\n",
    "        self.act = layers.Activation('gelu')\n",
    "        self.pwconv2 = layers.Dense(dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.dim = dim\n",
    "        self.layer_scale_init_value = layer_scale_init_value\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            initial_value=self.layer_scale_init_value * tf.ones((self.dim)),\n",
    "            trainable=True,\n",
    "            name='_gamma')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            #x = tf.cast(x, dtype=tf.float32)\n",
    "            x = self.gamma * x\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "\n",
    "        #x = tf.cast(x, dtype=tf.float16)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Downsample_Block(layers.Layer):\n",
    "    \"\"\"The Downsample Block in ConvNeXt\n",
    "        Args:\n",
    "            dim (int): number of channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.LN = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.conv = layers.Conv2D(dim, kernel_size=2, strides=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.LN(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class DropPath(tf.keras.layers.Layer):\n",
    "    \"\"\"The Drop path in ConvNeXt\n",
    "        Reference:\n",
    "            https://github.com/rishigami/Swin-Transformer-TF/blob/main/swintransformer/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        return self._drop_path(x, self.drop_prob, training)\n",
    "\n",
    "\n",
    "    def _drop_path(self, inputs, drop_prob, is_training):\n",
    "        if (not is_training) or (drop_prob == 0.):\n",
    "            return inputs\n",
    "\n",
    "        # Compute keep_prob\n",
    "        keep_prob = 1.0 - drop_prob\n",
    "\n",
    "        # Compute drop_connect tensor\n",
    "        random_tensor = keep_prob\n",
    "        shape = (tf.shape(inputs)[0],) + (1,) * (len(tf.shape(inputs)) - 1)\n",
    "        random_tensor += tf.random.uniform(shape, dtype=inputs.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_convnext_model(input_shape=(224, 224, 3), depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], emb_dim=1024, drop_path=0., layer_scale_init_value=1e-6):\n",
    "    \"\"\" Function to construct the ConvNeXt Model\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): (Width, Height , Channels)\n",
    "            depths (list): a list of size 4. denoting each stage's depth\n",
    "            dims (list): a list of size 4. denoting number of kernel's in each stage\n",
    "            num_classes (int): the number of classes\n",
    "            drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "            layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        Returns:\n",
    "            ConvNeXt model: an instance of tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    assert (len(depths) == 4 and len(dims) ==4), \"Must provide exactly 4 depths and 4 dims\"\n",
    "    assert (len(input_shape) == 3), \"Input shape must be (W, H, C)\"\n",
    "\n",
    "    input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Stem + res2\n",
    "    y = layers.Conv2D(dims[0], kernel_size=4, strides=4)(input)\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    for i in range(depths[0]):\n",
    "        y = ConvNeXt_Block(dims[0], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res3\n",
    "    y = Downsample_Block(dims[1])(y)\n",
    "    for i in range(depths[1]):\n",
    "        y = ConvNeXt_Block(dims[1], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res4\n",
    "    y = Downsample_Block(dims[2])(y)\n",
    "    for i in range(depths[2]):\n",
    "        y = ConvNeXt_Block(dims[2], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res5\n",
    "    y = Downsample_Block(dims[3])(y)\n",
    "    for i in range(depths[3]):\n",
    "        y = ConvNeXt_Block(dims[3], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    y = layers.GlobalAveragePooling2D()(y)\n",
    "    # final norm layer\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    # Head\n",
    "    y = layers.Dense(emb_dim, activation='relu')(y)\n",
    "\n",
    "    return tf.keras.Model(inputs=input, outputs=y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Vect 1024 -  Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3qaksu40\" target=\"_blank\">eternal-jazz-159</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 469s 234ms/step - loss: 0.1733 - accuracy: 0.7485 - val_loss: 0.1341 - val_accuracy: 0.8270\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 415s 231ms/step - loss: 0.1242 - accuracy: 0.8370 - val_loss: 0.1133 - val_accuracy: 0.8506\nEpoch 3/25\n1770/1770 [==============================] - 412s 230ms/step - loss: 0.1022 - accuracy: 0.8683 - val_loss: 0.0913 - val_accuracy: 0.8825\nEpoch 4/25\n1770/1770 [==============================] - 411s 229ms/step - loss: 0.0900 - accuracy: 0.8829 - val_loss: 0.0843 - val_accuracy: 0.8910\nEpoch 5/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0822 - accuracy: 0.8935 - val_loss: 0.0809 - val_accuracy: 0.8947\nEpoch 6/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0766 - accuracy: 0.9012 - val_loss: 0.0809 - val_accuracy: 0.8913\nEpoch 7/25\n1770/1770 [==============================] - 409s 228ms/step - loss: 0.0714 - accuracy: 0.9084 - val_loss: 0.0697 - val_accuracy: 0.9112\nEpoch 8/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0666 - accuracy: 0.9147 - val_loss: 0.0667 - val_accuracy: 0.9143\nEpoch 9/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0619 - accuracy: 0.9209 - val_loss: 0.0699 - val_accuracy: 0.9101\nEpoch 10/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0574 - accuracy: 0.9278 - val_loss: 0.0578 - val_accuracy: 0.9267\nEpoch 11/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0530 - accuracy: 0.9334 - val_loss: 0.0617 - val_accuracy: 0.9204\nEpoch 12/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0493 - accuracy: 0.9387 - val_loss: 0.0535 - val_accuracy: 0.9319\nEpoch 13/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0454 - accuracy: 0.9439 - val_loss: 0.0476 - val_accuracy: 0.9402\nEpoch 14/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0422 - accuracy: 0.9480 - val_loss: 0.0515 - val_accuracy: 0.9349\nEpoch 15/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0391 - accuracy: 0.9525 - val_loss: 0.0435 - val_accuracy: 0.9463\nEpoch 16/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0360 - accuracy: 0.9568 - val_loss: 0.0463 - val_accuracy: 0.9423\nEpoch 17/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0336 - accuracy: 0.9601 - val_loss: 0.0435 - val_accuracy: 0.9467\nEpoch 18/25\n1770/1770 [==============================] - 408s 229ms/step - loss: 0.0313 - accuracy: 0.9631 - val_loss: 0.0385 - val_accuracy: 0.9534\nEpoch 19/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0296 - accuracy: 0.9655 - val_loss: 0.0387 - val_accuracy: 0.9530\nEpoch 20/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0277 - accuracy: 0.9677 - val_loss: 0.0399 - val_accuracy: 0.9509\nEpoch 21/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0256 - accuracy: 0.9707 - val_loss: 0.0410 - val_accuracy: 0.9494\nEpoch 22/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0247 - accuracy: 0.9715 - val_loss: 0.0403 - val_accuracy: 0.9505\nEpoch 23/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0230 - accuracy: 0.9740 - val_loss: 0.0388 - val_accuracy: 0.9526\nEpoch 24/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0150 - accuracy: 0.9844 - val_loss: 0.0325 - val_accuracy: 0.9614\nEpoch 25/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0132 - accuracy: 0.9867 - val_loss: 0.0325 - val_accuracy: 0.9618\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows Gray 1000 - Vect 1024 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2ggsgp9k\" target=\"_blank\">hopeful-paper-160</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 409s 221ms/step - loss: 0.2089 - accuracy: 0.6588 - val_loss: 0.1921 - val_accuracy: 0.6997\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1854 - accuracy: 0.7159 - val_loss: 0.1692 - val_accuracy: 0.7565\nEpoch 3/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1622 - accuracy: 0.7629 - val_loss: 0.1552 - val_accuracy: 0.7739\nEpoch 4/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.1477 - accuracy: 0.7865 - val_loss: 0.1468 - val_accuracy: 0.7867\nEpoch 5/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1371 - accuracy: 0.8044 - val_loss: 0.1338 - val_accuracy: 0.8105\nEpoch 6/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.1266 - accuracy: 0.8222 - val_loss: 0.1268 - val_accuracy: 0.8224\nEpoch 7/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1158 - accuracy: 0.8398 - val_loss: 0.1146 - val_accuracy: 0.8404\nEpoch 8/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1054 - accuracy: 0.8554 - val_loss: 0.1069 - val_accuracy: 0.8533\nEpoch 9/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0952 - accuracy: 0.8722 - val_loss: 0.0952 - val_accuracy: 0.8713\nEpoch 10/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.0857 - accuracy: 0.8864 - val_loss: 0.0893 - val_accuracy: 0.8787\nEpoch 11/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0773 - accuracy: 0.8984 - val_loss: 0.0867 - val_accuracy: 0.8830\nEpoch 12/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0696 - accuracy: 0.9092 - val_loss: 0.0759 - val_accuracy: 0.8991\nEpoch 13/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0625 - accuracy: 0.9196 - val_loss: 0.0745 - val_accuracy: 0.9016\nEpoch 14/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0551 - accuracy: 0.9310 - val_loss: 0.0675 - val_accuracy: 0.9113\nEpoch 15/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0489 - accuracy: 0.9397 - val_loss: 0.0638 - val_accuracy: 0.9174\nEpoch 16/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0430 - accuracy: 0.9479 - val_loss: 0.0667 - val_accuracy: 0.9124\nEpoch 17/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0380 - accuracy: 0.9548 - val_loss: 0.0612 - val_accuracy: 0.9199\nEpoch 18/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0331 - accuracy: 0.9617 - val_loss: 0.0695 - val_accuracy: 0.9082\nEpoch 19/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0291 - accuracy: 0.9667 - val_loss: 0.0658 - val_accuracy: 0.9154\nEpoch 20/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0259 - accuracy: 0.9707 - val_loss: 0.0600 - val_accuracy: 0.9227\nEpoch 21/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0225 - accuracy: 0.9753 - val_loss: 0.0655 - val_accuracy: 0.9140\nEpoch 22/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0208 - accuracy: 0.9772 - val_loss: 0.0604 - val_accuracy: 0.9226\nEpoch 23/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.0191 - accuracy: 0.9791 - val_loss: 0.0635 - val_accuracy: 0.9189\nEpoch 24/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0177 - accuracy: 0.9808 - val_loss: 0.0627 - val_accuracy: 0.9207\nEpoch 25/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0168 - accuracy: 0.9817 - val_loss: 0.0644 - val_accuracy: 0.9177\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Vect 2048 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/170lfkni\" target=\"_blank\">fearless-butterfly-170</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 456s 236ms/step - loss: 0.1751 - accuracy: 0.7466 - val_loss: 0.1419 - val_accuracy: 0.8179\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 420s 234ms/step - loss: 0.1248 - accuracy: 0.8380 - val_loss: 0.1084 - val_accuracy: 0.8626\nEpoch 3/25\n1770/1770 [==============================] - 418s 233ms/step - loss: 0.1002 - accuracy: 0.8713 - val_loss: 0.0908 - val_accuracy: 0.8839\nEpoch 4/25\n1770/1770 [==============================] - 417s 233ms/step - loss: 0.0890 - accuracy: 0.8847 - val_loss: 0.0849 - val_accuracy: 0.8924\nEpoch 5/25\n1770/1770 [==============================] - 415s 232ms/step - loss: 0.0822 - accuracy: 0.8931 - val_loss: 0.0769 - val_accuracy: 0.9021\nEpoch 6/25\n1770/1770 [==============================] - 414s 231ms/step - loss: 0.0760 - accuracy: 0.9018 - val_loss: 0.0738 - val_accuracy: 0.9065\nEpoch 7/25\n1770/1770 [==============================] - 414s 231ms/step - loss: 0.0713 - accuracy: 0.9080 - val_loss: 0.0703 - val_accuracy: 0.9090\nEpoch 8/25\n1770/1770 [==============================] - 414s 231ms/step - loss: 0.0666 - accuracy: 0.9144 - val_loss: 0.0656 - val_accuracy: 0.9173\nEpoch 9/25\n1770/1770 [==============================] - 414s 231ms/step - loss: 0.0621 - accuracy: 0.9208 - val_loss: 0.0651 - val_accuracy: 0.9163\nEpoch 10/25\n1770/1770 [==============================] - 414s 231ms/step - loss: 0.0582 - accuracy: 0.9264 - val_loss: 0.0572 - val_accuracy: 0.9281\nEpoch 11/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0545 - accuracy: 0.9311 - val_loss: 0.0554 - val_accuracy: 0.9298\nEpoch 12/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0512 - accuracy: 0.9361 - val_loss: 0.0524 - val_accuracy: 0.9344\nEpoch 13/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0485 - accuracy: 0.9398 - val_loss: 0.0528 - val_accuracy: 0.9340\nEpoch 14/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0450 - accuracy: 0.9448 - val_loss: 0.0521 - val_accuracy: 0.9343\nEpoch 15/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0427 - accuracy: 0.9474 - val_loss: 0.0497 - val_accuracy: 0.9372\nEpoch 16/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0399 - accuracy: 0.9514 - val_loss: 0.0460 - val_accuracy: 0.9426\nEpoch 17/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0378 - accuracy: 0.9543 - val_loss: 0.0457 - val_accuracy: 0.9431\nEpoch 18/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0356 - accuracy: 0.9570 - val_loss: 0.0449 - val_accuracy: 0.9438\nEpoch 19/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0338 - accuracy: 0.9592 - val_loss: 0.0473 - val_accuracy: 0.9399\nEpoch 20/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0318 - accuracy: 0.9622 - val_loss: 0.0417 - val_accuracy: 0.9487\nEpoch 21/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0299 - accuracy: 0.9645 - val_loss: 0.0423 - val_accuracy: 0.9480\nEpoch 22/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0281 - accuracy: 0.9669 - val_loss: 0.0418 - val_accuracy: 0.9477\nEpoch 23/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0272 - accuracy: 0.9681 - val_loss: 0.0409 - val_accuracy: 0.9504\nEpoch 24/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0256 - accuracy: 0.9702 - val_loss: 0.0419 - val_accuracy: 0.9486\nEpoch 25/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0244 - accuracy: 0.9718 - val_loss: 0.0401 - val_accuracy: 0.9509\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Vect 512 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/70e8ta6h\" target=\"_blank\">rare-leaf-171</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 512,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 428s 232ms/step - loss: 0.1779 - accuracy: 0.7417 - val_loss: 0.1400 - val_accuracy: 0.8237\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 412s 230ms/step - loss: 0.1296 - accuracy: 0.8275 - val_loss: 0.1158 - val_accuracy: 0.8467\nEpoch 3/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.1087 - accuracy: 0.8578 - val_loss: 0.0960 - val_accuracy: 0.8779\nEpoch 4/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0956 - accuracy: 0.8752 - val_loss: 0.0913 - val_accuracy: 0.8802\nEpoch 5/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0880 - accuracy: 0.8849 - val_loss: 0.0831 - val_accuracy: 0.8922\nEpoch 6/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0823 - accuracy: 0.8929 - val_loss: 0.0793 - val_accuracy: 0.8969\nEpoch 7/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0779 - accuracy: 0.8989 - val_loss: 0.0783 - val_accuracy: 0.8969\nEpoch 8/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0731 - accuracy: 0.9053 - val_loss: 0.0719 - val_accuracy: 0.9074\nEpoch 9/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0697 - accuracy: 0.9098 - val_loss: 0.0657 - val_accuracy: 0.9158\nEpoch 10/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0660 - accuracy: 0.9155 - val_loss: 0.0639 - val_accuracy: 0.9178\nEpoch 11/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0622 - accuracy: 0.9210 - val_loss: 0.0659 - val_accuracy: 0.9150\nEpoch 12/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0594 - accuracy: 0.9247 - val_loss: 0.0598 - val_accuracy: 0.9241\nEpoch 13/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0559 - accuracy: 0.9297 - val_loss: 0.0588 - val_accuracy: 0.9254\nEpoch 14/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0539 - accuracy: 0.9322 - val_loss: 0.0568 - val_accuracy: 0.9274\nEpoch 15/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0511 - accuracy: 0.9367 - val_loss: 0.0536 - val_accuracy: 0.9324\nEpoch 16/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0487 - accuracy: 0.9393 - val_loss: 0.0519 - val_accuracy: 0.9341\nEpoch 17/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0467 - accuracy: 0.9420 - val_loss: 0.0522 - val_accuracy: 0.9338\nEpoch 18/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0444 - accuracy: 0.9451 - val_loss: 0.0502 - val_accuracy: 0.9366\nEpoch 19/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0418 - accuracy: 0.9490 - val_loss: 0.0535 - val_accuracy: 0.9319\nEpoch 20/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0400 - accuracy: 0.9510 - val_loss: 0.0474 - val_accuracy: 0.9400\nEpoch 21/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0379 - accuracy: 0.9539 - val_loss: 0.0482 - val_accuracy: 0.9389\nEpoch 22/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0361 - accuracy: 0.9563 - val_loss: 0.0447 - val_accuracy: 0.9447\nEpoch 23/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0343 - accuracy: 0.9589 - val_loss: 0.0456 - val_accuracy: 0.9429\nEpoch 24/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0328 - accuracy: 0.9608 - val_loss: 0.0450 - val_accuracy: 0.9438\nEpoch 25/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0311 - accuracy: 0.9632 - val_loss: 0.0443 - val_accuracy: 0.9447\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Vect 256 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 256,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 428s 232ms/step - loss: 0.2501 - accuracy: 0.4968 - val_loss: 0.2500 - val_accuracy: 0.5027\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 412s 230ms/step - loss: 0.2500 - accuracy: 0.5012 - val_loss: 0.2500 - val_accuracy: 0.4973\nEpoch 3/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.4999 - val_loss: 0.2500 - val_accuracy: 0.5027\nEpoch 4/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5002 - val_loss: 0.2500 - val_accuracy: 0.5027\nEpoch 5/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.4993 - val_loss: 0.2500 - val_accuracy: 0.5027\nEpoch 6/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5008 - val_loss: 0.2500 - val_accuracy: 0.4973\nEpoch 7/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5007 - val_loss: 0.2500 - val_accuracy: 0.4973\nEpoch 8/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5016 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 9/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5020 - val_loss: 0.2500 - val_accuracy: 0.5008\nEpoch 10/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5035 - val_loss: 0.2500 - val_accuracy: 0.5010\nEpoch 11/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5030 - val_loss: 0.2500 - val_accuracy: 0.4978\nEpoch 12/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5069 - val_loss: 0.2500 - val_accuracy: 0.4973\nEpoch 13/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5078 - val_loss: 0.2500 - val_accuracy: 0.4971\nEpoch 14/25\n1770/1770 [==============================] - 412s 230ms/step - loss: 0.2499 - accuracy: 0.5083 - val_loss: 0.2500 - val_accuracy: 0.4977\nEpoch 15/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5086 - val_loss: 0.2501 - val_accuracy: 0.4969\nEpoch 16/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5083 - val_loss: 0.2501 - val_accuracy: 0.4974\nEpoch 17/25\n1770/1770 [==============================] - 412s 230ms/step - loss: 0.2499 - accuracy: 0.5093 - val_loss: 0.2501 - val_accuracy: 0.4982\nEpoch 18/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5096 - val_loss: 0.2501 - val_accuracy: 0.4987\nEpoch 19/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5094 - val_loss: 0.2501 - val_accuracy: 0.4981\nEpoch 20/25\n1770/1770 [==============================] - 414s 231ms/step - loss: 0.2499 - accuracy: 0.5098 - val_loss: 0.2501 - val_accuracy: 0.4980\nEpoch 21/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5095 - val_loss: 0.2501 - val_accuracy: 0.4987\nEpoch 22/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5102 - val_loss: 0.2501 - val_accuracy: 0.4984\nEpoch 23/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5099 - val_loss: 0.2501 - val_accuracy: 0.4986\nEpoch 24/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5096 - val_loss: 0.2501 - val_accuracy: 0.4989\nEpoch 25/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5097 - val_loss: 0.2501 - val_accuracy: 0.4990\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m wandb uses only the first 10000 datapoints to create the plots.\n"
    }
   ],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Vect 2048 - Big - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/11fqvfs4\" target=\"_blank\">swept-waterfall-177</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [96, 192, 384, 768],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 774s 402ms/step - loss: 0.1769 - accuracy: 0.7403 - val_loss: 0.1422 - val_accuracy: 0.8244\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 715s 400ms/step - loss: 0.1284 - accuracy: 0.8310 - val_loss: 0.1170 - val_accuracy: 0.8478\nEpoch 3/25\n1770/1770 [==============================] - 710s 398ms/step - loss: 0.1117 - accuracy: 0.8519 - val_loss: 0.1093 - val_accuracy: 0.8518\nEpoch 4/25\n1770/1770 [==============================] - 709s 398ms/step - loss: 0.1038 - accuracy: 0.8623 - val_loss: 0.1094 - val_accuracy: 0.8532\nEpoch 5/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0981 - accuracy: 0.8706 - val_loss: 0.0942 - val_accuracy: 0.8757\nEpoch 6/25\n1770/1770 [==============================] - 708s 397ms/step - loss: 0.0941 - accuracy: 0.8756 - val_loss: 0.0944 - val_accuracy: 0.8746\nEpoch 7/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0901 - accuracy: 0.8816 - val_loss: 0.0852 - val_accuracy: 0.8894\nEpoch 8/25\n1770/1770 [==============================] - 707s 397ms/step - loss: 0.0865 - accuracy: 0.8871 - val_loss: 0.0860 - val_accuracy: 0.8869\nEpoch 9/25\n1770/1770 [==============================] - 708s 397ms/step - loss: 0.0833 - accuracy: 0.8914 - val_loss: 0.0814 - val_accuracy: 0.8940\nEpoch 10/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0802 - accuracy: 0.8962 - val_loss: 0.0851 - val_accuracy: 0.8893\nEpoch 11/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0771 - accuracy: 0.9005 - val_loss: 0.0779 - val_accuracy: 0.8987\nEpoch 12/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0740 - accuracy: 0.9045 - val_loss: 0.0774 - val_accuracy: 0.8992\nEpoch 13/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0709 - accuracy: 0.9091 - val_loss: 0.0729 - val_accuracy: 0.9061\nEpoch 14/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0680 - accuracy: 0.9137 - val_loss: 0.0734 - val_accuracy: 0.9041\nEpoch 15/25\n1770/1770 [==============================] - 707s 397ms/step - loss: 0.0655 - accuracy: 0.9165 - val_loss: 0.0717 - val_accuracy: 0.9075\nEpoch 16/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0631 - accuracy: 0.9200 - val_loss: 0.0738 - val_accuracy: 0.9047\nEpoch 17/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0604 - accuracy: 0.9238 - val_loss: 0.0727 - val_accuracy: 0.9067\nEpoch 18/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0576 - accuracy: 0.9274 - val_loss: 0.0712 - val_accuracy: 0.9069\nEpoch 19/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0552 - accuracy: 0.9308 - val_loss: 0.0687 - val_accuracy: 0.9130\nEpoch 20/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0522 - accuracy: 0.9352 - val_loss: 0.0699 - val_accuracy: 0.9108\nEpoch 21/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0495 - accuracy: 0.9385 - val_loss: 0.0710 - val_accuracy: 0.9082\nEpoch 22/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0469 - accuracy: 0.9422 - val_loss: 0.0663 - val_accuracy: 0.9157\nEpoch 23/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0448 - accuracy: 0.9447 - val_loss: 0.0720 - val_accuracy: 0.9081\nEpoch 24/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0418 - accuracy: 0.9491 - val_loss: 0.0671 - val_accuracy: 0.9152\nEpoch 25/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0397 - accuracy: 0.9514 - val_loss: 0.0721 - val_accuracy: 0.9074\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows Gray 1000 - Vect 2048 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2lcm2eri\" target=\"_blank\">ethereal-vortex-180</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 408s 221ms/step - loss: 0.2102 - accuracy: 0.6578 - val_loss: 0.1930 - val_accuracy: 0.7092\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 391s 220ms/step - loss: 0.1806 - accuracy: 0.7289 - val_loss: 0.1706 - val_accuracy: 0.7442\nEpoch 3/25\n1770/1770 [==============================] - 391s 220ms/step - loss: 0.1662 - accuracy: 0.7546 - val_loss: 0.1604 - val_accuracy: 0.7679\nEpoch 4/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1566 - accuracy: 0.7718 - val_loss: 0.1515 - val_accuracy: 0.7845\nEpoch 5/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1474 - accuracy: 0.7873 - val_loss: 0.1421 - val_accuracy: 0.7952\nEpoch 6/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1393 - accuracy: 0.8012 - val_loss: 0.1365 - val_accuracy: 0.8035\nEpoch 7/25\n1770/1770 [==============================] - 391s 220ms/step - loss: 0.1304 - accuracy: 0.8165 - val_loss: 0.1292 - val_accuracy: 0.8177\nEpoch 8/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1221 - accuracy: 0.8294 - val_loss: 0.1197 - val_accuracy: 0.8333\nEpoch 9/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1144 - accuracy: 0.8430 - val_loss: 0.1164 - val_accuracy: 0.8404\nEpoch 10/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1071 - accuracy: 0.8542 - val_loss: 0.1070 - val_accuracy: 0.8550\nEpoch 11/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1005 - accuracy: 0.8645 - val_loss: 0.1037 - val_accuracy: 0.8593\nEpoch 12/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0946 - accuracy: 0.8730 - val_loss: 0.1040 - val_accuracy: 0.8589\nEpoch 13/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0890 - accuracy: 0.8817 - val_loss: 0.0966 - val_accuracy: 0.8711\nEpoch 14/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0837 - accuracy: 0.8899 - val_loss: 0.0938 - val_accuracy: 0.8740\nEpoch 15/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0793 - accuracy: 0.8961 - val_loss: 0.0928 - val_accuracy: 0.8748\nEpoch 16/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0740 - accuracy: 0.9046 - val_loss: 0.0918 - val_accuracy: 0.8767\nEpoch 17/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0688 - accuracy: 0.9118 - val_loss: 0.0910 - val_accuracy: 0.8767\nEpoch 18/25\n1770/1770 [==============================] - 391s 220ms/step - loss: 0.0631 - accuracy: 0.9192 - val_loss: 0.0874 - val_accuracy: 0.8825\nEpoch 19/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0581 - accuracy: 0.9265 - val_loss: 0.0831 - val_accuracy: 0.8890\nEpoch 20/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0532 - accuracy: 0.9336 - val_loss: 0.0853 - val_accuracy: 0.8865\nEpoch 21/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0486 - accuracy: 0.9398 - val_loss: 0.0854 - val_accuracy: 0.8856\nEpoch 22/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0439 - accuracy: 0.9463 - val_loss: 0.0819 - val_accuracy: 0.8909\nEpoch 23/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0395 - accuracy: 0.9525 - val_loss: 0.0838 - val_accuracy: 0.8898\nEpoch 24/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0361 - accuracy: 0.9568 - val_loss: 0.0839 - val_accuracy: 0.8888\nEpoch 25/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0327 - accuracy: 0.9614 - val_loss: 0.0868 - val_accuracy: 0.8873\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows Gray 1000 - Vect 512 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 512,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 406s 221ms/step - loss: 0.2501 - accuracy: 0.4994 - val_loss: 0.2500 - val_accuracy: 0.4988\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 391s 220ms/step - loss: 0.2500 - accuracy: 0.4994 - val_loss: 0.2500 - val_accuracy: 0.5012\nEpoch 3/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4992 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 4/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4980 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 5/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5002 - val_loss: 0.2500 - val_accuracy: 0.5012\nEpoch 6/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4992 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 7/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5003 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 8/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5003 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 9/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4996 - val_loss: 0.2500 - val_accuracy: 0.4948\nEpoch 10/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4997 - val_loss: 0.2500 - val_accuracy: 0.4924\nEpoch 11/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5016 - val_loss: 0.2501 - val_accuracy: 0.4947\nEpoch 12/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5038 - val_loss: 0.2501 - val_accuracy: 0.4930\nEpoch 13/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5048 - val_loss: 0.2501 - val_accuracy: 0.4933\nEpoch 14/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5050 - val_loss: 0.2501 - val_accuracy: 0.4930\nEpoch 15/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5054 - val_loss: 0.2501 - val_accuracy: 0.4935\nEpoch 16/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5058 - val_loss: 0.2501 - val_accuracy: 0.4941\nEpoch 17/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5061 - val_loss: 0.2501 - val_accuracy: 0.4920\nEpoch 18/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5060 - val_loss: 0.2501 - val_accuracy: 0.4934\nEpoch 19/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5063 - val_loss: 0.2501 - val_accuracy: 0.4936\nEpoch 20/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5061 - val_loss: 0.2501 - val_accuracy: 0.4925\nEpoch 21/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5057 - val_loss: 0.2501 - val_accuracy: 0.4933\nEpoch 22/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5058 - val_loss: 0.2501 - val_accuracy: 0.4935\nEpoch 23/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.2499 - accuracy: 0.5060 - val_loss: 0.2501 - val_accuracy: 0.4929\nEpoch 24/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5063 - val_loss: 0.2501 - val_accuracy: 0.4928\nEpoch 25/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5063 - val_loss: 0.2501 - val_accuracy: 0.4937\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m wandb uses only the first 10000 datapoints to create the plots.\n"
    }
   ],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1500 - Vect 2048 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2cx1i779\" target=\"_blank\">olive-violet-178</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n 285/1770 [===>..........................] - ETA: 5:17 - loss: 0.2201 - accuracy: 0.5924"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1500 - Vect 1024 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1500 - Vect 512 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:3crcvbbp) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 34570... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e87fe3219a6b43f4960cf9417fa4d781"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 512,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 468s 234ms/step - loss: 0.2502 - accuracy: 0.4950 - val_loss: 0.2500 - val_accuracy: 0.4973\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 415s 231ms/step - loss: 0.2500 - accuracy: 0.4983 - val_loss: 0.2500 - val_accuracy: 0.4973\nEpoch 3/25\n1088/1770 [=================>............] - ETA: 2:21 - loss: 0.2500 - accuracy: 0.4987"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-5b71f04b4d7d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0msiamese_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcontrastive_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Adam\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"accuracy\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0mhistory_siamese_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msiamese_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mval_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReduceLROnPlateau\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmonitor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"val_loss\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpatience\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mWandbCallback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/wandb/integration/keras/keras.py\u001B[0m in \u001B[0;36mnew_v2\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mcbk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcbks\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m                 \u001B[0mset_wandb_attrs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcbk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 168\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mold_v2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    169\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m     \u001B[0mtraining_arrays\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morig_fit_loop\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_arrays\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1181\u001B[0m                 _r=1):\n\u001B[1;32m   1182\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1183\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1184\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1185\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    887\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    891\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    915\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    916\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 917\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    918\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    919\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3022\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m   3023\u001B[0m     return graph_function._call_flat(\n\u001B[0;32m-> 3024\u001B[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m   3025\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3026\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1959\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1960\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0;32m-> 1961\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[1;32m   1962\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1963\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    594\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    595\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 596\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    597\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    598\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0;32m---> 60\u001B[0;31m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows Bin 1000 - Vect 2048 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/14hqghll\" target=\"_blank\">wandering-planet-182</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m wandb uses only the first 10000 datapoints to create the plots.\n"
    }
   ],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows Bin 1000 - Vect 1024 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows Bin 1000 - Vect 512 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 512,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows Gray 1500 - Vect 1024 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3mk7zc98\" target=\"_blank\">glamorous-water-189</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows Gray 1500 - Vect 2048 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3dbijygp\" target=\"_blank\">drawn-thunder-191</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 407s 221ms/step - loss: 0.2132 - accuracy: 0.6513 - val_loss: 0.1962 - val_accuracy: 0.6982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.1856 - accuracy: 0.7199 - val_loss: 0.1723 - val_accuracy: 0.7459\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1711 - accuracy: 0.7459 - val_loss: 0.1670 - val_accuracy: 0.7555\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1623 - accuracy: 0.7623 - val_loss: 0.1642 - val_accuracy: 0.7556\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1514 - accuracy: 0.7818 - val_loss: 0.1452 - val_accuracy: 0.7929\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1395 - accuracy: 0.8023 - val_loss: 0.1351 - val_accuracy: 0.8094\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1263 - accuracy: 0.8249 - val_loss: 0.1250 - val_accuracy: 0.8268\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1127 - accuracy: 0.8466 - val_loss: 0.1077 - val_accuracy: 0.8530\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.1007 - accuracy: 0.8649 - val_loss: 0.1036 - val_accuracy: 0.8596\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0905 - accuracy: 0.8806 - val_loss: 0.0935 - val_accuracy: 0.8770\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0812 - accuracy: 0.8943 - val_loss: 0.0879 - val_accuracy: 0.8845\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.0730 - accuracy: 0.9064 - val_loss: 0.0860 - val_accuracy: 0.8859\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0653 - accuracy: 0.9172 - val_loss: 0.0744 - val_accuracy: 0.9047\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0598 - accuracy: 0.9251 - val_loss: 0.0711 - val_accuracy: 0.9090\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0542 - accuracy: 0.9329 - val_loss: 0.0691 - val_accuracy: 0.9131\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.0495 - accuracy: 0.9393 - val_loss: 0.0666 - val_accuracy: 0.9158\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0457 - accuracy: 0.9449 - val_loss: 0.0647 - val_accuracy: 0.9183\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0419 - accuracy: 0.9501 - val_loss: 0.0648 - val_accuracy: 0.9182\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0385 - accuracy: 0.9548 - val_loss: 0.0615 - val_accuracy: 0.9229\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0356 - accuracy: 0.9587 - val_loss: 0.0630 - val_accuracy: 0.9210\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0333 - accuracy: 0.9613 - val_loss: 0.0622 - val_accuracy: 0.9230\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.0305 - accuracy: 0.9646 - val_loss: 0.0626 - val_accuracy: 0.9229\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0283 - accuracy: 0.9679 - val_loss: 0.0660 - val_accuracy: 0.9169\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0270 - accuracy: 0.9692 - val_loss: 0.0592 - val_accuracy: 0.9268\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0251 - accuracy: 0.9718 - val_loss: 0.0602 - val_accuracy: 0.9262\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from helper_functions import get_classification_report\n",
    "from helper_functions import create_tf_data_datasets_contrastive\n",
    "from helper_functions import create_tf_data_testset_contrastive\n",
    "from helper_functions import euclidean_distance\n",
    "from helper_functions import manhattan_distance\n",
    "from helper_functions import contrastive_loss\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class ConvNeXt_Block(layers.Layer):\n",
    "    r\"\"\" ConvNeXt Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = layers.DepthwiseConv2D(kernel_size=7, padding='same')  # depthwise conv\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.pwconv1 = layers.Dense(4 * dim)\n",
    "        self.act = layers.Activation('gelu')\n",
    "        self.pwconv2 = layers.Dense(dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.dim = dim\n",
    "        self.layer_scale_init_value = layer_scale_init_value\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            initial_value=self.layer_scale_init_value * tf.ones((self.dim)),\n",
    "            trainable=True,\n",
    "            name='_gamma')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "class Downsample_Block(layers.Layer):\n",
    "    \"\"\"The Downsample Block in ConvNeXt\n",
    "        Args:\n",
    "            dim (int): number of channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.LN = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.conv = layers.Conv2D(dim, kernel_size=2, strides=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.LN(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class DropPath(tf.keras.layers.Layer):\n",
    "    \"\"\"The Drop path in ConvNeXt\n",
    "        Reference:\n",
    "            https://github.com/rishigami/Swin-Transformer-TF/blob/main/swintransformer/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        return self._drop_path(x, self.drop_prob, training)\n",
    "\n",
    "\n",
    "    def _drop_path(self, inputs, drop_prob, is_training):\n",
    "        if (not is_training) or (drop_prob == 0.):\n",
    "            return inputs\n",
    "\n",
    "        # Compute keep_prob\n",
    "        keep_prob = 1.0 - drop_prob\n",
    "\n",
    "        # Compute drop_connect tensor\n",
    "        random_tensor = keep_prob\n",
    "        shape = (tf.shape(inputs)[0],) + (1,) * (len(tf.shape(inputs)) - 1)\n",
    "        random_tensor += tf.random.uniform(shape, dtype=inputs.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def create_convnext_model(input_shape=(224, 224, 3), depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], emb_dim=1024, drop_path=0., layer_scale_init_value=1e-6):\n",
    "    \"\"\" Function to construct the ConvNeXt Model\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): (Width, Height , Channels)\n",
    "            depths (list): a list of size 4. denoting each stage's depth\n",
    "            dims (list): a list of size 4. denoting number of kernel's in each stage\n",
    "            num_classes (int): the number of classes\n",
    "            drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "            layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        Returns:\n",
    "            ConvNeXt model: an instance of tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    assert (len(depths) == 4 and len(dims) ==4), \"Must provide exactly 4 depths and 4 dims\"\n",
    "    assert (len(input_shape) == 3), \"Input shape must be (W, H, C)\"\n",
    "\n",
    "    input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Stem + res2\n",
    "    y = layers.Conv2D(dims[0], kernel_size=4, strides=4)(input)\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    for i in range(depths[0]):\n",
    "        y = ConvNeXt_Block(dims[0], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res3\n",
    "    y = Downsample_Block(dims[1])(y)\n",
    "    for i in range(depths[1]):\n",
    "        y = ConvNeXt_Block(dims[1], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res4\n",
    "    y = Downsample_Block(dims[2])(y)\n",
    "    for i in range(depths[2]):\n",
    "        y = ConvNeXt_Block(dims[2], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res5\n",
    "    y = Downsample_Block(dims[3])(y)\n",
    "    for i in range(depths[3]):\n",
    "        y = ConvNeXt_Block(dims[3], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    y = layers.GlobalAveragePooling2D()(y)\n",
    "    # final norm layer\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    # Head\n",
    "    y = layers.Dense(emb_dim, activation='relu')(y)\n",
    "\n",
    "    return tf.keras.Model(inputs=input, outputs=y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_90k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/26tn14ti\" target=\"_blank\">pious-haze-147</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.004,\n",
    "                         \"weight_decay\": 0.05,\n",
    "                         \"otimizer\": \"AdamW\",\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 50,\n",
    "                         \"batch_size\": 32,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 90k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "514/638 [=======================>......] - ETA: 45s - loss: 0.2502 - accuracy: 0.4948"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-51-6b053944ac93>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0msiamese_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcontrastive_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Adam\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"accuracy\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0mhistory_siamese_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msiamese_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mval_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReduceLROnPlateau\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmonitor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"val_loss\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpatience\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mWandbCallback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/wandb/integration/keras/keras.py\u001B[0m in \u001B[0;36mnew_v2\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mcbk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcbks\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m                 \u001B[0mset_wandb_attrs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcbk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 168\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mold_v2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    169\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m     \u001B[0mtraining_arrays\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morig_fit_loop\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_arrays\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1181\u001B[0m                 _r=1):\n\u001B[1;32m   1182\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1183\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1184\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1185\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    887\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    891\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    915\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    916\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 917\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    918\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    919\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3022\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m   3023\u001B[0m     return graph_function._call_flat(\n\u001B[0;32m-> 3024\u001B[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m   3025\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3026\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1959\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1960\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0;32m-> 1961\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[1;32m   1962\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1963\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    594\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    595\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 596\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    597\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    598\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0;32m---> 60\u001B[0;31m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(113, 113, 3), depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "#siamese_model.compile(loss=contrastive_loss, optimizer=tfa.optimizers.AdamW(weight_decay=config.weight_decay, learning_rate=config.learning_rate), metrics=[\"accuracy\"])\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
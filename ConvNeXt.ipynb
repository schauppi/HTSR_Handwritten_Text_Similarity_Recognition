{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from helper_functions import get_classification_report\n",
    "from helper_functions import create_tf_data_datasets_contrastive\n",
    "from helper_functions import create_tf_data_testset_contrastive\n",
    "from helper_functions import euclidean_distance\n",
    "from helper_functions import manhattan_distance\n",
    "from helper_functions import contrastive_loss\n",
    "from helper_functions import l2Norm\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNeXt_Block(layers.Layer):\n",
    "    r\"\"\" ConvNeXt Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = layers.DepthwiseConv2D(kernel_size=7, padding='same')  # depthwise conv\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.pwconv1 = layers.Dense(4 * dim)\n",
    "        self.act = layers.Activation('gelu')\n",
    "        self.pwconv2 = layers.Dense(dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.dim = dim\n",
    "        self.layer_scale_init_value = layer_scale_init_value\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            initial_value=self.layer_scale_init_value * tf.ones((self.dim)),\n",
    "            trainable=True,\n",
    "            name='_gamma')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            #self.gamma = tf.cast(self.gamma, dtype='float16')\n",
    "            x = self.gamma * x\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Downsample_Block(layers.Layer):\n",
    "    \"\"\"The Downsample Block in ConvNeXt\n",
    "        Args:\n",
    "            dim (int): number of channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.LN = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.conv = layers.Conv2D(dim, kernel_size=2, strides=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.LN(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class DropPath(tf.keras.layers.Layer):\n",
    "    \"\"\"The Drop path in ConvNeXt\n",
    "        Reference:\n",
    "            https://github.com/rishigami/Swin-Transformer-TF/blob/main/swintransformer/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        return self._drop_path(x, self.drop_prob, training)\n",
    "\n",
    "\n",
    "    def _drop_path(self, inputs, drop_prob, is_training):\n",
    "        if (not is_training) or (drop_prob == 0.):\n",
    "            return inputs\n",
    "\n",
    "        # Compute keep_prob\n",
    "        keep_prob = 1.0 - drop_prob\n",
    "\n",
    "        # Compute drop_connect tensor\n",
    "        random_tensor = keep_prob\n",
    "        shape = (tf.shape(inputs)[0],) + (1,) * (len(tf.shape(inputs)) - 1)\n",
    "        random_tensor += tf.random.uniform(shape, dtype=inputs.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_convnext_model(input_shape=(224, 224, 3), depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], emb_dim=1024, drop_path=0., layer_scale_init_value=1e-6, l2_norm=False):\n",
    "    \"\"\" Function to construct the ConvNeXt Model\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): (Width, Height , Channels)\n",
    "            depths (list): a list of size 4. denoting each stage's depth\n",
    "            dims (list): a list of size 4. denoting number of kernel's in each stage\n",
    "            num_classes (int): the number of classes\n",
    "            drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "            layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        Returns:\n",
    "            ConvNeXt model: an instance of tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    assert (len(depths) == 4 and len(dims) ==4), \"Must provide exactly 4 depths and 4 dims\"\n",
    "    assert (len(input_shape) == 3), \"Input shape must be (W, H, C)\"\n",
    "\n",
    "    input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Stem + res2\n",
    "    y = layers.Conv2D(dims[0], kernel_size=4, strides=4)(input)\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    for i in range(depths[0]):\n",
    "        y = ConvNeXt_Block(dims[0], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res3\n",
    "    y = Downsample_Block(dims[1])(y)\n",
    "    for i in range(depths[1]):\n",
    "        y = ConvNeXt_Block(dims[1], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res4\n",
    "    y = Downsample_Block(dims[2])(y)\n",
    "    for i in range(depths[2]):\n",
    "        y = ConvNeXt_Block(dims[2], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res5\n",
    "    y = Downsample_Block(dims[3])(y)\n",
    "    for i in range(depths[3]):\n",
    "        y = ConvNeXt_Block(dims[3], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    y = layers.GlobalAveragePooling2D()(y)\n",
    "    # final norm layer\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    # Head\n",
    "\n",
    "    if l2_norm == True:\n",
    "        y = layers.Lambda(l2Norm)(y)\n",
    "        y = layers.Dense(emb_dim, activation='relu')(y)\n",
    "    else:\n",
    "        y = layers.Dense(emb_dim, activation='relu')(y)\n",
    "\n",
    "    return tf.keras.Model(inputs=input, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows RGB 1000 - Vect 1024 -  Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 50,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1770/1770 [==============================] - 464s 230ms/step - loss: 0.1972 - accuracy: 0.6766 - val_loss: 0.1379 - val_accuracy: 0.8206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "1770/1770 [==============================] - 414s 227ms/step - loss: 0.1297 - accuracy: 0.8335 - val_loss: 0.1120 - val_accuracy: 0.8517\n",
      "Epoch 3/50\n",
      "1770/1770 [==============================] - 406s 226ms/step - loss: 0.1019 - accuracy: 0.8703 - val_loss: 0.0922 - val_accuracy: 0.8819\n",
      "Epoch 4/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0891 - accuracy: 0.8850 - val_loss: 0.0784 - val_accuracy: 0.9002\n",
      "Epoch 5/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0808 - accuracy: 0.8948 - val_loss: 0.0776 - val_accuracy: 0.8996\n",
      "Epoch 6/50\n",
      "1770/1770 [==============================] - 406s 227ms/step - loss: 0.0743 - accuracy: 0.9040 - val_loss: 0.0704 - val_accuracy: 0.9103\n",
      "Epoch 7/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0683 - accuracy: 0.9127 - val_loss: 0.0605 - val_accuracy: 0.9243\n",
      "Epoch 8/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0621 - accuracy: 0.9209 - val_loss: 0.0654 - val_accuracy: 0.9169\n",
      "Epoch 9/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0556 - accuracy: 0.9303 - val_loss: 0.0544 - val_accuracy: 0.9306\n",
      "Epoch 10/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0505 - accuracy: 0.9376 - val_loss: 0.0572 - val_accuracy: 0.9273\n",
      "Epoch 11/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0462 - accuracy: 0.9434 - val_loss: 0.0484 - val_accuracy: 0.9395\n",
      "Epoch 12/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0423 - accuracy: 0.9484 - val_loss: 0.0484 - val_accuracy: 0.9395\n",
      "Epoch 13/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0395 - accuracy: 0.9527 - val_loss: 0.0447 - val_accuracy: 0.9444\n",
      "Epoch 14/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0360 - accuracy: 0.9565 - val_loss: 0.0407 - val_accuracy: 0.9499\n",
      "Epoch 15/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0333 - accuracy: 0.9603 - val_loss: 0.0422 - val_accuracy: 0.9479\n",
      "Epoch 16/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0308 - accuracy: 0.9638 - val_loss: 0.0416 - val_accuracy: 0.9481\n",
      "Epoch 17/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0285 - accuracy: 0.9668 - val_loss: 0.0391 - val_accuracy: 0.9530\n",
      "Epoch 18/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0261 - accuracy: 0.9699 - val_loss: 0.0372 - val_accuracy: 0.9553\n",
      "Epoch 19/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0249 - accuracy: 0.9716 - val_loss: 0.0392 - val_accuracy: 0.9517\n",
      "Epoch 20/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0238 - accuracy: 0.9729 - val_loss: 0.0359 - val_accuracy: 0.9570\n",
      "Epoch 21/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0216 - accuracy: 0.9757 - val_loss: 0.0374 - val_accuracy: 0.9539\n",
      "Epoch 22/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0203 - accuracy: 0.9773 - val_loss: 0.0428 - val_accuracy: 0.9468\n",
      "Epoch 23/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0195 - accuracy: 0.9782 - val_loss: 0.0385 - val_accuracy: 0.9534\n",
      "Epoch 24/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0181 - accuracy: 0.9799 - val_loss: 0.0358 - val_accuracy: 0.9565\n",
      "Epoch 25/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0167 - accuracy: 0.9819 - val_loss: 0.0363 - val_accuracy: 0.9557\n",
      "Epoch 26/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0121 - accuracy: 0.9875 - val_loss: 0.0318 - val_accuracy: 0.9618\n",
      "Epoch 27/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0105 - accuracy: 0.9897 - val_loss: 0.0323 - val_accuracy: 0.9616\n",
      "Epoch 28/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0093 - accuracy: 0.9909 - val_loss: 0.0321 - val_accuracy: 0.9616\n",
      "Epoch 29/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0088 - accuracy: 0.9914 - val_loss: 0.0324 - val_accuracy: 0.9612\n",
      "Epoch 30/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0082 - accuracy: 0.9920 - val_loss: 0.0317 - val_accuracy: 0.9621\n",
      "Epoch 31/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0074 - accuracy: 0.9928 - val_loss: 0.0331 - val_accuracy: 0.9605\n",
      "Epoch 32/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0072 - accuracy: 0.9931 - val_loss: 0.0326 - val_accuracy: 0.9611\n",
      "Epoch 33/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0070 - accuracy: 0.9933 - val_loss: 0.0325 - val_accuracy: 0.9615\n",
      "Epoch 34/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0071 - accuracy: 0.9932 - val_loss: 0.0327 - val_accuracy: 0.9613\n",
      "Epoch 35/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0067 - accuracy: 0.9936 - val_loss: 0.0327 - val_accuracy: 0.9614\n",
      "Epoch 36/50\n",
      "1770/1770 [==============================] - 406s 227ms/step - loss: 0.0068 - accuracy: 0.9935 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 37/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0069 - accuracy: 0.9933 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 38/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0065 - accuracy: 0.9938 - val_loss: 0.0326 - val_accuracy: 0.9612\n",
      "Epoch 39/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0068 - accuracy: 0.9935 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 40/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0064 - accuracy: 0.9939 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 41/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0068 - accuracy: 0.9935 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 42/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0065 - accuracy: 0.9938 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 43/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0065 - accuracy: 0.9937 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 44/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0068 - accuracy: 0.9935 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 45/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0065 - accuracy: 0.9938 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 46/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0066 - accuracy: 0.9936 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 47/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0066 - accuracy: 0.9938 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 48/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0066 - accuracy: 0.9937 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 49/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0067 - accuracy: 0.9936 - val_loss: 0.0326 - val_accuracy: 0.9613\n",
      "Epoch 50/50\n",
      "1770/1770 [==============================] - 405s 226ms/step - loss: 0.0063 - accuracy: 0.9940 - val_loss: 0.0326 - val_accuracy: 0.9613\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows Gray 1000 - Vect 1024 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschauppi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2ggsgp9k\" target=\"_blank\">hopeful-paper-160</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 409s 221ms/step - loss: 0.2089 - accuracy: 0.6588 - val_loss: 0.1921 - val_accuracy: 0.6997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1854 - accuracy: 0.7159 - val_loss: 0.1692 - val_accuracy: 0.7565\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1622 - accuracy: 0.7629 - val_loss: 0.1552 - val_accuracy: 0.7739\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.1477 - accuracy: 0.7865 - val_loss: 0.1468 - val_accuracy: 0.7867\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1371 - accuracy: 0.8044 - val_loss: 0.1338 - val_accuracy: 0.8105\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.1266 - accuracy: 0.8222 - val_loss: 0.1268 - val_accuracy: 0.8224\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1158 - accuracy: 0.8398 - val_loss: 0.1146 - val_accuracy: 0.8404\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1054 - accuracy: 0.8554 - val_loss: 0.1069 - val_accuracy: 0.8533\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0952 - accuracy: 0.8722 - val_loss: 0.0952 - val_accuracy: 0.8713\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.0857 - accuracy: 0.8864 - val_loss: 0.0893 - val_accuracy: 0.8787\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0773 - accuracy: 0.8984 - val_loss: 0.0867 - val_accuracy: 0.8830\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0696 - accuracy: 0.9092 - val_loss: 0.0759 - val_accuracy: 0.8991\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0625 - accuracy: 0.9196 - val_loss: 0.0745 - val_accuracy: 0.9016\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0551 - accuracy: 0.9310 - val_loss: 0.0675 - val_accuracy: 0.9113\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0489 - accuracy: 0.9397 - val_loss: 0.0638 - val_accuracy: 0.9174\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0430 - accuracy: 0.9479 - val_loss: 0.0667 - val_accuracy: 0.9124\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0380 - accuracy: 0.9548 - val_loss: 0.0612 - val_accuracy: 0.9199\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0331 - accuracy: 0.9617 - val_loss: 0.0695 - val_accuracy: 0.9082\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0291 - accuracy: 0.9667 - val_loss: 0.0658 - val_accuracy: 0.9154\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0259 - accuracy: 0.9707 - val_loss: 0.0600 - val_accuracy: 0.9227\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0225 - accuracy: 0.9753 - val_loss: 0.0655 - val_accuracy: 0.9140\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0208 - accuracy: 0.9772 - val_loss: 0.0604 - val_accuracy: 0.9226\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.0191 - accuracy: 0.9791 - val_loss: 0.0635 - val_accuracy: 0.9189\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0177 - accuracy: 0.9808 - val_loss: 0.0627 - val_accuracy: 0.9207\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0168 - accuracy: 0.9817 - val_loss: 0.0644 - val_accuracy: 0.9177\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientnetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"optimizer\": \"Adam\",\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"VGG19 - 1m - RGB\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "base_model = tf.keras.applications.EfficientNetB0(weights='imagenet', include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.layers.Input((height, width, channels))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(config.embedding_dimension, activation=\"relu\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=config.optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### EfficientNetB0V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"optimizer\": \"Adam\",\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"VGG19 - 1m - RGB\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "import keras_efficientnet_v2\n",
    "\n",
    "base_model = keras_efficientnet_v2.EfficientNetV2S(pretrained=\"imagenet\", num_classes=0, input_shape=(height, width, channels), classifier_activation=\"relu\")\n",
    "\n",
    "#set layers not trainable\n",
    "for layer in base_model.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "inputs = keras.layers.Input((height, width, channels))\n",
    "x = base_model(inputs)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(config.embedding_dimension, activation=\"relu\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=config.optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionResnetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1rx6avkt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fragrant-sound-455</strong>: <a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1rx6avkt\" target=\"_blank\">https://wandb.ai/schauppi/Architecture_1/runs/1rx6avkt</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220330_095644-1rx6avkt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1rx6avkt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/js/workspace/thesis_schaupp/HTSR_Handwritten_Text_Similarity_Recognition/wandb/run-20220330_095812-2hdsay93</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2hdsay93\" target=\"_blank\">warm-pine-456</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/38\n",
      "7080/7080 [==============================] - 821s 109ms/step - loss: 0.2156 - accuracy: 0.6399 - val_loss: 0.1930 - val_accuracy: 0.7011\n",
      "Epoch 2/38\n",
      "2055/7080 [=======>......................] - ETA: 6:14 - loss: 0.1886 - accuracy: 0.7117"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ea0fd6c2e3f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0msiamese_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontrastive_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mhistory_siamese_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msiamese_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWandbCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"optimizer\": \"Adam\",\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"InceptionResnetV2 - 1m - RGB\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.layers.Input((height, width, channels))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(config.embedding_dimension, activation=\"relu\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=config.optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNext Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"optimizer\": \"Adam\",\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 1,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"ConvNeXt Tiny - 1m - RGB\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "from convnext.models.convnext_tf import create_model \n",
    "\n",
    "base_model = create_model('convnext_tiny_224', input_shape=(height, width), include_top=False, pretrained=True)\n",
    "\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable=False\n",
    "    \n",
    "inputs = keras.layers.Input((height, width, channels))\n",
    "x = base_model(inputs)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(config.embedding_dimension, activation=\"relu\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=config.optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pretraining / Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#CVL DATASET\n",
    "\n",
    "anchor_images_path = \"npz_datasets/pairs_cvl_500k_224_224_rows_1000_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_cvl_500k_224_224_rows_1000_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_cvl_224_224_rows_1000_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_cvl_224_224_rows_1000_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)\n",
    "\n",
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [1,1,1,1],\n",
    "                         \"conv_dims\": [24, 24, 48, 48],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"ConvNeXt - CVL - Bin\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=config.otimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})\n",
    "\n",
    "run.finish()\n",
    "\n",
    "#DATASET\n",
    "\n",
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [1,1,1,1],\n",
    "                         \"conv_dims\": [24, 24, 48, 48],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"ConvNeXt - 1m - Bin\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "anchor_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)\n",
    "\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=config.otimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pretraining with 500k CVL and 1m bin - finetuning 500k bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0', '/device:GPU:1')\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.AUTO\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2w24q179) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e85d9d383bf4e479af243307515bb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">laced-frog-422</strong>: <a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2w24q179\" target=\"_blank\">https://wandb.ai/schauppi/Architecture_1/runs/2w24q179</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220324_102332-2w24q179/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2w24q179). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/js/workspace/thesis_schaupp/HTSR_Handwritten_Text_Similarity_Recognition/wandb/run-20220324_102654-2rywq02z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2rywq02z\" target=\"_blank\">dashing-planet-423</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/38\n",
      "INFO:tensorflow:Collective batch_all_reduce: 58 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 58 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "3540/3540 [==============================] - ETA: 0s - loss: 0.2216 - accuracy: 0.6285INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 2, implementation = AUTO, num_packs = 1\n",
      "3540/3540 [==============================] - 273s 74ms/step - loss: 0.2216 - accuracy: 0.6285 - val_loss: 0.1454 - val_accuracy: 0.8038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/38\n",
      "3540/3540 [==============================] - 259s 73ms/step - loss: 0.1329 - accuracy: 0.8224 - val_loss: 0.1182 - val_accuracy: 0.8412\n",
      "Epoch 3/38\n",
      "3540/3540 [==============================] - ETA: 0s - loss: 0.1179 - accuracy: 0.8416"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [1,1,1,1],\n",
    "                         \"conv_dims\": [48, 96, 192, 192],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 256,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"ConvNeXt - 500k CVL - Gray\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    anchor_images_path = \"npz_datasets/pairs_cvl_500k_224_224_rows_1000/anchor\"\n",
    "    positive_images_path = \"npz_datasets/pairs_cvl_500k_224_224_rows_1000/positive\"\n",
    "    width, height, channels = 113, 113, 1\n",
    "    batch_size = config.batch_size\n",
    "    train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "    anchor_images_test_path = \"npz_datasets/test_pairs_cvl_224_224_rows_1000/anchor\"\n",
    "    positive_images_test_path = \"npz_datasets/test_pairs_cvl_224_224_rows_1000/positive\"\n",
    "    test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)\n",
    "\n",
    "    model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "    left_input = layers.Input(shape=(height, width, channels))\n",
    "    right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "    distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "    prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "    siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "    siamese_model.compile(loss=contrastive_loss, optimizer=config.otimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})\n",
    "\n",
    "run.finish()\n",
    "\n",
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [1,1,1,1],\n",
    "                         \"conv_dims\": [48, 96, 192, 192],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 256,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"ConvNeXt - 1m - Gray\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    anchor_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/anchor\"\n",
    "    positive_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/positive\"\n",
    "    width, height, channels = 113, 113, 1\n",
    "    batch_size = config.batch_size\n",
    "    train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "    anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "    positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "    test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)\n",
    "\n",
    "    siamese_model.compile(loss=contrastive_loss, optimizer=config.otimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})\n",
    "\n",
    "run.finish()\n",
    "\n",
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [1,1,1,1],\n",
    "                         \"conv_dims\": [48, 96, 192, 192],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 256,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"ConvNeXt - 500k - Gray\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    anchor_images_path = \"npz_datasets/pairs_500k_224_224_rows_1000/anchor\"\n",
    "    positive_images_path = \"npz_datasets/pairs_500k_224_224_rows_1000/positive\"\n",
    "    width, height, channels = 113, 113, 1\n",
    "    batch_size = config.batch_size\n",
    "    train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "    anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "    positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "    test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)\n",
    "\n",
    "    siamese_model.compile(loss=contrastive_loss, optimizer=config.otimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows RGB 1000 - Vect 2048 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/js/workspace/thesis_schaupp/HTSR_Handwritten_Text_Similarity_Recognition/wandb/run-20220318_081000-3r2cod22</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3r2cod22\" target=\"_blank\">rare-glade-353</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/38\n",
      "7080/7080 [==============================] - 574s 80ms/step - loss: 0.2022 - accuracy: 0.6726 - val_loss: 0.1560 - val_accuracy: 0.7737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/38\n",
      "7080/7080 [==============================] - 570s 80ms/step - loss: 0.1531 - accuracy: 0.7767 - val_loss: 0.1449 - val_accuracy: 0.7882\n",
      "Epoch 3/38\n",
      "7080/7080 [==============================] - 569s 80ms/step - loss: 0.1329 - accuracy: 0.8114 - val_loss: 0.1108 - val_accuracy: 0.8478\n",
      "Epoch 4/38\n",
      "7080/7080 [==============================] - 569s 80ms/step - loss: 0.1063 - accuracy: 0.8546 - val_loss: 0.0962 - val_accuracy: 0.8699\n",
      "Epoch 5/38\n",
      "1879/7080 [======>.......................] - ETA: 6:11 - loss: 0.0900 - accuracy: 0.8792"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [1,1,1,1],\n",
    "                         \"conv_dims\": [48, 96, 192, 192],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"ConvNeXt - 1m - Gray\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=config.otimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows RGB 1000 - Vect 512 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/70e8ta6h\" target=\"_blank\">rare-leaf-171</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 512,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 428s 232ms/step - loss: 0.1779 - accuracy: 0.7417 - val_loss: 0.1400 - val_accuracy: 0.8237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 412s 230ms/step - loss: 0.1296 - accuracy: 0.8275 - val_loss: 0.1158 - val_accuracy: 0.8467\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.1087 - accuracy: 0.8578 - val_loss: 0.0960 - val_accuracy: 0.8779\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0956 - accuracy: 0.8752 - val_loss: 0.0913 - val_accuracy: 0.8802\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0880 - accuracy: 0.8849 - val_loss: 0.0831 - val_accuracy: 0.8922\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0823 - accuracy: 0.8929 - val_loss: 0.0793 - val_accuracy: 0.8969\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0779 - accuracy: 0.8989 - val_loss: 0.0783 - val_accuracy: 0.8969\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0731 - accuracy: 0.9053 - val_loss: 0.0719 - val_accuracy: 0.9074\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0697 - accuracy: 0.9098 - val_loss: 0.0657 - val_accuracy: 0.9158\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0660 - accuracy: 0.9155 - val_loss: 0.0639 - val_accuracy: 0.9178\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0622 - accuracy: 0.9210 - val_loss: 0.0659 - val_accuracy: 0.9150\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0594 - accuracy: 0.9247 - val_loss: 0.0598 - val_accuracy: 0.9241\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0559 - accuracy: 0.9297 - val_loss: 0.0588 - val_accuracy: 0.9254\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0539 - accuracy: 0.9322 - val_loss: 0.0568 - val_accuracy: 0.9274\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0511 - accuracy: 0.9367 - val_loss: 0.0536 - val_accuracy: 0.9324\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0487 - accuracy: 0.9393 - val_loss: 0.0519 - val_accuracy: 0.9341\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0467 - accuracy: 0.9420 - val_loss: 0.0522 - val_accuracy: 0.9338\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0444 - accuracy: 0.9451 - val_loss: 0.0502 - val_accuracy: 0.9366\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0418 - accuracy: 0.9490 - val_loss: 0.0535 - val_accuracy: 0.9319\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0400 - accuracy: 0.9510 - val_loss: 0.0474 - val_accuracy: 0.9400\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0379 - accuracy: 0.9539 - val_loss: 0.0482 - val_accuracy: 0.9389\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0361 - accuracy: 0.9563 - val_loss: 0.0447 - val_accuracy: 0.9447\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0343 - accuracy: 0.9589 - val_loss: 0.0456 - val_accuracy: 0.9429\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0328 - accuracy: 0.9608 - val_loss: 0.0450 - val_accuracy: 0.9438\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0311 - accuracy: 0.9632 - val_loss: 0.0443 - val_accuracy: 0.9447\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows RGB 1000 - Vect 256 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 256,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 428s 232ms/step - loss: 0.2501 - accuracy: 0.4968 - val_loss: 0.2500 - val_accuracy: 0.5027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 412s 230ms/step - loss: 0.2500 - accuracy: 0.5012 - val_loss: 0.2500 - val_accuracy: 0.4973\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.4999 - val_loss: 0.2500 - val_accuracy: 0.5027\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5002 - val_loss: 0.2500 - val_accuracy: 0.5027\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.4993 - val_loss: 0.2500 - val_accuracy: 0.5027\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5008 - val_loss: 0.2500 - val_accuracy: 0.4973\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5007 - val_loss: 0.2500 - val_accuracy: 0.4973\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5016 - val_loss: 0.2500 - val_accuracy: 0.4988\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5020 - val_loss: 0.2500 - val_accuracy: 0.5008\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5035 - val_loss: 0.2500 - val_accuracy: 0.5010\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5030 - val_loss: 0.2500 - val_accuracy: 0.4978\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5069 - val_loss: 0.2500 - val_accuracy: 0.4973\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5078 - val_loss: 0.2500 - val_accuracy: 0.4971\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 412s 230ms/step - loss: 0.2499 - accuracy: 0.5083 - val_loss: 0.2500 - val_accuracy: 0.4977\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5086 - val_loss: 0.2501 - val_accuracy: 0.4969\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5083 - val_loss: 0.2501 - val_accuracy: 0.4974\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 412s 230ms/step - loss: 0.2499 - accuracy: 0.5093 - val_loss: 0.2501 - val_accuracy: 0.4982\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5096 - val_loss: 0.2501 - val_accuracy: 0.4987\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5094 - val_loss: 0.2501 - val_accuracy: 0.4981\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 414s 231ms/step - loss: 0.2499 - accuracy: 0.5098 - val_loss: 0.2501 - val_accuracy: 0.4980\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5095 - val_loss: 0.2501 - val_accuracy: 0.4987\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5102 - val_loss: 0.2501 - val_accuracy: 0.4984\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5099 - val_loss: 0.2501 - val_accuracy: 0.4986\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5096 - val_loss: 0.2501 - val_accuracy: 0.4989\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5097 - val_loss: 0.2501 - val_accuracy: 0.4990\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m wandb uses only the first 10000 datapoints to create the plots.\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows RGB 1000 - Vect 2048 - Big - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschauppi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/11fqvfs4\" target=\"_blank\">swept-waterfall-177</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [96, 192, 384, 768],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 774s 402ms/step - loss: 0.1769 - accuracy: 0.7403 - val_loss: 0.1422 - val_accuracy: 0.8244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 715s 400ms/step - loss: 0.1284 - accuracy: 0.8310 - val_loss: 0.1170 - val_accuracy: 0.8478\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 710s 398ms/step - loss: 0.1117 - accuracy: 0.8519 - val_loss: 0.1093 - val_accuracy: 0.8518\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 709s 398ms/step - loss: 0.1038 - accuracy: 0.8623 - val_loss: 0.1094 - val_accuracy: 0.8532\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0981 - accuracy: 0.8706 - val_loss: 0.0942 - val_accuracy: 0.8757\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 708s 397ms/step - loss: 0.0941 - accuracy: 0.8756 - val_loss: 0.0944 - val_accuracy: 0.8746\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0901 - accuracy: 0.8816 - val_loss: 0.0852 - val_accuracy: 0.8894\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 707s 397ms/step - loss: 0.0865 - accuracy: 0.8871 - val_loss: 0.0860 - val_accuracy: 0.8869\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 708s 397ms/step - loss: 0.0833 - accuracy: 0.8914 - val_loss: 0.0814 - val_accuracy: 0.8940\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0802 - accuracy: 0.8962 - val_loss: 0.0851 - val_accuracy: 0.8893\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0771 - accuracy: 0.9005 - val_loss: 0.0779 - val_accuracy: 0.8987\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0740 - accuracy: 0.9045 - val_loss: 0.0774 - val_accuracy: 0.8992\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0709 - accuracy: 0.9091 - val_loss: 0.0729 - val_accuracy: 0.9061\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0680 - accuracy: 0.9137 - val_loss: 0.0734 - val_accuracy: 0.9041\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 707s 397ms/step - loss: 0.0655 - accuracy: 0.9165 - val_loss: 0.0717 - val_accuracy: 0.9075\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0631 - accuracy: 0.9200 - val_loss: 0.0738 - val_accuracy: 0.9047\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0604 - accuracy: 0.9238 - val_loss: 0.0727 - val_accuracy: 0.9067\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0576 - accuracy: 0.9274 - val_loss: 0.0712 - val_accuracy: 0.9069\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0552 - accuracy: 0.9308 - val_loss: 0.0687 - val_accuracy: 0.9130\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0522 - accuracy: 0.9352 - val_loss: 0.0699 - val_accuracy: 0.9108\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0495 - accuracy: 0.9385 - val_loss: 0.0710 - val_accuracy: 0.9082\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0469 - accuracy: 0.9422 - val_loss: 0.0663 - val_accuracy: 0.9157\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0448 - accuracy: 0.9447 - val_loss: 0.0720 - val_accuracy: 0.9081\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0418 - accuracy: 0.9491 - val_loss: 0.0671 - val_accuracy: 0.9152\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 708s 398ms/step - loss: 0.0397 - accuracy: 0.9514 - val_loss: 0.0721 - val_accuracy: 0.9074\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows Gray 1000 - Vect 2048 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschauppi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2lcm2eri\" target=\"_blank\">ethereal-vortex-180</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 408s 221ms/step - loss: 0.2102 - accuracy: 0.6578 - val_loss: 0.1930 - val_accuracy: 0.7092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 391s 220ms/step - loss: 0.1806 - accuracy: 0.7289 - val_loss: 0.1706 - val_accuracy: 0.7442\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 391s 220ms/step - loss: 0.1662 - accuracy: 0.7546 - val_loss: 0.1604 - val_accuracy: 0.7679\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1566 - accuracy: 0.7718 - val_loss: 0.1515 - val_accuracy: 0.7845\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1474 - accuracy: 0.7873 - val_loss: 0.1421 - val_accuracy: 0.7952\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1393 - accuracy: 0.8012 - val_loss: 0.1365 - val_accuracy: 0.8035\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 391s 220ms/step - loss: 0.1304 - accuracy: 0.8165 - val_loss: 0.1292 - val_accuracy: 0.8177\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1221 - accuracy: 0.8294 - val_loss: 0.1197 - val_accuracy: 0.8333\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1144 - accuracy: 0.8430 - val_loss: 0.1164 - val_accuracy: 0.8404\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1071 - accuracy: 0.8542 - val_loss: 0.1070 - val_accuracy: 0.8550\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1005 - accuracy: 0.8645 - val_loss: 0.1037 - val_accuracy: 0.8593\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0946 - accuracy: 0.8730 - val_loss: 0.1040 - val_accuracy: 0.8589\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0890 - accuracy: 0.8817 - val_loss: 0.0966 - val_accuracy: 0.8711\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0837 - accuracy: 0.8899 - val_loss: 0.0938 - val_accuracy: 0.8740\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0793 - accuracy: 0.8961 - val_loss: 0.0928 - val_accuracy: 0.8748\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0740 - accuracy: 0.9046 - val_loss: 0.0918 - val_accuracy: 0.8767\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0688 - accuracy: 0.9118 - val_loss: 0.0910 - val_accuracy: 0.8767\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 391s 220ms/step - loss: 0.0631 - accuracy: 0.9192 - val_loss: 0.0874 - val_accuracy: 0.8825\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0581 - accuracy: 0.9265 - val_loss: 0.0831 - val_accuracy: 0.8890\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0532 - accuracy: 0.9336 - val_loss: 0.0853 - val_accuracy: 0.8865\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0486 - accuracy: 0.9398 - val_loss: 0.0854 - val_accuracy: 0.8856\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0439 - accuracy: 0.9463 - val_loss: 0.0819 - val_accuracy: 0.8909\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0395 - accuracy: 0.9525 - val_loss: 0.0838 - val_accuracy: 0.8898\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0361 - accuracy: 0.9568 - val_loss: 0.0839 - val_accuracy: 0.8888\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0327 - accuracy: 0.9614 - val_loss: 0.0868 - val_accuracy: 0.8873\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows Gray 1000 - Vect 512 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 512,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 406s 221ms/step - loss: 0.2501 - accuracy: 0.4994 - val_loss: 0.2500 - val_accuracy: 0.4988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 391s 220ms/step - loss: 0.2500 - accuracy: 0.4994 - val_loss: 0.2500 - val_accuracy: 0.5012\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4992 - val_loss: 0.2500 - val_accuracy: 0.4988\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4980 - val_loss: 0.2500 - val_accuracy: 0.4988\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5002 - val_loss: 0.2500 - val_accuracy: 0.5012\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4992 - val_loss: 0.2500 - val_accuracy: 0.4988\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5003 - val_loss: 0.2500 - val_accuracy: 0.4988\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5003 - val_loss: 0.2500 - val_accuracy: 0.4988\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4996 - val_loss: 0.2500 - val_accuracy: 0.4948\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4997 - val_loss: 0.2500 - val_accuracy: 0.4924\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5016 - val_loss: 0.2501 - val_accuracy: 0.4947\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5038 - val_loss: 0.2501 - val_accuracy: 0.4930\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5048 - val_loss: 0.2501 - val_accuracy: 0.4933\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5050 - val_loss: 0.2501 - val_accuracy: 0.4930\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5054 - val_loss: 0.2501 - val_accuracy: 0.4935\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5058 - val_loss: 0.2501 - val_accuracy: 0.4941\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5061 - val_loss: 0.2501 - val_accuracy: 0.4920\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5060 - val_loss: 0.2501 - val_accuracy: 0.4934\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5063 - val_loss: 0.2501 - val_accuracy: 0.4936\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5061 - val_loss: 0.2501 - val_accuracy: 0.4925\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5057 - val_loss: 0.2501 - val_accuracy: 0.4933\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5058 - val_loss: 0.2501 - val_accuracy: 0.4935\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.2499 - accuracy: 0.5060 - val_loss: 0.2501 - val_accuracy: 0.4929\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5063 - val_loss: 0.2501 - val_accuracy: 0.4928\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5063 - val_loss: 0.2501 - val_accuracy: 0.4937\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m wandb uses only the first 10000 datapoints to create the plots.\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows RGB 1500 - Vect 2048 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschauppi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2cx1i779\" target=\"_blank\">olive-violet-178</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " 285/1770 [===>..........................] - ETA: 5:17 - loss: 0.2201 - accuracy: 0.5924"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows RGB 1500 - Vect 1024 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows RGB 1500 - Vect 512 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3crcvbbp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34570... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87fe3219a6b43f4960cf9417fa4d781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 512,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 468s 234ms/step - loss: 0.2502 - accuracy: 0.4950 - val_loss: 0.2500 - val_accuracy: 0.4973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 415s 231ms/step - loss: 0.2500 - accuracy: 0.4983 - val_loss: 0.2500 - val_accuracy: 0.4973\n",
      "Epoch 3/25\n",
      "1088/1770 [=================>............] - ETA: 2:21 - loss: 0.2500 - accuracy: 0.4987"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5b71f04b4d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msiamese_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontrastive_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mhistory_siamese_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msiamese_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWandbCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows Bin 1000 - Vect 2048 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschauppi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/14hqghll\" target=\"_blank\">wandering-planet-182</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m wandb uses only the first 10000 datapoints to create the plots.\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows Bin 1000 - Vect 1024 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:30b4y7nr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fb4b73f92441c0b026add7bc712b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vibrant-bird-350</strong>: <a href=\"https://wandb.ai/schauppi/Architecture_1/runs/30b4y7nr\" target=\"_blank\">https://wandb.ai/schauppi/Architecture_1/runs/30b4y7nr</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220317_081939-30b4y7nr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:30b4y7nr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/js/workspace/thesis_schaupp/HTSR_Handwritten_Text_Similarity_Recognition/wandb/run-20220317_082024-1vcop3jr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1vcop3jr\" target=\"_blank\">prime-dawn-351</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/38\n",
      "7080/7080 [==============================] - 402s 56ms/step - loss: 0.2432 - accuracy: 0.5589 - val_loss: 0.2224 - val_accuracy: 0.6257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/38\n",
      "7080/7080 [==============================] - 398s 56ms/step - loss: 0.2156 - accuracy: 0.6424 - val_loss: 0.1888 - val_accuracy: 0.7092\n",
      "Epoch 3/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.1809 - accuracy: 0.7243 - val_loss: 0.1654 - val_accuracy: 0.7546\n",
      "Epoch 4/38\n",
      "7080/7080 [==============================] - 398s 56ms/step - loss: 0.1570 - accuracy: 0.7684 - val_loss: 0.1443 - val_accuracy: 0.7895\n",
      "Epoch 5/38\n",
      "7080/7080 [==============================] - 398s 56ms/step - loss: 0.1390 - accuracy: 0.8001 - val_loss: 0.1319 - val_accuracy: 0.8099\n",
      "Epoch 6/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.1263 - accuracy: 0.8208 - val_loss: 0.1225 - val_accuracy: 0.8263\n",
      "Epoch 7/38\n",
      "7080/7080 [==============================] - 398s 56ms/step - loss: 0.1154 - accuracy: 0.8390 - val_loss: 0.1113 - val_accuracy: 0.8451\n",
      "Epoch 8/38\n",
      "7080/7080 [==============================] - 398s 56ms/step - loss: 0.1058 - accuracy: 0.8543 - val_loss: 0.1076 - val_accuracy: 0.8510\n",
      "Epoch 9/38\n",
      "7080/7080 [==============================] - 400s 56ms/step - loss: 0.0975 - accuracy: 0.8666 - val_loss: 0.1007 - val_accuracy: 0.8616\n",
      "Epoch 10/38\n",
      "7080/7080 [==============================] - 398s 56ms/step - loss: 0.0912 - accuracy: 0.8765 - val_loss: 0.0938 - val_accuracy: 0.8708\n",
      "Epoch 11/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0860 - accuracy: 0.8848 - val_loss: 0.0877 - val_accuracy: 0.8805\n",
      "Epoch 12/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0815 - accuracy: 0.8905 - val_loss: 0.0870 - val_accuracy: 0.8826\n",
      "Epoch 13/38\n",
      "7080/7080 [==============================] - 401s 56ms/step - loss: 0.0777 - accuracy: 0.8965 - val_loss: 0.0821 - val_accuracy: 0.8893\n",
      "Epoch 14/38\n",
      "7080/7080 [==============================] - 398s 56ms/step - loss: 0.0743 - accuracy: 0.9017 - val_loss: 0.0814 - val_accuracy: 0.8892\n",
      "Epoch 15/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0723 - accuracy: 0.9049 - val_loss: 0.0779 - val_accuracy: 0.8959\n",
      "Epoch 16/38\n",
      "7080/7080 [==============================] - 398s 56ms/step - loss: 0.0694 - accuracy: 0.9097 - val_loss: 0.0751 - val_accuracy: 0.8996\n",
      "Epoch 17/38\n",
      "7080/7080 [==============================] - 400s 56ms/step - loss: 0.0672 - accuracy: 0.9119 - val_loss: 0.0736 - val_accuracy: 0.9022\n",
      "Epoch 18/38\n",
      "7080/7080 [==============================] - 398s 56ms/step - loss: 0.0656 - accuracy: 0.9147 - val_loss: 0.0747 - val_accuracy: 0.9006\n",
      "Epoch 19/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0639 - accuracy: 0.9169 - val_loss: 0.0716 - val_accuracy: 0.9059\n",
      "Epoch 20/38\n",
      "7080/7080 [==============================] - 398s 56ms/step - loss: 0.0624 - accuracy: 0.9193 - val_loss: 0.0723 - val_accuracy: 0.9039\n",
      "Epoch 21/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0607 - accuracy: 0.9218 - val_loss: 0.0696 - val_accuracy: 0.9081\n",
      "Epoch 22/38\n",
      "7080/7080 [==============================] - 400s 56ms/step - loss: 0.0595 - accuracy: 0.9236 - val_loss: 0.0702 - val_accuracy: 0.9068\n",
      "Epoch 23/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0584 - accuracy: 0.9249 - val_loss: 0.0705 - val_accuracy: 0.9075\n",
      "Epoch 24/38\n",
      "7080/7080 [==============================] - 400s 56ms/step - loss: 0.0571 - accuracy: 0.9269 - val_loss: 0.0683 - val_accuracy: 0.9099\n",
      "Epoch 25/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0560 - accuracy: 0.9287 - val_loss: 0.0670 - val_accuracy: 0.9114\n",
      "Epoch 26/38\n",
      "7080/7080 [==============================] - 402s 56ms/step - loss: 0.0550 - accuracy: 0.9301 - val_loss: 0.0678 - val_accuracy: 0.9111\n",
      "Epoch 27/38\n",
      "7080/7080 [==============================] - 398s 56ms/step - loss: 0.0544 - accuracy: 0.9308 - val_loss: 0.0650 - val_accuracy: 0.9152\n",
      "Epoch 28/38\n",
      "7080/7080 [==============================] - 400s 56ms/step - loss: 0.0530 - accuracy: 0.9328 - val_loss: 0.0666 - val_accuracy: 0.9138\n",
      "Epoch 29/38\n",
      "7080/7080 [==============================] - 400s 56ms/step - loss: 0.0524 - accuracy: 0.9335 - val_loss: 0.0656 - val_accuracy: 0.9140\n",
      "Epoch 30/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0516 - accuracy: 0.9347 - val_loss: 0.0631 - val_accuracy: 0.9190\n",
      "Epoch 31/38\n",
      "7080/7080 [==============================] - 400s 56ms/step - loss: 0.0509 - accuracy: 0.9359 - val_loss: 0.0651 - val_accuracy: 0.9157\n",
      "Epoch 32/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0501 - accuracy: 0.9371 - val_loss: 0.0644 - val_accuracy: 0.9163\n",
      "Epoch 33/38\n",
      "7080/7080 [==============================] - 400s 56ms/step - loss: 0.0495 - accuracy: 0.9379 - val_loss: 0.0633 - val_accuracy: 0.9186\n",
      "Epoch 34/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0488 - accuracy: 0.9390 - val_loss: 0.0623 - val_accuracy: 0.9193\n",
      "Epoch 35/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0482 - accuracy: 0.9395 - val_loss: 0.0629 - val_accuracy: 0.9188\n",
      "Epoch 36/38\n",
      "7080/7080 [==============================] - 400s 56ms/step - loss: 0.0479 - accuracy: 0.9400 - val_loss: 0.0634 - val_accuracy: 0.9180\n",
      "Epoch 37/38\n",
      "7080/7080 [==============================] - 399s 56ms/step - loss: 0.0470 - accuracy: 0.9414 - val_loss: 0.0628 - val_accuracy: 0.9188\n",
      "Epoch 38/38\n",
      "7080/7080 [==============================] - 400s 56ms/step - loss: 0.0468 - accuracy: 0.9416 - val_loss: 0.0619 - val_accuracy: 0.9203\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0475c9597edf4311812ba7cd00c7bdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.123 MB of 0.123 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.993203â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1 - Score</td><td>â–</td></tr><tr><td>Precision</td><td>â–</td></tr><tr><td>Recall</td><td>â–</td></tr><tr><td>accuracy</td><td>â–â–ƒâ–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>loss</td><td>â–ˆâ–‡â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>lr</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_accuracy</td><td>â–â–ƒâ–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–‡â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1 - Score</td><td>0.92369</td></tr><tr><td>Precision</td><td>0.92399</td></tr><tr><td>Recall</td><td>0.9237</td></tr><tr><td>accuracy</td><td>0.94232</td></tr><tr><td>best_epoch</td><td>37</td></tr><tr><td>best_val_loss</td><td>0.06192</td></tr><tr><td>epoch</td><td>37</td></tr><tr><td>loss</td><td>0.0463</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>val_accuracy</td><td>0.92027</td></tr><tr><td>val_loss</td><td>0.06192</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">prime-dawn-351</strong>: <a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1vcop3jr\" target=\"_blank\">https://wandb.ai/schauppi/Architecture_1/runs/1vcop3jr</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220317_082024-1vcop3jr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [1,1,1,1],\n",
    "                         \"conv_dims\": [24, 24, 48, 48],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"ConvNeXt - 500k - Gray\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=config.otimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [1,1,1,1],\n",
    "                         \"conv_dims\": [24, 24, 48, 48],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 50,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": [height,width, channels],\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "learning rate sheduled from 0.0010000000474974513 to 0.0009048373904079199\n",
      "3540/3540 [==============================] - 213s 58ms/step - loss: 0.2480 - accuracy: 0.5269 - val_loss: 0.2429 - val_accuracy: 0.5692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "3540/3540 [==============================] - 204s 57ms/step - loss: 0.2425 - accuracy: 0.5697 - val_loss: 0.2397 - val_accuracy: 0.5829\n",
      "Epoch 3/50\n",
      "learning rate sheduled from 0.0009048373904079199 to 0.0008187306812033057\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.2375 - accuracy: 0.5896 - val_loss: 0.2325 - val_accuracy: 0.5982\n",
      "Epoch 4/50\n",
      "3540/3540 [==============================] - 199s 56ms/step - loss: 0.2308 - accuracy: 0.6049 - val_loss: 0.2290 - val_accuracy: 0.6068\n",
      "Epoch 5/50\n",
      "learning rate sheduled from 0.0008187306812033057 to 0.000740818097256124\n",
      "3540/3540 [==============================] - 200s 56ms/step - loss: 0.2260 - accuracy: 0.6149 - val_loss: 0.2263 - val_accuracy: 0.6127\n",
      "Epoch 6/50\n",
      "3540/3540 [==============================] - 207s 58ms/step - loss: 0.2224 - accuracy: 0.6243 - val_loss: 0.2228 - val_accuracy: 0.6209\n",
      "Epoch 7/50\n",
      "learning rate sheduled from 0.000740818097256124 to 0.000670319888740778\n",
      "3540/3540 [==============================] - 205s 57ms/step - loss: 0.2180 - accuracy: 0.6361 - val_loss: 0.2188 - val_accuracy: 0.6338\n",
      "Epoch 8/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.2129 - accuracy: 0.6513 - val_loss: 0.2158 - val_accuracy: 0.6442\n",
      "Epoch 9/50\n",
      "learning rate sheduled from 0.000670319888740778 to 0.0006065304623916745\n",
      "3540/3540 [==============================] - 199s 56ms/step - loss: 0.2056 - accuracy: 0.6698 - val_loss: 0.2065 - val_accuracy: 0.6647\n",
      "Epoch 10/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1969 - accuracy: 0.6877 - val_loss: 0.2017 - val_accuracy: 0.6708\n",
      "Epoch 11/50\n",
      "learning rate sheduled from 0.0006065304623916745 to 0.0005488114547915757\n",
      "3540/3540 [==============================] - 201s 56ms/step - loss: 0.1897 - accuracy: 0.7013 - val_loss: 0.1970 - val_accuracy: 0.6839\n",
      "Epoch 12/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1847 - accuracy: 0.7103 - val_loss: 0.1939 - val_accuracy: 0.6882\n",
      "Epoch 13/50\n",
      "learning rate sheduled from 0.0005488114547915757 to 0.0004965850966982543\n",
      "3540/3540 [==============================] - 204s 57ms/step - loss: 0.1800 - accuracy: 0.7196 - val_loss: 0.1931 - val_accuracy: 0.6934\n",
      "Epoch 14/50\n",
      "3540/3540 [==============================] - 201s 56ms/step - loss: 0.1758 - accuracy: 0.7282 - val_loss: 0.1902 - val_accuracy: 0.7017\n",
      "Epoch 15/50\n",
      "learning rate sheduled from 0.0004965850966982543 to 0.0004493287415243685\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1714 - accuracy: 0.7387 - val_loss: 0.1880 - val_accuracy: 0.7028\n",
      "Epoch 16/50\n",
      "3540/3540 [==============================] - 200s 56ms/step - loss: 0.1674 - accuracy: 0.7471 - val_loss: 0.1875 - val_accuracy: 0.7081\n",
      "Epoch 17/50\n",
      "learning rate sheduled from 0.0004493287415243685 to 0.0004065694229211658\n",
      "3540/3540 [==============================] - 199s 55ms/step - loss: 0.1636 - accuracy: 0.7539 - val_loss: 0.1845 - val_accuracy: 0.7132\n",
      "Epoch 18/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1602 - accuracy: 0.7605 - val_loss: 0.1820 - val_accuracy: 0.7167\n",
      "Epoch 19/50\n",
      "learning rate sheduled from 0.0004065694229211658 to 0.00036787919816561043\n",
      "3540/3540 [==============================] - 202s 56ms/step - loss: 0.1561 - accuracy: 0.7685 - val_loss: 0.1811 - val_accuracy: 0.7218\n",
      "Epoch 20/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1526 - accuracy: 0.7758 - val_loss: 0.1795 - val_accuracy: 0.7265\n",
      "Epoch 21/50\n",
      "learning rate sheduled from 0.00036787919816561043 to 0.0003328708407934755\n",
      "3540/3540 [==============================] - 204s 57ms/step - loss: 0.1493 - accuracy: 0.7815 - val_loss: 0.1786 - val_accuracy: 0.7290\n",
      "Epoch 22/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1464 - accuracy: 0.7870 - val_loss: 0.1758 - val_accuracy: 0.7333\n",
      "Epoch 23/50\n",
      "learning rate sheduled from 0.0003328708407934755 to 0.00030119396978989244\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1424 - accuracy: 0.7955 - val_loss: 0.1742 - val_accuracy: 0.7357\n",
      "Epoch 24/50\n",
      "3540/3540 [==============================] - 205s 57ms/step - loss: 0.1405 - accuracy: 0.7973 - val_loss: 0.1741 - val_accuracy: 0.7358\n",
      "Epoch 25/50\n",
      "learning rate sheduled from 0.00030119396978989244 to 0.000272531557129696\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1377 - accuracy: 0.8031 - val_loss: 0.1704 - val_accuracy: 0.7443\n",
      "Epoch 26/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1350 - accuracy: 0.8068 - val_loss: 0.1716 - val_accuracy: 0.7424\n",
      "Epoch 27/50\n",
      "learning rate sheduled from 0.000272531557129696 to 0.0002465967263560742\n",
      "3540/3540 [==============================] - 202s 56ms/step - loss: 0.1319 - accuracy: 0.8123 - val_loss: 0.1719 - val_accuracy: 0.7432\n",
      "Epoch 28/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1301 - accuracy: 0.8158 - val_loss: 0.1702 - val_accuracy: 0.7455\n",
      "Epoch 29/50\n",
      "learning rate sheduled from 0.0002465967263560742 to 0.0002231299295090139\n",
      "3540/3540 [==============================] - 204s 57ms/step - loss: 0.1281 - accuracy: 0.8194 - val_loss: 0.1695 - val_accuracy: 0.7482\n",
      "Epoch 30/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1259 - accuracy: 0.8235 - val_loss: 0.1694 - val_accuracy: 0.7491\n",
      "Epoch 31/50\n",
      "learning rate sheduled from 0.0002231299295090139 to 0.0002018962986767292\n",
      "3540/3540 [==============================] - 202s 56ms/step - loss: 0.1238 - accuracy: 0.8264 - val_loss: 0.1687 - val_accuracy: 0.7508\n",
      "Epoch 32/50\n",
      "3540/3540 [==============================] - 208s 58ms/step - loss: 0.1220 - accuracy: 0.8297 - val_loss: 0.1678 - val_accuracy: 0.7520\n",
      "Epoch 33/50\n",
      "learning rate sheduled from 0.0002018962986767292 to 0.00018268331768922508\n",
      "3540/3540 [==============================] - 199s 55ms/step - loss: 0.1202 - accuracy: 0.8334 - val_loss: 0.1689 - val_accuracy: 0.7515\n",
      "Epoch 34/50\n",
      "3540/3540 [==============================] - 204s 57ms/step - loss: 0.1191 - accuracy: 0.8346 - val_loss: 0.1684 - val_accuracy: 0.7523\n",
      "Epoch 35/50\n",
      "learning rate sheduled from 0.00018268331768922508 to 0.00016529869753867388\n",
      "3540/3540 [==============================] - 199s 55ms/step - loss: 0.1169 - accuracy: 0.8383 - val_loss: 0.1690 - val_accuracy: 0.7518\n",
      "Epoch 36/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1159 - accuracy: 0.8401 - val_loss: 0.1690 - val_accuracy: 0.7520\n",
      "Epoch 37/50\n",
      "learning rate sheduled from 0.00016529869753867388 to 0.00014956844097469002\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1145 - accuracy: 0.8428 - val_loss: 0.1667 - val_accuracy: 0.7564\n",
      "Epoch 38/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1135 - accuracy: 0.8446 - val_loss: 0.1669 - val_accuracy: 0.7556\n",
      "Epoch 39/50\n",
      "learning rate sheduled from 0.00014956844097469002 to 0.0001353351108264178\n",
      "3540/3540 [==============================] - 200s 56ms/step - loss: 0.1120 - accuracy: 0.8471 - val_loss: 0.1661 - val_accuracy: 0.7588\n",
      "Epoch 40/50\n",
      "3540/3540 [==============================] - 204s 57ms/step - loss: 0.1106 - accuracy: 0.8499 - val_loss: 0.1671 - val_accuracy: 0.7587\n",
      "Epoch 41/50\n",
      "learning rate sheduled from 0.0001353351108264178 to 0.00012245627294760197\n",
      "3540/3540 [==============================] - 199s 56ms/step - loss: 0.1093 - accuracy: 0.8518 - val_loss: 0.1674 - val_accuracy: 0.7571\n",
      "Epoch 42/50\n",
      "3540/3540 [==============================] - 198s 55ms/step - loss: 0.1086 - accuracy: 0.8534 - val_loss: 0.1671 - val_accuracy: 0.7581\n",
      "Epoch 43/50\n",
      "learning rate sheduled from 0.00012245627294760197 to 0.00011080301192123443\n",
      "3540/3540 [==============================] - 199s 55ms/step - loss: 0.1072 - accuracy: 0.8564 - val_loss: 0.1690 - val_accuracy: 0.7566\n",
      "Epoch 44/50\n",
      "3540/3540 [==============================] - 200s 56ms/step - loss: 0.1065 - accuracy: 0.8569 - val_loss: 0.1669 - val_accuracy: 0.7595\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6, l2_norm=config.l2_norm)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 2 == 0:\n",
    "        lr_new = lr * tf.math.exp(-0.1)\n",
    "        print(f\"learning rate sheduled from {lr} to {lr_new}\")\n",
    "        return lr_new\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = config.learning_rate)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[WandbCallback(), tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\"), tf.keras.callbacks.LearningRateScheduler(scheduler)])\n",
    "\n",
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows Bin 1000 - Vect 512 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 512,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows Gray 1500 - Vect 1024 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschauppi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3mk7zc98\" target=\"_blank\">glamorous-water-189</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows Gray 1500 - Vect 2048 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3dbijygp\" target=\"_blank\">drawn-thunder-191</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 407s 221ms/step - loss: 0.2132 - accuracy: 0.6513 - val_loss: 0.1962 - val_accuracy: 0.6982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.1856 - accuracy: 0.7199 - val_loss: 0.1723 - val_accuracy: 0.7459\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1711 - accuracy: 0.7459 - val_loss: 0.1670 - val_accuracy: 0.7555\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1623 - accuracy: 0.7623 - val_loss: 0.1642 - val_accuracy: 0.7556\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1514 - accuracy: 0.7818 - val_loss: 0.1452 - val_accuracy: 0.7929\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1395 - accuracy: 0.8023 - val_loss: 0.1351 - val_accuracy: 0.8094\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1263 - accuracy: 0.8249 - val_loss: 0.1250 - val_accuracy: 0.8268\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1127 - accuracy: 0.8466 - val_loss: 0.1077 - val_accuracy: 0.8530\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.1007 - accuracy: 0.8649 - val_loss: 0.1036 - val_accuracy: 0.8596\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0905 - accuracy: 0.8806 - val_loss: 0.0935 - val_accuracy: 0.8770\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0812 - accuracy: 0.8943 - val_loss: 0.0879 - val_accuracy: 0.8845\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.0730 - accuracy: 0.9064 - val_loss: 0.0860 - val_accuracy: 0.8859\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0653 - accuracy: 0.9172 - val_loss: 0.0744 - val_accuracy: 0.9047\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0598 - accuracy: 0.9251 - val_loss: 0.0711 - val_accuracy: 0.9090\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0542 - accuracy: 0.9329 - val_loss: 0.0691 - val_accuracy: 0.9131\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.0495 - accuracy: 0.9393 - val_loss: 0.0666 - val_accuracy: 0.9158\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0457 - accuracy: 0.9449 - val_loss: 0.0647 - val_accuracy: 0.9183\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0419 - accuracy: 0.9501 - val_loss: 0.0648 - val_accuracy: 0.9182\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0385 - accuracy: 0.9548 - val_loss: 0.0615 - val_accuracy: 0.9229\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0356 - accuracy: 0.9587 - val_loss: 0.0630 - val_accuracy: 0.9210\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0333 - accuracy: 0.9613 - val_loss: 0.0622 - val_accuracy: 0.9230\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.0305 - accuracy: 0.9646 - val_loss: 0.0626 - val_accuracy: 0.9229\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0283 - accuracy: 0.9679 - val_loss: 0.0660 - val_accuracy: 0.9169\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0270 - accuracy: 0.9692 - val_loss: 0.0592 - val_accuracy: 0.9268\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0251 - accuracy: 0.9718 - val_loss: 0.0602 - val_accuracy: 0.9262\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows Bin 1500 - Vect 2048 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250K_224_224_1500_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250K_224_224_1500_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_224_224_1500_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_224_224_1500_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschauppi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/js/workspace/thesis_schaupp/HTSR_Handwritten_Text_Similarity_Recognition/wandb/run-20220302_122654-bluwn3wc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/bluwn3wc\" target=\"_blank\">absurd-blaze-209</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows Bin 1500 - Vect 1024 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250K_224_224_1500_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250K_224_224_1500_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_224_224_1500_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_224_224_1500_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschauppi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/js/workspace/thesis_schaupp/HTSR_Handwritten_Text_Similarity_Recognition/wandb/run-20220302_135558-2wf91upe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2wf91upe\" target=\"_blank\">eager-voice-220</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    <ipython-input-4-572c06eae5e1>:37 call  *\n        x = self.gamma * x\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:1074 _run_op\n        return tensor_oper(a.value(), *args, **kwargs)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n        raise e\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n        return func(x, y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1496 _mul_dispatch\n        return multiply(x, y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:518 multiply\n        return gen_math_ops.mul(x, y, name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:6078 mul\n        \"Mul\", x=x, y=y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n        inferred_from[input_arg.type_attr]))\n\n    TypeError: Input 'y' of 'Mul' Op has type float16 that does not match type float32 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5b71f04b4d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_convnext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_scale_init_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mleft_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mright_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0f165846b765>\u001b[0m in \u001b[0;36mcreate_convnext_model\u001b[0;34m(input_shape, depths, dims, emb_dim, drop_path, layer_scale_init_value)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvNeXt_Block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_scale_init_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# downsample + res3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 952\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1091\u001b[0;31m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    <ipython-input-4-572c06eae5e1>:37 call  *\n        x = self.gamma * x\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:1074 _run_op\n        return tensor_oper(a.value(), *args, **kwargs)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n        raise e\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n        return func(x, y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1496 _mul_dispatch\n        return multiply(x, y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:518 multiply\n        return gen_math_ops.mul(x, y, name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:6078 mul\n        \"Mul\", x=x, y=y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n        inferred_from[input_arg.type_attr]))\n\n    TypeError: Input 'y' of 'Mul' Op has type float16 that does not match type float32 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

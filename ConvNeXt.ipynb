{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from helper_functions import get_classification_report\n",
    "from helper_functions import create_tf_data_datasets_contrastive\n",
    "from helper_functions import create_tf_data_testset_contrastive\n",
    "from helper_functions import euclidean_distance\n",
    "from helper_functions import manhattan_distance\n",
    "from helper_functions import contrastive_loss\n",
    "from helper_functions import l2Norm\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "class ConvNeXt_Block(layers.Layer):\n",
    "    r\"\"\" ConvNeXt Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = layers.DepthwiseConv2D(kernel_size=7, padding='same')  # depthwise conv\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.pwconv1 = layers.Dense(4 * dim)\n",
    "        self.act = layers.Activation('gelu')\n",
    "        self.pwconv2 = layers.Dense(dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.dim = dim\n",
    "        self.layer_scale_init_value = layer_scale_init_value\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            initial_value=self.layer_scale_init_value * tf.ones((self.dim)),\n",
    "            trainable=True,\n",
    "            name='_gamma')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            #self.gamma = tf.cast(self.gamma, dtype='float16')\n",
    "            x = self.gamma * x\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Downsample_Block(layers.Layer):\n",
    "    \"\"\"The Downsample Block in ConvNeXt\n",
    "        Args:\n",
    "            dim (int): number of channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.LN = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.conv = layers.Conv2D(dim, kernel_size=2, strides=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.LN(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class DropPath(tf.keras.layers.Layer):\n",
    "    \"\"\"The Drop path in ConvNeXt\n",
    "        Reference:\n",
    "            https://github.com/rishigami/Swin-Transformer-TF/blob/main/swintransformer/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        return self._drop_path(x, self.drop_prob, training)\n",
    "\n",
    "\n",
    "    def _drop_path(self, inputs, drop_prob, is_training):\n",
    "        if (not is_training) or (drop_prob == 0.):\n",
    "            return inputs\n",
    "\n",
    "        # Compute keep_prob\n",
    "        keep_prob = 1.0 - drop_prob\n",
    "\n",
    "        # Compute drop_connect tensor\n",
    "        random_tensor = keep_prob\n",
    "        shape = (tf.shape(inputs)[0],) + (1,) * (len(tf.shape(inputs)) - 1)\n",
    "        random_tensor += tf.random.uniform(shape, dtype=inputs.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def create_convnext_model(input_shape=(224, 224, 3), depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], emb_dim=1024, drop_path=0., layer_scale_init_value=1e-6, l2_norm=False):\n",
    "    \"\"\" Function to construct the ConvNeXt Model\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): (Width, Height , Channels)\n",
    "            depths (list): a list of size 4. denoting each stage's depth\n",
    "            dims (list): a list of size 4. denoting number of kernel's in each stage\n",
    "            num_classes (int): the number of classes\n",
    "            drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "            layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        Returns:\n",
    "            ConvNeXt model: an instance of tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    assert (len(depths) == 4 and len(dims) ==4), \"Must provide exactly 4 depths and 4 dims\"\n",
    "    assert (len(input_shape) == 3), \"Input shape must be (W, H, C)\"\n",
    "\n",
    "    input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Stem + res2\n",
    "    y = layers.Conv2D(dims[0], kernel_size=4, strides=4)(input)\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    for i in range(depths[0]):\n",
    "        y = ConvNeXt_Block(dims[0], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res3\n",
    "    y = Downsample_Block(dims[1])(y)\n",
    "    for i in range(depths[1]):\n",
    "        y = ConvNeXt_Block(dims[1], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res4\n",
    "    y = Downsample_Block(dims[2])(y)\n",
    "    for i in range(depths[2]):\n",
    "        y = ConvNeXt_Block(dims[2], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res5\n",
    "    y = Downsample_Block(dims[3])(y)\n",
    "    for i in range(depths[3]):\n",
    "        y = ConvNeXt_Block(dims[3], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    y = layers.GlobalAveragePooling2D()(y)\n",
    "    # final norm layer\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    # Head\n",
    "\n",
    "    if l2_norm == True:\n",
    "        y = layers.Lambda(l2Norm)(y)\n",
    "        y = layers.Dense(emb_dim, activation='relu')(y)\n",
    "    else:\n",
    "        y = layers.Dense(emb_dim, activation='relu')(y)\n",
    "\n",
    "    return tf.keras.Model(inputs=input, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows RGB 1000 - Vect 1024 -  Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 50,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/50\n1770/1770 [==============================] - 464s 230ms/step - loss: 0.1972 - accuracy: 0.6766 - val_loss: 0.1379 - val_accuracy: 0.8206\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/50\n1770/1770 [==============================] - 414s 227ms/step - loss: 0.1297 - accuracy: 0.8335 - val_loss: 0.1120 - val_accuracy: 0.8517\nEpoch 3/50\n1770/1770 [==============================] - 406s 226ms/step - loss: 0.1019 - accuracy: 0.8703 - val_loss: 0.0922 - val_accuracy: 0.8819\nEpoch 4/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0891 - accuracy: 0.8850 - val_loss: 0.0784 - val_accuracy: 0.9002\nEpoch 5/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0808 - accuracy: 0.8948 - val_loss: 0.0776 - val_accuracy: 0.8996\nEpoch 6/50\n1770/1770 [==============================] - 406s 227ms/step - loss: 0.0743 - accuracy: 0.9040 - val_loss: 0.0704 - val_accuracy: 0.9103\nEpoch 7/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0683 - accuracy: 0.9127 - val_loss: 0.0605 - val_accuracy: 0.9243\nEpoch 8/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0621 - accuracy: 0.9209 - val_loss: 0.0654 - val_accuracy: 0.9169\nEpoch 9/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0556 - accuracy: 0.9303 - val_loss: 0.0544 - val_accuracy: 0.9306\nEpoch 10/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0505 - accuracy: 0.9376 - val_loss: 0.0572 - val_accuracy: 0.9273\nEpoch 11/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0462 - accuracy: 0.9434 - val_loss: 0.0484 - val_accuracy: 0.9395\nEpoch 12/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0423 - accuracy: 0.9484 - val_loss: 0.0484 - val_accuracy: 0.9395\nEpoch 13/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0395 - accuracy: 0.9527 - val_loss: 0.0447 - val_accuracy: 0.9444\nEpoch 14/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0360 - accuracy: 0.9565 - val_loss: 0.0407 - val_accuracy: 0.9499\nEpoch 15/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0333 - accuracy: 0.9603 - val_loss: 0.0422 - val_accuracy: 0.9479\nEpoch 16/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0308 - accuracy: 0.9638 - val_loss: 0.0416 - val_accuracy: 0.9481\nEpoch 17/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0285 - accuracy: 0.9668 - val_loss: 0.0391 - val_accuracy: 0.9530\nEpoch 18/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0261 - accuracy: 0.9699 - val_loss: 0.0372 - val_accuracy: 0.9553\nEpoch 19/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0249 - accuracy: 0.9716 - val_loss: 0.0392 - val_accuracy: 0.9517\nEpoch 20/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0238 - accuracy: 0.9729 - val_loss: 0.0359 - val_accuracy: 0.9570\nEpoch 21/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0216 - accuracy: 0.9757 - val_loss: 0.0374 - val_accuracy: 0.9539\nEpoch 22/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0203 - accuracy: 0.9773 - val_loss: 0.0428 - val_accuracy: 0.9468\nEpoch 23/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0195 - accuracy: 0.9782 - val_loss: 0.0385 - val_accuracy: 0.9534\nEpoch 24/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0181 - accuracy: 0.9799 - val_loss: 0.0358 - val_accuracy: 0.9565\nEpoch 25/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0167 - accuracy: 0.9819 - val_loss: 0.0363 - val_accuracy: 0.9557\nEpoch 26/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0121 - accuracy: 0.9875 - val_loss: 0.0318 - val_accuracy: 0.9618\nEpoch 27/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0105 - accuracy: 0.9897 - val_loss: 0.0323 - val_accuracy: 0.9616\nEpoch 28/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0093 - accuracy: 0.9909 - val_loss: 0.0321 - val_accuracy: 0.9616\nEpoch 29/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0088 - accuracy: 0.9914 - val_loss: 0.0324 - val_accuracy: 0.9612\nEpoch 30/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0082 - accuracy: 0.9920 - val_loss: 0.0317 - val_accuracy: 0.9621\nEpoch 31/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0074 - accuracy: 0.9928 - val_loss: 0.0331 - val_accuracy: 0.9605\nEpoch 32/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0072 - accuracy: 0.9931 - val_loss: 0.0326 - val_accuracy: 0.9611\nEpoch 33/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0070 - accuracy: 0.9933 - val_loss: 0.0325 - val_accuracy: 0.9615\nEpoch 34/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0071 - accuracy: 0.9932 - val_loss: 0.0327 - val_accuracy: 0.9613\nEpoch 35/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0067 - accuracy: 0.9936 - val_loss: 0.0327 - val_accuracy: 0.9614\nEpoch 36/50\n1770/1770 [==============================] - 406s 227ms/step - loss: 0.0068 - accuracy: 0.9935 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 37/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0069 - accuracy: 0.9933 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 38/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0065 - accuracy: 0.9938 - val_loss: 0.0326 - val_accuracy: 0.9612\nEpoch 39/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0068 - accuracy: 0.9935 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 40/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0064 - accuracy: 0.9939 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 41/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0068 - accuracy: 0.9935 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 42/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0065 - accuracy: 0.9938 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 43/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0065 - accuracy: 0.9937 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 44/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0068 - accuracy: 0.9935 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 45/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0065 - accuracy: 0.9938 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 46/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0066 - accuracy: 0.9936 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 47/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0066 - accuracy: 0.9938 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 48/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0066 - accuracy: 0.9937 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 49/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0067 - accuracy: 0.9936 - val_loss: 0.0326 - val_accuracy: 0.9613\nEpoch 50/50\n1770/1770 [==============================] - 405s 226ms/step - loss: 0.0063 - accuracy: 0.9940 - val_loss: 0.0326 - val_accuracy: 0.9613\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows Gray 1000 - Vect 1024 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2ggsgp9k\" target=\"_blank\">hopeful-paper-160</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 409s 221ms/step - loss: 0.2089 - accuracy: 0.6588 - val_loss: 0.1921 - val_accuracy: 0.6997\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1854 - accuracy: 0.7159 - val_loss: 0.1692 - val_accuracy: 0.7565\nEpoch 3/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1622 - accuracy: 0.7629 - val_loss: 0.1552 - val_accuracy: 0.7739\nEpoch 4/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.1477 - accuracy: 0.7865 - val_loss: 0.1468 - val_accuracy: 0.7867\nEpoch 5/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1371 - accuracy: 0.8044 - val_loss: 0.1338 - val_accuracy: 0.8105\nEpoch 6/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.1266 - accuracy: 0.8222 - val_loss: 0.1268 - val_accuracy: 0.8224\nEpoch 7/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1158 - accuracy: 0.8398 - val_loss: 0.1146 - val_accuracy: 0.8404\nEpoch 8/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1054 - accuracy: 0.8554 - val_loss: 0.1069 - val_accuracy: 0.8533\nEpoch 9/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0952 - accuracy: 0.8722 - val_loss: 0.0952 - val_accuracy: 0.8713\nEpoch 10/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.0857 - accuracy: 0.8864 - val_loss: 0.0893 - val_accuracy: 0.8787\nEpoch 11/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0773 - accuracy: 0.8984 - val_loss: 0.0867 - val_accuracy: 0.8830\nEpoch 12/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0696 - accuracy: 0.9092 - val_loss: 0.0759 - val_accuracy: 0.8991\nEpoch 13/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0625 - accuracy: 0.9196 - val_loss: 0.0745 - val_accuracy: 0.9016\nEpoch 14/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0551 - accuracy: 0.9310 - val_loss: 0.0675 - val_accuracy: 0.9113\nEpoch 15/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0489 - accuracy: 0.9397 - val_loss: 0.0638 - val_accuracy: 0.9174\nEpoch 16/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0430 - accuracy: 0.9479 - val_loss: 0.0667 - val_accuracy: 0.9124\nEpoch 17/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0380 - accuracy: 0.9548 - val_loss: 0.0612 - val_accuracy: 0.9199\nEpoch 18/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0331 - accuracy: 0.9617 - val_loss: 0.0695 - val_accuracy: 0.9082\nEpoch 19/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0291 - accuracy: 0.9667 - val_loss: 0.0658 - val_accuracy: 0.9154\nEpoch 20/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0259 - accuracy: 0.9707 - val_loss: 0.0600 - val_accuracy: 0.9227\nEpoch 21/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0225 - accuracy: 0.9753 - val_loss: 0.0655 - val_accuracy: 0.9140\nEpoch 22/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0208 - accuracy: 0.9772 - val_loss: 0.0604 - val_accuracy: 0.9226\nEpoch 23/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.0191 - accuracy: 0.9791 - val_loss: 0.0635 - val_accuracy: 0.9189\nEpoch 24/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0177 - accuracy: 0.9808 - val_loss: 0.0627 - val_accuracy: 0.9207\nEpoch 25/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0168 - accuracy: 0.9817 - val_loss: 0.0644 - val_accuracy: 0.9177\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transfer Learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_500k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_500k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/sayakpaul/convnext_tiny_1k_224_fe/1\", trainable=False),\n",
    "    keras.layers.Dense(2048, activation=\"relu\")\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "model.build(input_shape=(None, 224, 224, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "left_input = layers.Input(shape=(224, 224, channels))\n",
    "right_input = layers.Input(shape=(224, 224, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_11\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_24 (InputLayer)           [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\ninput_25 (InputLayer)           [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nsequential_2 (Sequential)       (None, 2048)         29395040    input_24[0][0]                   \n                                                                 input_25[0][0]                   \n__________________________________________________________________________________________________\nlambda_7 (Lambda)               (None, 1)            0           sequential_2[0][0]               \n                                                                 sequential_2[1][0]               \n__________________________________________________________________________________________________\ndense_136 (Dense)               (None, 1)            2           lambda_7[0][0]                   \n==================================================================================================\nTotal params: 29,395,042\nTrainable params: 1,574,914\nNon-trainable params: 27,820,128\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "siamese_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pretraining / Finetuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:jx1ga8ks) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a15a38d784d742eca5e6d35980635fb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">rich-firefly-379</strong>: <a href=\"https://wandb.ai/schauppi/Architecture_1/runs/jx1ga8ks\" target=\"_blank\">https://wandb.ai/schauppi/Architecture_1/runs/jx1ga8ks</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20220321_193226-jx1ga8ks/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:jx1ga8ks). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/js/workspace/thesis_schaupp/HTSR_Handwritten_Text_Similarity_Recognition/wandb/run-20220321_193409-817b0rsz</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/817b0rsz\" target=\"_blank\">golden-butterfly-380</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/38\n",
      "3540/3540 [==============================] - 217s 60ms/step - loss: 0.2326 - accuracy: 0.5877 - val_loss: 0.1737 - val_accuracy: 0.7503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/38\n",
      "  30/3540 [..............................] - ETA: 2:47 - loss: 0.1775 - accuracy: 0.7517"
     ]
    }
   ],
   "source": [
    "#CVL DATASET\n",
    "\n",
    "anchor_images_path = \"npz_datasets/pairs_cvl_500k_224_224_rows_1000_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_cvl_500k_224_224_rows_1000_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_cvl_224_224_rows_1000_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_cvl_224_224_rows_1000_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)\n",
    "\n",
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [1,1,1,1],\n",
    "                         \"conv_dims\": [24, 24, 48, 48],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"ConvNeXt - CVL - Bin\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=config.otimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})\n",
    "\n",
    "run.finish()\n",
    "\n",
    "#DATASET\n",
    "\n",
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.001,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [1,1,1,1],\n",
    "                         \"conv_dims\": [24, 24, 48, 48],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 38,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"l2_norm\": False,\n",
    "                         \"architecture\": \"ConvNeXt - 1m - Bin\"})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "anchor_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000_bin/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000_bin/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000_bin/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000_bin/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)\n",
    "\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=config.otimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n",
    "\n",
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})\n",
    "\n",
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 90K Rows RGB 1000 - Vect 2048 - Contrastive Eucledian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/anchor\"\npositive_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000/positive\"\nwidth, height, channels = 113, 113, 3\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/js/workspace/thesis_schaupp/HTSR_Handwritten_Text_Similarity_Recognition/wandb/run-20220318_081000-3r2cod22</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3r2cod22\" target=\"_blank\">rare-glade-353</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/38\n7080/7080 [==============================] - 574s 80ms/step - loss: 0.2022 - accuracy: 0.6726 - val_loss: 0.1560 - val_accuracy: 0.7737\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/38\n7080/7080 [==============================] - 570s 80ms/step - loss: 0.1531 - accuracy: 0.7767 - val_loss: 0.1449 - val_accuracy: 0.7882\nEpoch 3/38\n7080/7080 [==============================] - 569s 80ms/step - loss: 0.1329 - accuracy: 0.8114 - val_loss: 0.1108 - val_accuracy: 0.8478\nEpoch 4/38\n7080/7080 [==============================] - 569s 80ms/step - loss: 0.1063 - accuracy: 0.8546 - val_loss: 0.0962 - val_accuracy: 0.8699\nEpoch 5/38\n1879/7080 [======>.......................] - ETA: 6:11 - loss: 0.0900 - accuracy: 0.8792"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.001,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [1,1,1,1],\n                         \"conv_dims\": [48, 96, 192, 192],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 38,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 2048,\n                         \"image_size\": \"113x113x3\",\n                         \"l2_norm\": False,\n                         \"architecture\": \"ConvNeXt - 1m - Gray\"})\n\nconfig = wandb.config\n\nmodel = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=config.otimizer, metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n\nprecision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})\n\nrun.finish()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "run.finish()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows RGB 1000 - Vect 512 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\npositive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\nwidth, height, channels = 113, 113, 3\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/70e8ta6h\" target=\"_blank\">rare-leaf-171</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 512,\n                         \"image_size\": \"113x113x3\",\n                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 428s 232ms/step - loss: 0.1779 - accuracy: 0.7417 - val_loss: 0.1400 - val_accuracy: 0.8237\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 412s 230ms/step - loss: 0.1296 - accuracy: 0.8275 - val_loss: 0.1158 - val_accuracy: 0.8467\nEpoch 3/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.1087 - accuracy: 0.8578 - val_loss: 0.0960 - val_accuracy: 0.8779\nEpoch 4/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0956 - accuracy: 0.8752 - val_loss: 0.0913 - val_accuracy: 0.8802\nEpoch 5/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0880 - accuracy: 0.8849 - val_loss: 0.0831 - val_accuracy: 0.8922\nEpoch 6/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0823 - accuracy: 0.8929 - val_loss: 0.0793 - val_accuracy: 0.8969\nEpoch 7/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0779 - accuracy: 0.8989 - val_loss: 0.0783 - val_accuracy: 0.8969\nEpoch 8/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0731 - accuracy: 0.9053 - val_loss: 0.0719 - val_accuracy: 0.9074\nEpoch 9/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0697 - accuracy: 0.9098 - val_loss: 0.0657 - val_accuracy: 0.9158\nEpoch 10/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0660 - accuracy: 0.9155 - val_loss: 0.0639 - val_accuracy: 0.9178\nEpoch 11/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0622 - accuracy: 0.9210 - val_loss: 0.0659 - val_accuracy: 0.9150\nEpoch 12/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0594 - accuracy: 0.9247 - val_loss: 0.0598 - val_accuracy: 0.9241\nEpoch 13/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0559 - accuracy: 0.9297 - val_loss: 0.0588 - val_accuracy: 0.9254\nEpoch 14/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0539 - accuracy: 0.9322 - val_loss: 0.0568 - val_accuracy: 0.9274\nEpoch 15/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0511 - accuracy: 0.9367 - val_loss: 0.0536 - val_accuracy: 0.9324\nEpoch 16/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0487 - accuracy: 0.9393 - val_loss: 0.0519 - val_accuracy: 0.9341\nEpoch 17/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0467 - accuracy: 0.9420 - val_loss: 0.0522 - val_accuracy: 0.9338\nEpoch 18/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0444 - accuracy: 0.9451 - val_loss: 0.0502 - val_accuracy: 0.9366\nEpoch 19/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0418 - accuracy: 0.9490 - val_loss: 0.0535 - val_accuracy: 0.9319\nEpoch 20/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0400 - accuracy: 0.9510 - val_loss: 0.0474 - val_accuracy: 0.9400\nEpoch 21/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0379 - accuracy: 0.9539 - val_loss: 0.0482 - val_accuracy: 0.9389\nEpoch 22/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0361 - accuracy: 0.9563 - val_loss: 0.0447 - val_accuracy: 0.9447\nEpoch 23/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0343 - accuracy: 0.9589 - val_loss: 0.0456 - val_accuracy: 0.9429\nEpoch 24/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.0328 - accuracy: 0.9608 - val_loss: 0.0450 - val_accuracy: 0.9438\nEpoch 25/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.0311 - accuracy: 0.9632 - val_loss: 0.0443 - val_accuracy: 0.9447\n"
    }
   ],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows RGB 1000 - Vect 256 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\npositive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\nwidth, height, channels = 113, 113, 3\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 256,\n                         \"image_size\": \"113x113x3\",\n                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 428s 232ms/step - loss: 0.2501 - accuracy: 0.4968 - val_loss: 0.2500 - val_accuracy: 0.5027\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 412s 230ms/step - loss: 0.2500 - accuracy: 0.5012 - val_loss: 0.2500 - val_accuracy: 0.4973\nEpoch 3/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.4999 - val_loss: 0.2500 - val_accuracy: 0.5027\nEpoch 4/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5002 - val_loss: 0.2500 - val_accuracy: 0.5027\nEpoch 5/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.4993 - val_loss: 0.2500 - val_accuracy: 0.5027\nEpoch 6/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5008 - val_loss: 0.2500 - val_accuracy: 0.4973\nEpoch 7/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5007 - val_loss: 0.2500 - val_accuracy: 0.4973\nEpoch 8/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5016 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 9/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5020 - val_loss: 0.2500 - val_accuracy: 0.5008\nEpoch 10/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5035 - val_loss: 0.2500 - val_accuracy: 0.5010\nEpoch 11/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5030 - val_loss: 0.2500 - val_accuracy: 0.4978\nEpoch 12/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5069 - val_loss: 0.2500 - val_accuracy: 0.4973\nEpoch 13/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5078 - val_loss: 0.2500 - val_accuracy: 0.4971\nEpoch 14/25\n1770/1770 [==============================] - 412s 230ms/step - loss: 0.2499 - accuracy: 0.5083 - val_loss: 0.2500 - val_accuracy: 0.4977\nEpoch 15/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5086 - val_loss: 0.2501 - val_accuracy: 0.4969\nEpoch 16/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5083 - val_loss: 0.2501 - val_accuracy: 0.4974\nEpoch 17/25\n1770/1770 [==============================] - 412s 230ms/step - loss: 0.2499 - accuracy: 0.5093 - val_loss: 0.2501 - val_accuracy: 0.4982\nEpoch 18/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5096 - val_loss: 0.2501 - val_accuracy: 0.4987\nEpoch 19/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5094 - val_loss: 0.2501 - val_accuracy: 0.4981\nEpoch 20/25\n1770/1770 [==============================] - 414s 231ms/step - loss: 0.2499 - accuracy: 0.5098 - val_loss: 0.2501 - val_accuracy: 0.4980\nEpoch 21/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5095 - val_loss: 0.2501 - val_accuracy: 0.4987\nEpoch 22/25\n1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5102 - val_loss: 0.2501 - val_accuracy: 0.4984\nEpoch 23/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5099 - val_loss: 0.2501 - val_accuracy: 0.4986\nEpoch 24/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5096 - val_loss: 0.2501 - val_accuracy: 0.4989\nEpoch 25/25\n1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5097 - val_loss: 0.2501 - val_accuracy: 0.4990\n"
    }
   ],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m wandb uses only the first 10000 datapoints to create the plots.\n"
    }
   ],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows RGB 1000 - Vect 2048 - Big - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\npositive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\nwidth, height, channels = 113, 113, 3\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/11fqvfs4\" target=\"_blank\">swept-waterfall-177</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [96, 192, 384, 768],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 2048,\n                         \"image_size\": \"113x113x3\",\n                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 774s 402ms/step - loss: 0.1769 - accuracy: 0.7403 - val_loss: 0.1422 - val_accuracy: 0.8244\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 715s 400ms/step - loss: 0.1284 - accuracy: 0.8310 - val_loss: 0.1170 - val_accuracy: 0.8478\nEpoch 3/25\n1770/1770 [==============================] - 710s 398ms/step - loss: 0.1117 - accuracy: 0.8519 - val_loss: 0.1093 - val_accuracy: 0.8518\nEpoch 4/25\n1770/1770 [==============================] - 709s 398ms/step - loss: 0.1038 - accuracy: 0.8623 - val_loss: 0.1094 - val_accuracy: 0.8532\nEpoch 5/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0981 - accuracy: 0.8706 - val_loss: 0.0942 - val_accuracy: 0.8757\nEpoch 6/25\n1770/1770 [==============================] - 708s 397ms/step - loss: 0.0941 - accuracy: 0.8756 - val_loss: 0.0944 - val_accuracy: 0.8746\nEpoch 7/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0901 - accuracy: 0.8816 - val_loss: 0.0852 - val_accuracy: 0.8894\nEpoch 8/25\n1770/1770 [==============================] - 707s 397ms/step - loss: 0.0865 - accuracy: 0.8871 - val_loss: 0.0860 - val_accuracy: 0.8869\nEpoch 9/25\n1770/1770 [==============================] - 708s 397ms/step - loss: 0.0833 - accuracy: 0.8914 - val_loss: 0.0814 - val_accuracy: 0.8940\nEpoch 10/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0802 - accuracy: 0.8962 - val_loss: 0.0851 - val_accuracy: 0.8893\nEpoch 11/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0771 - accuracy: 0.9005 - val_loss: 0.0779 - val_accuracy: 0.8987\nEpoch 12/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0740 - accuracy: 0.9045 - val_loss: 0.0774 - val_accuracy: 0.8992\nEpoch 13/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0709 - accuracy: 0.9091 - val_loss: 0.0729 - val_accuracy: 0.9061\nEpoch 14/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0680 - accuracy: 0.9137 - val_loss: 0.0734 - val_accuracy: 0.9041\nEpoch 15/25\n1770/1770 [==============================] - 707s 397ms/step - loss: 0.0655 - accuracy: 0.9165 - val_loss: 0.0717 - val_accuracy: 0.9075\nEpoch 16/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0631 - accuracy: 0.9200 - val_loss: 0.0738 - val_accuracy: 0.9047\nEpoch 17/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0604 - accuracy: 0.9238 - val_loss: 0.0727 - val_accuracy: 0.9067\nEpoch 18/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0576 - accuracy: 0.9274 - val_loss: 0.0712 - val_accuracy: 0.9069\nEpoch 19/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0552 - accuracy: 0.9308 - val_loss: 0.0687 - val_accuracy: 0.9130\nEpoch 20/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0522 - accuracy: 0.9352 - val_loss: 0.0699 - val_accuracy: 0.9108\nEpoch 21/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0495 - accuracy: 0.9385 - val_loss: 0.0710 - val_accuracy: 0.9082\nEpoch 22/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0469 - accuracy: 0.9422 - val_loss: 0.0663 - val_accuracy: 0.9157\nEpoch 23/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0448 - accuracy: 0.9447 - val_loss: 0.0720 - val_accuracy: 0.9081\nEpoch 24/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0418 - accuracy: 0.9491 - val_loss: 0.0671 - val_accuracy: 0.9152\nEpoch 25/25\n1770/1770 [==============================] - 708s 398ms/step - loss: 0.0397 - accuracy: 0.9514 - val_loss: 0.0721 - val_accuracy: 0.9074\n"
    }
   ],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "run.finish()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows Gray 1000 - Vect 2048 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\npositive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\nwidth, height, channels = 113, 113, 1\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2lcm2eri\" target=\"_blank\">ethereal-vortex-180</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 2048,\n                         \"image_size\": \"113x113x1\",\n                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 408s 221ms/step - loss: 0.2102 - accuracy: 0.6578 - val_loss: 0.1930 - val_accuracy: 0.7092\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 391s 220ms/step - loss: 0.1806 - accuracy: 0.7289 - val_loss: 0.1706 - val_accuracy: 0.7442\nEpoch 3/25\n1770/1770 [==============================] - 391s 220ms/step - loss: 0.1662 - accuracy: 0.7546 - val_loss: 0.1604 - val_accuracy: 0.7679\nEpoch 4/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1566 - accuracy: 0.7718 - val_loss: 0.1515 - val_accuracy: 0.7845\nEpoch 5/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1474 - accuracy: 0.7873 - val_loss: 0.1421 - val_accuracy: 0.7952\nEpoch 6/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1393 - accuracy: 0.8012 - val_loss: 0.1365 - val_accuracy: 0.8035\nEpoch 7/25\n1770/1770 [==============================] - 391s 220ms/step - loss: 0.1304 - accuracy: 0.8165 - val_loss: 0.1292 - val_accuracy: 0.8177\nEpoch 8/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1221 - accuracy: 0.8294 - val_loss: 0.1197 - val_accuracy: 0.8333\nEpoch 9/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1144 - accuracy: 0.8430 - val_loss: 0.1164 - val_accuracy: 0.8404\nEpoch 10/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1071 - accuracy: 0.8542 - val_loss: 0.1070 - val_accuracy: 0.8550\nEpoch 11/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1005 - accuracy: 0.8645 - val_loss: 0.1037 - val_accuracy: 0.8593\nEpoch 12/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0946 - accuracy: 0.8730 - val_loss: 0.1040 - val_accuracy: 0.8589\nEpoch 13/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0890 - accuracy: 0.8817 - val_loss: 0.0966 - val_accuracy: 0.8711\nEpoch 14/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0837 - accuracy: 0.8899 - val_loss: 0.0938 - val_accuracy: 0.8740\nEpoch 15/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0793 - accuracy: 0.8961 - val_loss: 0.0928 - val_accuracy: 0.8748\nEpoch 16/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0740 - accuracy: 0.9046 - val_loss: 0.0918 - val_accuracy: 0.8767\nEpoch 17/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0688 - accuracy: 0.9118 - val_loss: 0.0910 - val_accuracy: 0.8767\nEpoch 18/25\n1770/1770 [==============================] - 391s 220ms/step - loss: 0.0631 - accuracy: 0.9192 - val_loss: 0.0874 - val_accuracy: 0.8825\nEpoch 19/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0581 - accuracy: 0.9265 - val_loss: 0.0831 - val_accuracy: 0.8890\nEpoch 20/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0532 - accuracy: 0.9336 - val_loss: 0.0853 - val_accuracy: 0.8865\nEpoch 21/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0486 - accuracy: 0.9398 - val_loss: 0.0854 - val_accuracy: 0.8856\nEpoch 22/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0439 - accuracy: 0.9463 - val_loss: 0.0819 - val_accuracy: 0.8909\nEpoch 23/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0395 - accuracy: 0.9525 - val_loss: 0.0838 - val_accuracy: 0.8898\nEpoch 24/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0361 - accuracy: 0.9568 - val_loss: 0.0839 - val_accuracy: 0.8888\nEpoch 25/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0327 - accuracy: 0.9614 - val_loss: 0.0868 - val_accuracy: 0.8873\n"
    }
   ],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows Gray 1000 - Vect 512 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\npositive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\nwidth, height, channels = 113, 113, 1\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 512,\n                         \"image_size\": \"113x113x1\",\n                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 406s 221ms/step - loss: 0.2501 - accuracy: 0.4994 - val_loss: 0.2500 - val_accuracy: 0.4988\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 391s 220ms/step - loss: 0.2500 - accuracy: 0.4994 - val_loss: 0.2500 - val_accuracy: 0.5012\nEpoch 3/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4992 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 4/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4980 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 5/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5002 - val_loss: 0.2500 - val_accuracy: 0.5012\nEpoch 6/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4992 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 7/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5003 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 8/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5003 - val_loss: 0.2500 - val_accuracy: 0.4988\nEpoch 9/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4996 - val_loss: 0.2500 - val_accuracy: 0.4948\nEpoch 10/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.4997 - val_loss: 0.2500 - val_accuracy: 0.4924\nEpoch 11/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5016 - val_loss: 0.2501 - val_accuracy: 0.4947\nEpoch 12/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2500 - accuracy: 0.5038 - val_loss: 0.2501 - val_accuracy: 0.4930\nEpoch 13/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5048 - val_loss: 0.2501 - val_accuracy: 0.4933\nEpoch 14/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5050 - val_loss: 0.2501 - val_accuracy: 0.4930\nEpoch 15/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5054 - val_loss: 0.2501 - val_accuracy: 0.4935\nEpoch 16/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5058 - val_loss: 0.2501 - val_accuracy: 0.4941\nEpoch 17/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5061 - val_loss: 0.2501 - val_accuracy: 0.4920\nEpoch 18/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5060 - val_loss: 0.2501 - val_accuracy: 0.4934\nEpoch 19/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5063 - val_loss: 0.2501 - val_accuracy: 0.4936\nEpoch 20/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5061 - val_loss: 0.2501 - val_accuracy: 0.4925\nEpoch 21/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5057 - val_loss: 0.2501 - val_accuracy: 0.4933\nEpoch 22/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5058 - val_loss: 0.2501 - val_accuracy: 0.4935\nEpoch 23/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.2499 - accuracy: 0.5060 - val_loss: 0.2501 - val_accuracy: 0.4929\nEpoch 24/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5063 - val_loss: 0.2501 - val_accuracy: 0.4928\nEpoch 25/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.2499 - accuracy: 0.5063 - val_loss: 0.2501 - val_accuracy: 0.4937\n"
    }
   ],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m wandb uses only the first 10000 datapoints to create the plots.\n"
    }
   ],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "run.finish()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows RGB 1500 - Vect 2048 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\npositive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\nwidth, height, channels = 113, 113, 3\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2cx1i779\" target=\"_blank\">olive-violet-178</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 2048,\n                         \"image_size\": \"113x113x3\",\n                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n 285/1770 [===>..........................] - ETA: 5:17 - loss: 0.2201 - accuracy: 0.5924"
    }
   ],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows RGB 1500 - Vect 1024 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\npositive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\nwidth, height, channels = 113, 113, 3\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 1024,\n                         \"image_size\": \"113x113x1\",\n                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows RGB 1500 - Vect 512 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\npositive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\nwidth, height, channels = 113, 113, 3\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "Finishing last run (ID:3crcvbbp) before initializing another...",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<br/>Waiting for W&B process to finish, PID 34570... <strong style=\"color:green\">(success).</strong>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87fe3219a6b43f4960cf9417fa4d781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 512,\n                         \"image_size\": \"113x113x1\",\n                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 468s 234ms/step - loss: 0.2502 - accuracy: 0.4950 - val_loss: 0.2500 - val_accuracy: 0.4973\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 415s 231ms/step - loss: 0.2500 - accuracy: 0.4983 - val_loss: 0.2500 - val_accuracy: 0.4973\nEpoch 3/25\n1088/1770 [=================>............] - ETA: 2:21 - loss: 0.2500 - accuracy: 0.4987"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-5b71f04b4d7d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0msiamese_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcontrastive_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Adam\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"accuracy\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0mhistory_siamese_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msiamese_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mval_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReduceLROnPlateau\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmonitor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"val_loss\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpatience\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mWandbCallback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/wandb/integration/keras/keras.py\u001B[0m in \u001B[0;36mnew_v2\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mcbk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcbks\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m                 \u001B[0mset_wandb_attrs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcbk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 168\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mold_v2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    169\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m     \u001B[0mtraining_arrays\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morig_fit_loop\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_arrays\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1181\u001B[0m                 _r=1):\n\u001B[1;32m   1182\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1183\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1184\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1185\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    887\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    891\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    915\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    916\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 917\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    918\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    919\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3022\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m   3023\u001B[0m     return graph_function._call_flat(\n\u001B[0;32m-> 3024\u001B[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m   3025\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3026\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1959\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1960\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0;32m-> 1961\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[1;32m   1962\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1963\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    594\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    595\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 596\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    597\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    598\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0;32m---> 60\u001B[0;31m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows Bin 1000 - Vect 2048 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/anchor\"\npositive_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/positive\"\nwidth, height, channels = 113, 113, 1\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/14hqghll\" target=\"_blank\">wandering-planet-182</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 2048,\n                         \"image_size\": \"113x113x1\",\n                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m wandb uses only the first 10000 datapoints to create the plots.\n"
    }
   ],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows Bin 1000 - Vect 1024 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000_bin/anchor\"\npositive_images_path = \"npz_datasets/pairs_1m_224_224_rows_1000_bin/positive\"\nwidth, height, channels = 113, 113, 1\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:30b4y7nr) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06fb4b73f92441c0b026add7bc712b0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">vibrant-bird-350</strong>: <a href=\"https://wandb.ai/schauppi/Architecture_1/runs/30b4y7nr\" target=\"_blank\">https://wandb.ai/schauppi/Architecture_1/runs/30b4y7nr</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20220317_081939-30b4y7nr/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:30b4y7nr). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/js/workspace/thesis_schaupp/HTSR_Handwritten_Text_Similarity_Recognition/wandb/run-20220317_082024-1vcop3jr</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1vcop3jr\" target=\"_blank\">prime-dawn-351</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/38\n7080/7080 [==============================] - 402s 56ms/step - loss: 0.2432 - accuracy: 0.5589 - val_loss: 0.2224 - val_accuracy: 0.6257\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/38\n7080/7080 [==============================] - 398s 56ms/step - loss: 0.2156 - accuracy: 0.6424 - val_loss: 0.1888 - val_accuracy: 0.7092\nEpoch 3/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.1809 - accuracy: 0.7243 - val_loss: 0.1654 - val_accuracy: 0.7546\nEpoch 4/38\n7080/7080 [==============================] - 398s 56ms/step - loss: 0.1570 - accuracy: 0.7684 - val_loss: 0.1443 - val_accuracy: 0.7895\nEpoch 5/38\n7080/7080 [==============================] - 398s 56ms/step - loss: 0.1390 - accuracy: 0.8001 - val_loss: 0.1319 - val_accuracy: 0.8099\nEpoch 6/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.1263 - accuracy: 0.8208 - val_loss: 0.1225 - val_accuracy: 0.8263\nEpoch 7/38\n7080/7080 [==============================] - 398s 56ms/step - loss: 0.1154 - accuracy: 0.8390 - val_loss: 0.1113 - val_accuracy: 0.8451\nEpoch 8/38\n7080/7080 [==============================] - 398s 56ms/step - loss: 0.1058 - accuracy: 0.8543 - val_loss: 0.1076 - val_accuracy: 0.8510\nEpoch 9/38\n7080/7080 [==============================] - 400s 56ms/step - loss: 0.0975 - accuracy: 0.8666 - val_loss: 0.1007 - val_accuracy: 0.8616\nEpoch 10/38\n7080/7080 [==============================] - 398s 56ms/step - loss: 0.0912 - accuracy: 0.8765 - val_loss: 0.0938 - val_accuracy: 0.8708\nEpoch 11/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0860 - accuracy: 0.8848 - val_loss: 0.0877 - val_accuracy: 0.8805\nEpoch 12/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0815 - accuracy: 0.8905 - val_loss: 0.0870 - val_accuracy: 0.8826\nEpoch 13/38\n7080/7080 [==============================] - 401s 56ms/step - loss: 0.0777 - accuracy: 0.8965 - val_loss: 0.0821 - val_accuracy: 0.8893\nEpoch 14/38\n7080/7080 [==============================] - 398s 56ms/step - loss: 0.0743 - accuracy: 0.9017 - val_loss: 0.0814 - val_accuracy: 0.8892\nEpoch 15/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0723 - accuracy: 0.9049 - val_loss: 0.0779 - val_accuracy: 0.8959\nEpoch 16/38\n7080/7080 [==============================] - 398s 56ms/step - loss: 0.0694 - accuracy: 0.9097 - val_loss: 0.0751 - val_accuracy: 0.8996\nEpoch 17/38\n7080/7080 [==============================] - 400s 56ms/step - loss: 0.0672 - accuracy: 0.9119 - val_loss: 0.0736 - val_accuracy: 0.9022\nEpoch 18/38\n7080/7080 [==============================] - 398s 56ms/step - loss: 0.0656 - accuracy: 0.9147 - val_loss: 0.0747 - val_accuracy: 0.9006\nEpoch 19/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0639 - accuracy: 0.9169 - val_loss: 0.0716 - val_accuracy: 0.9059\nEpoch 20/38\n7080/7080 [==============================] - 398s 56ms/step - loss: 0.0624 - accuracy: 0.9193 - val_loss: 0.0723 - val_accuracy: 0.9039\nEpoch 21/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0607 - accuracy: 0.9218 - val_loss: 0.0696 - val_accuracy: 0.9081\nEpoch 22/38\n7080/7080 [==============================] - 400s 56ms/step - loss: 0.0595 - accuracy: 0.9236 - val_loss: 0.0702 - val_accuracy: 0.9068\nEpoch 23/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0584 - accuracy: 0.9249 - val_loss: 0.0705 - val_accuracy: 0.9075\nEpoch 24/38\n7080/7080 [==============================] - 400s 56ms/step - loss: 0.0571 - accuracy: 0.9269 - val_loss: 0.0683 - val_accuracy: 0.9099\nEpoch 25/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0560 - accuracy: 0.9287 - val_loss: 0.0670 - val_accuracy: 0.9114\nEpoch 26/38\n7080/7080 [==============================] - 402s 56ms/step - loss: 0.0550 - accuracy: 0.9301 - val_loss: 0.0678 - val_accuracy: 0.9111\nEpoch 27/38\n7080/7080 [==============================] - 398s 56ms/step - loss: 0.0544 - accuracy: 0.9308 - val_loss: 0.0650 - val_accuracy: 0.9152\nEpoch 28/38\n7080/7080 [==============================] - 400s 56ms/step - loss: 0.0530 - accuracy: 0.9328 - val_loss: 0.0666 - val_accuracy: 0.9138\nEpoch 29/38\n7080/7080 [==============================] - 400s 56ms/step - loss: 0.0524 - accuracy: 0.9335 - val_loss: 0.0656 - val_accuracy: 0.9140\nEpoch 30/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0516 - accuracy: 0.9347 - val_loss: 0.0631 - val_accuracy: 0.9190\nEpoch 31/38\n7080/7080 [==============================] - 400s 56ms/step - loss: 0.0509 - accuracy: 0.9359 - val_loss: 0.0651 - val_accuracy: 0.9157\nEpoch 32/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0501 - accuracy: 0.9371 - val_loss: 0.0644 - val_accuracy: 0.9163\nEpoch 33/38\n7080/7080 [==============================] - 400s 56ms/step - loss: 0.0495 - accuracy: 0.9379 - val_loss: 0.0633 - val_accuracy: 0.9186\nEpoch 34/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0488 - accuracy: 0.9390 - val_loss: 0.0623 - val_accuracy: 0.9193\nEpoch 35/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0482 - accuracy: 0.9395 - val_loss: 0.0629 - val_accuracy: 0.9188\nEpoch 36/38\n7080/7080 [==============================] - 400s 56ms/step - loss: 0.0479 - accuracy: 0.9400 - val_loss: 0.0634 - val_accuracy: 0.9180\nEpoch 37/38\n7080/7080 [==============================] - 399s 56ms/step - loss: 0.0470 - accuracy: 0.9414 - val_loss: 0.0628 - val_accuracy: 0.9188\nEpoch 38/38\n7080/7080 [==============================] - 400s 56ms/step - loss: 0.0468 - accuracy: 0.9416 - val_loss: 0.0619 - val_accuracy: 0.9203\n\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.123 MB of 0.123 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.993203â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0475c9597edf4311812ba7cd00c7bdb1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1 - Score</td><td>â–</td></tr><tr><td>Precision</td><td>â–</td></tr><tr><td>Recall</td><td>â–</td></tr><tr><td>accuracy</td><td>â–â–ƒâ–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>loss</td><td>â–ˆâ–‡â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>lr</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_accuracy</td><td>â–â–ƒâ–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–‡â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1 - Score</td><td>0.92369</td></tr><tr><td>Precision</td><td>0.92399</td></tr><tr><td>Recall</td><td>0.9237</td></tr><tr><td>accuracy</td><td>0.94232</td></tr><tr><td>best_epoch</td><td>37</td></tr><tr><td>best_val_loss</td><td>0.06192</td></tr><tr><td>epoch</td><td>37</td></tr><tr><td>loss</td><td>0.0463</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>val_accuracy</td><td>0.92027</td></tr><tr><td>val_loss</td><td>0.06192</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">prime-dawn-351</strong>: <a href=\"https://wandb.ai/schauppi/Architecture_1/runs/1vcop3jr\" target=\"_blank\">https://wandb.ai/schauppi/Architecture_1/runs/1vcop3jr</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20220317_082024-1vcop3jr/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.001,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [1,1,1,1],\n                         \"conv_dims\": [24, 24, 48, 48],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 38,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 2048,\n                         \"image_size\": \"113x113x3\",\n                         \"l2_norm\": False,\n                         \"architecture\": \"ConvNeXt - 500k - Gray\"})\n\nconfig = wandb.config\n\nmodel = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=config.otimizer, metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])\n\nprecision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})\n\nrun.finish()",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.001,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [1,1,1,1],\n                         \"conv_dims\": [24, 24, 48, 48],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 50,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 2048,\n                         \"image_size\": [height,width, channels],\n                         \"l2_norm\": False,\n                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/50\nlearning rate sheduled from 0.0010000000474974513 to 0.0009048373904079199\n3540/3540 [==============================] - 213s 58ms/step - loss: 0.2480 - accuracy: 0.5269 - val_loss: 0.2429 - val_accuracy: 0.5692\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/50\n3540/3540 [==============================] - 204s 57ms/step - loss: 0.2425 - accuracy: 0.5697 - val_loss: 0.2397 - val_accuracy: 0.5829\nEpoch 3/50\nlearning rate sheduled from 0.0009048373904079199 to 0.0008187306812033057\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.2375 - accuracy: 0.5896 - val_loss: 0.2325 - val_accuracy: 0.5982\nEpoch 4/50\n3540/3540 [==============================] - 199s 56ms/step - loss: 0.2308 - accuracy: 0.6049 - val_loss: 0.2290 - val_accuracy: 0.6068\nEpoch 5/50\nlearning rate sheduled from 0.0008187306812033057 to 0.000740818097256124\n3540/3540 [==============================] - 200s 56ms/step - loss: 0.2260 - accuracy: 0.6149 - val_loss: 0.2263 - val_accuracy: 0.6127\nEpoch 6/50\n3540/3540 [==============================] - 207s 58ms/step - loss: 0.2224 - accuracy: 0.6243 - val_loss: 0.2228 - val_accuracy: 0.6209\nEpoch 7/50\nlearning rate sheduled from 0.000740818097256124 to 0.000670319888740778\n3540/3540 [==============================] - 205s 57ms/step - loss: 0.2180 - accuracy: 0.6361 - val_loss: 0.2188 - val_accuracy: 0.6338\nEpoch 8/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.2129 - accuracy: 0.6513 - val_loss: 0.2158 - val_accuracy: 0.6442\nEpoch 9/50\nlearning rate sheduled from 0.000670319888740778 to 0.0006065304623916745\n3540/3540 [==============================] - 199s 56ms/step - loss: 0.2056 - accuracy: 0.6698 - val_loss: 0.2065 - val_accuracy: 0.6647\nEpoch 10/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1969 - accuracy: 0.6877 - val_loss: 0.2017 - val_accuracy: 0.6708\nEpoch 11/50\nlearning rate sheduled from 0.0006065304623916745 to 0.0005488114547915757\n3540/3540 [==============================] - 201s 56ms/step - loss: 0.1897 - accuracy: 0.7013 - val_loss: 0.1970 - val_accuracy: 0.6839\nEpoch 12/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1847 - accuracy: 0.7103 - val_loss: 0.1939 - val_accuracy: 0.6882\nEpoch 13/50\nlearning rate sheduled from 0.0005488114547915757 to 0.0004965850966982543\n3540/3540 [==============================] - 204s 57ms/step - loss: 0.1800 - accuracy: 0.7196 - val_loss: 0.1931 - val_accuracy: 0.6934\nEpoch 14/50\n3540/3540 [==============================] - 201s 56ms/step - loss: 0.1758 - accuracy: 0.7282 - val_loss: 0.1902 - val_accuracy: 0.7017\nEpoch 15/50\nlearning rate sheduled from 0.0004965850966982543 to 0.0004493287415243685\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1714 - accuracy: 0.7387 - val_loss: 0.1880 - val_accuracy: 0.7028\nEpoch 16/50\n3540/3540 [==============================] - 200s 56ms/step - loss: 0.1674 - accuracy: 0.7471 - val_loss: 0.1875 - val_accuracy: 0.7081\nEpoch 17/50\nlearning rate sheduled from 0.0004493287415243685 to 0.0004065694229211658\n3540/3540 [==============================] - 199s 55ms/step - loss: 0.1636 - accuracy: 0.7539 - val_loss: 0.1845 - val_accuracy: 0.7132\nEpoch 18/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1602 - accuracy: 0.7605 - val_loss: 0.1820 - val_accuracy: 0.7167\nEpoch 19/50\nlearning rate sheduled from 0.0004065694229211658 to 0.00036787919816561043\n3540/3540 [==============================] - 202s 56ms/step - loss: 0.1561 - accuracy: 0.7685 - val_loss: 0.1811 - val_accuracy: 0.7218\nEpoch 20/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1526 - accuracy: 0.7758 - val_loss: 0.1795 - val_accuracy: 0.7265\nEpoch 21/50\nlearning rate sheduled from 0.00036787919816561043 to 0.0003328708407934755\n3540/3540 [==============================] - 204s 57ms/step - loss: 0.1493 - accuracy: 0.7815 - val_loss: 0.1786 - val_accuracy: 0.7290\nEpoch 22/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1464 - accuracy: 0.7870 - val_loss: 0.1758 - val_accuracy: 0.7333\nEpoch 23/50\nlearning rate sheduled from 0.0003328708407934755 to 0.00030119396978989244\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1424 - accuracy: 0.7955 - val_loss: 0.1742 - val_accuracy: 0.7357\nEpoch 24/50\n3540/3540 [==============================] - 205s 57ms/step - loss: 0.1405 - accuracy: 0.7973 - val_loss: 0.1741 - val_accuracy: 0.7358\nEpoch 25/50\nlearning rate sheduled from 0.00030119396978989244 to 0.000272531557129696\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1377 - accuracy: 0.8031 - val_loss: 0.1704 - val_accuracy: 0.7443\nEpoch 26/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1350 - accuracy: 0.8068 - val_loss: 0.1716 - val_accuracy: 0.7424\nEpoch 27/50\nlearning rate sheduled from 0.000272531557129696 to 0.0002465967263560742\n3540/3540 [==============================] - 202s 56ms/step - loss: 0.1319 - accuracy: 0.8123 - val_loss: 0.1719 - val_accuracy: 0.7432\nEpoch 28/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1301 - accuracy: 0.8158 - val_loss: 0.1702 - val_accuracy: 0.7455\nEpoch 29/50\nlearning rate sheduled from 0.0002465967263560742 to 0.0002231299295090139\n3540/3540 [==============================] - 204s 57ms/step - loss: 0.1281 - accuracy: 0.8194 - val_loss: 0.1695 - val_accuracy: 0.7482\nEpoch 30/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1259 - accuracy: 0.8235 - val_loss: 0.1694 - val_accuracy: 0.7491\nEpoch 31/50\nlearning rate sheduled from 0.0002231299295090139 to 0.0002018962986767292\n3540/3540 [==============================] - 202s 56ms/step - loss: 0.1238 - accuracy: 0.8264 - val_loss: 0.1687 - val_accuracy: 0.7508\nEpoch 32/50\n3540/3540 [==============================] - 208s 58ms/step - loss: 0.1220 - accuracy: 0.8297 - val_loss: 0.1678 - val_accuracy: 0.7520\nEpoch 33/50\nlearning rate sheduled from 0.0002018962986767292 to 0.00018268331768922508\n3540/3540 [==============================] - 199s 55ms/step - loss: 0.1202 - accuracy: 0.8334 - val_loss: 0.1689 - val_accuracy: 0.7515\nEpoch 34/50\n3540/3540 [==============================] - 204s 57ms/step - loss: 0.1191 - accuracy: 0.8346 - val_loss: 0.1684 - val_accuracy: 0.7523\nEpoch 35/50\nlearning rate sheduled from 0.00018268331768922508 to 0.00016529869753867388\n3540/3540 [==============================] - 199s 55ms/step - loss: 0.1169 - accuracy: 0.8383 - val_loss: 0.1690 - val_accuracy: 0.7518\nEpoch 36/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1159 - accuracy: 0.8401 - val_loss: 0.1690 - val_accuracy: 0.7520\nEpoch 37/50\nlearning rate sheduled from 0.00016529869753867388 to 0.00014956844097469002\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1145 - accuracy: 0.8428 - val_loss: 0.1667 - val_accuracy: 0.7564\nEpoch 38/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1135 - accuracy: 0.8446 - val_loss: 0.1669 - val_accuracy: 0.7556\nEpoch 39/50\nlearning rate sheduled from 0.00014956844097469002 to 0.0001353351108264178\n3540/3540 [==============================] - 200s 56ms/step - loss: 0.1120 - accuracy: 0.8471 - val_loss: 0.1661 - val_accuracy: 0.7588\nEpoch 40/50\n3540/3540 [==============================] - 204s 57ms/step - loss: 0.1106 - accuracy: 0.8499 - val_loss: 0.1671 - val_accuracy: 0.7587\nEpoch 41/50\nlearning rate sheduled from 0.0001353351108264178 to 0.00012245627294760197\n3540/3540 [==============================] - 199s 56ms/step - loss: 0.1093 - accuracy: 0.8518 - val_loss: 0.1674 - val_accuracy: 0.7571\nEpoch 42/50\n3540/3540 [==============================] - 198s 55ms/step - loss: 0.1086 - accuracy: 0.8534 - val_loss: 0.1671 - val_accuracy: 0.7581\nEpoch 43/50\nlearning rate sheduled from 0.00012245627294760197 to 0.00011080301192123443\n3540/3540 [==============================] - 199s 55ms/step - loss: 0.1072 - accuracy: 0.8564 - val_loss: 0.1690 - val_accuracy: 0.7566\nEpoch 44/50\n3540/3540 [==============================] - 200s 56ms/step - loss: 0.1065 - accuracy: 0.8569 - val_loss: 0.1669 - val_accuracy: 0.7595\n"
    }
   ],
   "source": "tf.random.set_seed(42)\n\nmodel = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6, l2_norm=config.l2_norm)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\ndef scheduler(epoch, lr):\n    if epoch % 2 == 0:\n        lr_new = lr * tf.math.exp(-0.1)\n        print(f\"learning rate sheduled from {lr} to {lr_new}\")\n        return lr_new\n    else:\n        return lr\n\noptimizer = tf.keras.optimizers.Adam(learning_rate = config.learning_rate)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=optimizer, metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[WandbCallback(), tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\"), tf.keras.callbacks.LearningRateScheduler(scheduler)])\n\nprecision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows Bin 1000 - Vect 512 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/anchor\"\npositive_images_path = \"npz_datasets/pairs_250K_224_224_1000_bin/positive\"\nwidth, height, channels = 113, 113, 1\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_224_224_1000_bin/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 512,\n                         \"image_size\": \"113x113x1\",\n                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows Gray 1500 - Vect 1024 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\npositive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\nwidth, height, channels = 113, 113, 1\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3mk7zc98\" target=\"_blank\">glamorous-water-189</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 2048,\n                         \"image_size\": \"113x113x1\",\n                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows Gray 1500 - Vect 2048 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/anchor\"\npositive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1500/positive\"\nwidth, height, channels = 113, 113, 1\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1500/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3dbijygp\" target=\"_blank\">drawn-thunder-191</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 2048,\n                         \"image_size\": \"113x113x1\",\n                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 407s 221ms/step - loss: 0.2132 - accuracy: 0.6513 - val_loss: 0.1962 - val_accuracy: 0.6982\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.1856 - accuracy: 0.7199 - val_loss: 0.1723 - val_accuracy: 0.7459\nEpoch 3/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1711 - accuracy: 0.7459 - val_loss: 0.1670 - val_accuracy: 0.7555\nEpoch 4/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1623 - accuracy: 0.7623 - val_loss: 0.1642 - val_accuracy: 0.7556\nEpoch 5/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1514 - accuracy: 0.7818 - val_loss: 0.1452 - val_accuracy: 0.7929\nEpoch 6/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1395 - accuracy: 0.8023 - val_loss: 0.1351 - val_accuracy: 0.8094\nEpoch 7/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1263 - accuracy: 0.8249 - val_loss: 0.1250 - val_accuracy: 0.8268\nEpoch 8/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1127 - accuracy: 0.8466 - val_loss: 0.1077 - val_accuracy: 0.8530\nEpoch 9/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.1007 - accuracy: 0.8649 - val_loss: 0.1036 - val_accuracy: 0.8596\nEpoch 10/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0905 - accuracy: 0.8806 - val_loss: 0.0935 - val_accuracy: 0.8770\nEpoch 11/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0812 - accuracy: 0.8943 - val_loss: 0.0879 - val_accuracy: 0.8845\nEpoch 12/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.0730 - accuracy: 0.9064 - val_loss: 0.0860 - val_accuracy: 0.8859\nEpoch 13/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0653 - accuracy: 0.9172 - val_loss: 0.0744 - val_accuracy: 0.9047\nEpoch 14/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0598 - accuracy: 0.9251 - val_loss: 0.0711 - val_accuracy: 0.9090\nEpoch 15/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0542 - accuracy: 0.9329 - val_loss: 0.0691 - val_accuracy: 0.9131\nEpoch 16/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.0495 - accuracy: 0.9393 - val_loss: 0.0666 - val_accuracy: 0.9158\nEpoch 17/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0457 - accuracy: 0.9449 - val_loss: 0.0647 - val_accuracy: 0.9183\nEpoch 18/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0419 - accuracy: 0.9501 - val_loss: 0.0648 - val_accuracy: 0.9182\nEpoch 19/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0385 - accuracy: 0.9548 - val_loss: 0.0615 - val_accuracy: 0.9229\nEpoch 20/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0356 - accuracy: 0.9587 - val_loss: 0.0630 - val_accuracy: 0.9210\nEpoch 21/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0333 - accuracy: 0.9613 - val_loss: 0.0622 - val_accuracy: 0.9230\nEpoch 22/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.0305 - accuracy: 0.9646 - val_loss: 0.0626 - val_accuracy: 0.9229\nEpoch 23/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0283 - accuracy: 0.9679 - val_loss: 0.0660 - val_accuracy: 0.9169\nEpoch 24/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0270 - accuracy: 0.9692 - val_loss: 0.0592 - val_accuracy: 0.9268\nEpoch 25/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0251 - accuracy: 0.9718 - val_loss: 0.0602 - val_accuracy: 0.9262\n"
    }
   ],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "run.finish()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows Bin 1500 - Vect 2048 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250K_224_224_1500_bin/anchor\"\npositive_images_path = \"npz_datasets/pairs_250K_224_224_1500_bin/positive\"\nwidth, height, channels = 113, 113, 1\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_224_224_1500_bin/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_224_224_1500_bin/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/html": "Tracking run with wandb version 0.12.11",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "Run data is saved locally in <code>/home/js/workspace/thesis_schaupp/HTSR_Handwritten_Text_Similarity_Recognition/wandb/run-20220302_122654-bluwn3wc</code>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/bluwn3wc\" target=\"_blank\">absurd-blaze-209</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 2048,\n                         \"image_size\": \"113x113x1\",\n                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n"
    }
   ],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## 90K Rows Bin 1500 - Vect 1024 - Contrastive Eucledian"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "anchor_images_path = \"npz_datasets/pairs_250K_224_224_1500_bin/anchor\"\npositive_images_path = \"npz_datasets/pairs_250K_224_224_1500_bin/positive\"\nwidth, height, channels = 113, 113, 1\nbatch_size = 128\ntrain_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n\nanchor_images_test_path = \"npz_datasets/test_pairs_224_224_1500_bin/anchor\"\npositive_images_test_path = \"npz_datasets/test_pairs_224_224_1500_bin/positive\"\ntest_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/html": "Tracking run with wandb version 0.12.11",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "Run data is saved locally in <code>/home/js/workspace/thesis_schaupp/HTSR_Handwritten_Text_Similarity_Recognition/wandb/run-20220302_135558-2wf91upe</code>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2wf91upe\" target=\"_blank\">eager-voice-220</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "run = wandb.init(project=\"Architecture_1\",\n                 config={\"learning_rate\": 0.01,\n                         \"weight_decay\": \"None\",\n                         \"otimizer\": \"Adam\",\n                         \"conv_depth\": [3,3,9,3],\n                         \"conv_dims\": [48, 96, 192, 384],\n                         \"loss_function\": \"contrastive loss\",\n                         \"distance_function\": \"eucledian\",\n                         \"epochs\": 25,\n                         \"batch_size\": 128,\n                         \"embedding_dimension\": 1024,\n                         \"image_size\": \"113x113x1\",\n                         \"architecture\": \"ConvNeXt - 250k - Bin\"})\n\nconfig = wandb.config"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    <ipython-input-4-572c06eae5e1>:37 call  *\n        x = self.gamma * x\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:1074 _run_op\n        return tensor_oper(a.value(), *args, **kwargs)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n        raise e\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n        return func(x, y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1496 _mul_dispatch\n        return multiply(x, y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:518 multiply\n        return gen_math_ops.mul(x, y, name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:6078 mul\n        \"Mul\", x=x, y=y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n        inferred_from[input_arg.type_attr]))\n\n    TypeError: Input 'y' of 'Mul' Op has type float16 that does not match type float32 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-5b71f04b4d7d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_convnext_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mheight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwidth\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mchannels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdepths\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv_depth\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdims\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv_dims\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memb_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding_dimension\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdrop_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlayer_scale_init_value\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mleft_input\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInput\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mheight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwidth\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mchannels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mright_input\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInput\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mheight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwidth\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mchannels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-0f165846b765>\u001B[0m in \u001B[0;36mcreate_convnext_model\u001B[0;34m(input_shape, depths, dims, emb_dim, drop_path, layer_scale_init_value)\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLayerNormalization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepsilon\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdepths\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mConvNeXt_Block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdims\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdrop_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlayer_scale_init_value\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[0;31m# downsample + res3\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    950\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0m_in_functional_construction_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    951\u001B[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001B[0;32m--> 952\u001B[0;31m                                                 input_list)\n\u001B[0m\u001B[1;32m    953\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    954\u001B[0m     \u001B[0;31m# Maintains info about the `Layer.call` stack.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m_functional_construction_call\u001B[0;34m(self, inputs, args, kwargs, input_list)\u001B[0m\n\u001B[1;32m   1089\u001B[0m         \u001B[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1090\u001B[0m         outputs = self._keras_tensor_symbolic_call(\n\u001B[0;32m-> 1091\u001B[0;31m             inputs, input_masks, args, kwargs)\n\u001B[0m\u001B[1;32m   1092\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1093\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moutputs\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m_keras_tensor_symbolic_call\u001B[0;34m(self, inputs, input_masks, args, kwargs)\u001B[0m\n\u001B[1;32m    820\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mnest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_structure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkeras_tensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mKerasTensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_signature\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    821\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 822\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_infer_output_signature\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    823\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    824\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_infer_output_signature\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m_infer_output_signature\u001B[0;34m(self, inputs, args, kwargs, input_masks)\u001B[0m\n\u001B[1;32m    861\u001B[0m           \u001B[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    862\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 863\u001B[0;31m           \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    864\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    865\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    668\u001B[0m       \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint:disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    669\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'ag_error_metadata'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 670\u001B[0;31m           \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mag_error_metadata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    671\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    672\u001B[0m           \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: in user code:\n\n    <ipython-input-4-572c06eae5e1>:37 call  *\n        x = self.gamma * x\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:1074 _run_op\n        return tensor_oper(a.value(), *args, **kwargs)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n        raise e\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n        return func(x, y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1496 _mul_dispatch\n        return multiply(x, y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:518 multiply\n        return gen_math_ops.mul(x, y, name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:6078 mul\n        \"Mul\", x=x, y=y, name=name)\n    /home/js/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n        inferred_from[input_arg.type_attr]))\n\n    TypeError: Input 'y' of 'Mul' Op has type float16 that does not match type float32 of argument 'x'.\n"
     ]
    }
   ],
   "source": "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n\nleft_input = layers.Input(shape=(height, width, channels))\nright_input = layers.Input(shape=(height, width, channels))\n\nencoded_l = model(left_input)\nencoded_r = model(right_input)\n\ndistance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\nprediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n\nsiamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\nsiamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n\nhistory_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\nwandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\nwandb.log({'Precision': precision})\nwandb.log({'Recall': recall})\nwandb.log({'F1 - Score': f1_score})"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
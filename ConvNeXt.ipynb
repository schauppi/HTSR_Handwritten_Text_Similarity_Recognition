{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from helper_functions import get_classification_report\n",
    "from helper_functions import create_tf_data_datasets_contrastive\n",
    "from helper_functions import create_tf_data_testset_contrastive\n",
    "from helper_functions import euclidean_distance\n",
    "from helper_functions import manhattan_distance\n",
    "from helper_functions import contrastive_loss\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class ConvNeXt_Block(layers.Layer):\n",
    "    r\"\"\" ConvNeXt Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = layers.DepthwiseConv2D(kernel_size=7, padding='same')  # depthwise conv\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.pwconv1 = layers.Dense(4 * dim)\n",
    "        self.act = layers.Activation('gelu')\n",
    "        self.pwconv2 = layers.Dense(dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.dim = dim\n",
    "        self.layer_scale_init_value = layer_scale_init_value\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            initial_value=self.layer_scale_init_value * tf.ones((self.dim)),\n",
    "            trainable=True,\n",
    "            name='_gamma')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "class Downsample_Block(layers.Layer):\n",
    "    \"\"\"The Downsample Block in ConvNeXt\n",
    "        Args:\n",
    "            dim (int): number of channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.LN = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.conv = layers.Conv2D(dim, kernel_size=2, strides=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.LN(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class DropPath(tf.keras.layers.Layer):\n",
    "    \"\"\"The Drop path in ConvNeXt\n",
    "        Reference:\n",
    "            https://github.com/rishigami/Swin-Transformer-TF/blob/main/swintransformer/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        return self._drop_path(x, self.drop_prob, training)\n",
    "\n",
    "\n",
    "    def _drop_path(self, inputs, drop_prob, is_training):\n",
    "        if (not is_training) or (drop_prob == 0.):\n",
    "            return inputs\n",
    "\n",
    "        # Compute keep_prob\n",
    "        keep_prob = 1.0 - drop_prob\n",
    "\n",
    "        # Compute drop_connect tensor\n",
    "        random_tensor = keep_prob\n",
    "        shape = (tf.shape(inputs)[0],) + (1,) * (len(tf.shape(inputs)) - 1)\n",
    "        random_tensor += tf.random.uniform(shape, dtype=inputs.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_convnext_model(input_shape=(224, 224, 3), depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], emb_dim=1024, drop_path=0., layer_scale_init_value=1e-6):\n",
    "    \"\"\" Function to construct the ConvNeXt Model\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): (Width, Height , Channels)\n",
    "            depths (list): a list of size 4. denoting each stage's depth\n",
    "            dims (list): a list of size 4. denoting number of kernel's in each stage\n",
    "            num_classes (int): the number of classes\n",
    "            drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "            layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        Returns:\n",
    "            ConvNeXt model: an instance of tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    assert (len(depths) == 4 and len(dims) ==4), \"Must provide exactly 4 depths and 4 dims\"\n",
    "    assert (len(input_shape) == 3), \"Input shape must be (W, H, C)\"\n",
    "\n",
    "    input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Stem + res2\n",
    "    y = layers.Conv2D(dims[0], kernel_size=4, strides=4)(input)\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    for i in range(depths[0]):\n",
    "        y = ConvNeXt_Block(dims[0], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res3\n",
    "    y = Downsample_Block(dims[1])(y)\n",
    "    for i in range(depths[1]):\n",
    "        y = ConvNeXt_Block(dims[1], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res4\n",
    "    y = Downsample_Block(dims[2])(y)\n",
    "    for i in range(depths[2]):\n",
    "        y = ConvNeXt_Block(dims[2], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res5\n",
    "    y = Downsample_Block(dims[3])(y)\n",
    "    for i in range(depths[3]):\n",
    "        y = ConvNeXt_Block(dims[3], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    y = layers.GlobalAveragePooling2D()(y)\n",
    "    # final norm layer\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    # Head\n",
    "    y = layers.Dense(emb_dim, activation='relu')(y)\n",
    "\n",
    "    return tf.keras.Model(inputs=input, outputs=y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3qaksu40\" target=\"_blank\">eternal-jazz-159</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 469s 234ms/step - loss: 0.1733 - accuracy: 0.7485 - val_loss: 0.1341 - val_accuracy: 0.8270\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 415s 231ms/step - loss: 0.1242 - accuracy: 0.8370 - val_loss: 0.1133 - val_accuracy: 0.8506\nEpoch 3/25\n1770/1770 [==============================] - 412s 230ms/step - loss: 0.1022 - accuracy: 0.8683 - val_loss: 0.0913 - val_accuracy: 0.8825\nEpoch 4/25\n1770/1770 [==============================] - 411s 229ms/step - loss: 0.0900 - accuracy: 0.8829 - val_loss: 0.0843 - val_accuracy: 0.8910\nEpoch 5/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0822 - accuracy: 0.8935 - val_loss: 0.0809 - val_accuracy: 0.8947\nEpoch 6/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0766 - accuracy: 0.9012 - val_loss: 0.0809 - val_accuracy: 0.8913\nEpoch 7/25\n1770/1770 [==============================] - 409s 228ms/step - loss: 0.0714 - accuracy: 0.9084 - val_loss: 0.0697 - val_accuracy: 0.9112\nEpoch 8/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0666 - accuracy: 0.9147 - val_loss: 0.0667 - val_accuracy: 0.9143\nEpoch 9/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0619 - accuracy: 0.9209 - val_loss: 0.0699 - val_accuracy: 0.9101\nEpoch 10/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0574 - accuracy: 0.9278 - val_loss: 0.0578 - val_accuracy: 0.9267\nEpoch 11/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0530 - accuracy: 0.9334 - val_loss: 0.0617 - val_accuracy: 0.9204\nEpoch 12/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0493 - accuracy: 0.9387 - val_loss: 0.0535 - val_accuracy: 0.9319\nEpoch 13/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0454 - accuracy: 0.9439 - val_loss: 0.0476 - val_accuracy: 0.9402\nEpoch 14/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0422 - accuracy: 0.9480 - val_loss: 0.0515 - val_accuracy: 0.9349\nEpoch 15/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0391 - accuracy: 0.9525 - val_loss: 0.0435 - val_accuracy: 0.9463\nEpoch 16/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0360 - accuracy: 0.9568 - val_loss: 0.0463 - val_accuracy: 0.9423\nEpoch 17/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0336 - accuracy: 0.9601 - val_loss: 0.0435 - val_accuracy: 0.9467\nEpoch 18/25\n1770/1770 [==============================] - 408s 229ms/step - loss: 0.0313 - accuracy: 0.9631 - val_loss: 0.0385 - val_accuracy: 0.9534\nEpoch 19/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0296 - accuracy: 0.9655 - val_loss: 0.0387 - val_accuracy: 0.9530\nEpoch 20/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0277 - accuracy: 0.9677 - val_loss: 0.0399 - val_accuracy: 0.9509\nEpoch 21/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0256 - accuracy: 0.9707 - val_loss: 0.0410 - val_accuracy: 0.9494\nEpoch 22/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0247 - accuracy: 0.9715 - val_loss: 0.0403 - val_accuracy: 0.9505\nEpoch 23/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0230 - accuracy: 0.9740 - val_loss: 0.0388 - val_accuracy: 0.9526\nEpoch 24/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0150 - accuracy: 0.9844 - val_loss: 0.0325 - val_accuracy: 0.9614\nEpoch 25/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0132 - accuracy: 0.9867 - val_loss: 0.0325 - val_accuracy: 0.9618\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows Gray 1000 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2ggsgp9k\" target=\"_blank\">hopeful-paper-160</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 409s 221ms/step - loss: 0.2089 - accuracy: 0.6588 - val_loss: 0.1921 - val_accuracy: 0.6997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1854 - accuracy: 0.7159 - val_loss: 0.1692 - val_accuracy: 0.7565\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1622 - accuracy: 0.7629 - val_loss: 0.1552 - val_accuracy: 0.7739\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.1477 - accuracy: 0.7865 - val_loss: 0.1468 - val_accuracy: 0.7867\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1371 - accuracy: 0.8044 - val_loss: 0.1338 - val_accuracy: 0.8105\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.1266 - accuracy: 0.8222 - val_loss: 0.1268 - val_accuracy: 0.8224\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1158 - accuracy: 0.8398 - val_loss: 0.1146 - val_accuracy: 0.8404\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1054 - accuracy: 0.8554 - val_loss: 0.1069 - val_accuracy: 0.8533\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0952 - accuracy: 0.8722 - val_loss: 0.0952 - val_accuracy: 0.8713\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.0857 - accuracy: 0.8864 - val_loss: 0.0893 - val_accuracy: 0.8787\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0773 - accuracy: 0.8984 - val_loss: 0.0867 - val_accuracy: 0.8830\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0696 - accuracy: 0.9092 - val_loss: 0.0759 - val_accuracy: 0.8991\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0625 - accuracy: 0.9196 - val_loss: 0.0745 - val_accuracy: 0.9016\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0551 - accuracy: 0.9310 - val_loss: 0.0675 - val_accuracy: 0.9113\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0489 - accuracy: 0.9397 - val_loss: 0.0638 - val_accuracy: 0.9174\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0430 - accuracy: 0.9479 - val_loss: 0.0667 - val_accuracy: 0.9124\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0380 - accuracy: 0.9548 - val_loss: 0.0612 - val_accuracy: 0.9199\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0331 - accuracy: 0.9617 - val_loss: 0.0695 - val_accuracy: 0.9082\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0291 - accuracy: 0.9667 - val_loss: 0.0658 - val_accuracy: 0.9154\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0259 - accuracy: 0.9707 - val_loss: 0.0600 - val_accuracy: 0.9227\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0225 - accuracy: 0.9753 - val_loss: 0.0655 - val_accuracy: 0.9140\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0208 - accuracy: 0.9772 - val_loss: 0.0604 - val_accuracy: 0.9226\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 393s 220ms/step - loss: 0.0191 - accuracy: 0.9791 - val_loss: 0.0635 - val_accuracy: 0.9189\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0177 - accuracy: 0.9808 - val_loss: 0.0627 - val_accuracy: 0.9207\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0168 - accuracy: 0.9817 - val_loss: 0.0644 - val_accuracy: 0.9177\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
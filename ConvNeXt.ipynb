{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from helper_functions import get_classification_report\n",
    "from helper_functions import create_tf_data_datasets_contrastive\n",
    "from helper_functions import create_tf_data_testset_contrastive\n",
    "from helper_functions import euclidean_distance\n",
    "from helper_functions import manhattan_distance\n",
    "from helper_functions import contrastive_loss\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class ConvNeXt_Block(layers.Layer):\n",
    "    r\"\"\" ConvNeXt Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = layers.DepthwiseConv2D(kernel_size=7, padding='same')  # depthwise conv\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.pwconv1 = layers.Dense(4 * dim)\n",
    "        self.act = layers.Activation('gelu')\n",
    "        self.pwconv2 = layers.Dense(dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.dim = dim\n",
    "        self.layer_scale_init_value = layer_scale_init_value\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            initial_value=self.layer_scale_init_value * tf.ones((self.dim)),\n",
    "            trainable=True,\n",
    "            name='_gamma')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            #x = tf.cast(x, dtype=tf.float32)\n",
    "            x = self.gamma * x\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "\n",
    "        #x = tf.cast(x, dtype=tf.float16)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Downsample_Block(layers.Layer):\n",
    "    \"\"\"The Downsample Block in ConvNeXt\n",
    "        Args:\n",
    "            dim (int): number of channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.LN = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.conv = layers.Conv2D(dim, kernel_size=2, strides=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.LN(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class DropPath(tf.keras.layers.Layer):\n",
    "    \"\"\"The Drop path in ConvNeXt\n",
    "        Reference:\n",
    "            https://github.com/rishigami/Swin-Transformer-TF/blob/main/swintransformer/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        return self._drop_path(x, self.drop_prob, training)\n",
    "\n",
    "\n",
    "    def _drop_path(self, inputs, drop_prob, is_training):\n",
    "        if (not is_training) or (drop_prob == 0.):\n",
    "            return inputs\n",
    "\n",
    "        # Compute keep_prob\n",
    "        keep_prob = 1.0 - drop_prob\n",
    "\n",
    "        # Compute drop_connect tensor\n",
    "        random_tensor = keep_prob\n",
    "        shape = (tf.shape(inputs)[0],) + (1,) * (len(tf.shape(inputs)) - 1)\n",
    "        random_tensor += tf.random.uniform(shape, dtype=inputs.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_convnext_model(input_shape=(224, 224, 3), depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], emb_dim=1024, drop_path=0., layer_scale_init_value=1e-6):\n",
    "    \"\"\" Function to construct the ConvNeXt Model\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): (Width, Height , Channels)\n",
    "            depths (list): a list of size 4. denoting each stage's depth\n",
    "            dims (list): a list of size 4. denoting number of kernel's in each stage\n",
    "            num_classes (int): the number of classes\n",
    "            drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "            layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        Returns:\n",
    "            ConvNeXt model: an instance of tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    assert (len(depths) == 4 and len(dims) ==4), \"Must provide exactly 4 depths and 4 dims\"\n",
    "    assert (len(input_shape) == 3), \"Input shape must be (W, H, C)\"\n",
    "\n",
    "    input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Stem + res2\n",
    "    y = layers.Conv2D(dims[0], kernel_size=4, strides=4)(input)\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    for i in range(depths[0]):\n",
    "        y = ConvNeXt_Block(dims[0], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res3\n",
    "    y = Downsample_Block(dims[1])(y)\n",
    "    for i in range(depths[1]):\n",
    "        y = ConvNeXt_Block(dims[1], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res4\n",
    "    y = Downsample_Block(dims[2])(y)\n",
    "    for i in range(depths[2]):\n",
    "        y = ConvNeXt_Block(dims[2], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    # downsample + res5\n",
    "    y = Downsample_Block(dims[3])(y)\n",
    "    for i in range(depths[3]):\n",
    "        y = ConvNeXt_Block(dims[3], drop_path, layer_scale_init_value)(y)\n",
    "\n",
    "    y = layers.GlobalAveragePooling2D()(y)\n",
    "    # final norm layer\n",
    "    y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "    # Head\n",
    "    y = layers.Dense(emb_dim, activation='relu')(y)\n",
    "\n",
    "    return tf.keras.Model(inputs=input, outputs=y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Vect 1024 -  Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/3qaksu40\" target=\"_blank\">eternal-jazz-159</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 469s 234ms/step - loss: 0.1733 - accuracy: 0.7485 - val_loss: 0.1341 - val_accuracy: 0.8270\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 415s 231ms/step - loss: 0.1242 - accuracy: 0.8370 - val_loss: 0.1133 - val_accuracy: 0.8506\nEpoch 3/25\n1770/1770 [==============================] - 412s 230ms/step - loss: 0.1022 - accuracy: 0.8683 - val_loss: 0.0913 - val_accuracy: 0.8825\nEpoch 4/25\n1770/1770 [==============================] - 411s 229ms/step - loss: 0.0900 - accuracy: 0.8829 - val_loss: 0.0843 - val_accuracy: 0.8910\nEpoch 5/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0822 - accuracy: 0.8935 - val_loss: 0.0809 - val_accuracy: 0.8947\nEpoch 6/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0766 - accuracy: 0.9012 - val_loss: 0.0809 - val_accuracy: 0.8913\nEpoch 7/25\n1770/1770 [==============================] - 409s 228ms/step - loss: 0.0714 - accuracy: 0.9084 - val_loss: 0.0697 - val_accuracy: 0.9112\nEpoch 8/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0666 - accuracy: 0.9147 - val_loss: 0.0667 - val_accuracy: 0.9143\nEpoch 9/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0619 - accuracy: 0.9209 - val_loss: 0.0699 - val_accuracy: 0.9101\nEpoch 10/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0574 - accuracy: 0.9278 - val_loss: 0.0578 - val_accuracy: 0.9267\nEpoch 11/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0530 - accuracy: 0.9334 - val_loss: 0.0617 - val_accuracy: 0.9204\nEpoch 12/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0493 - accuracy: 0.9387 - val_loss: 0.0535 - val_accuracy: 0.9319\nEpoch 13/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0454 - accuracy: 0.9439 - val_loss: 0.0476 - val_accuracy: 0.9402\nEpoch 14/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0422 - accuracy: 0.9480 - val_loss: 0.0515 - val_accuracy: 0.9349\nEpoch 15/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0391 - accuracy: 0.9525 - val_loss: 0.0435 - val_accuracy: 0.9463\nEpoch 16/25\n1770/1770 [==============================] - 410s 229ms/step - loss: 0.0360 - accuracy: 0.9568 - val_loss: 0.0463 - val_accuracy: 0.9423\nEpoch 17/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0336 - accuracy: 0.9601 - val_loss: 0.0435 - val_accuracy: 0.9467\nEpoch 18/25\n1770/1770 [==============================] - 408s 229ms/step - loss: 0.0313 - accuracy: 0.9631 - val_loss: 0.0385 - val_accuracy: 0.9534\nEpoch 19/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0296 - accuracy: 0.9655 - val_loss: 0.0387 - val_accuracy: 0.9530\nEpoch 20/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0277 - accuracy: 0.9677 - val_loss: 0.0399 - val_accuracy: 0.9509\nEpoch 21/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0256 - accuracy: 0.9707 - val_loss: 0.0410 - val_accuracy: 0.9494\nEpoch 22/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0247 - accuracy: 0.9715 - val_loss: 0.0403 - val_accuracy: 0.9505\nEpoch 23/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0230 - accuracy: 0.9740 - val_loss: 0.0388 - val_accuracy: 0.9526\nEpoch 24/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0150 - accuracy: 0.9844 - val_loss: 0.0325 - val_accuracy: 0.9614\nEpoch 25/25\n1770/1770 [==============================] - 409s 229ms/step - loss: 0.0132 - accuracy: 0.9867 - val_loss: 0.0325 - val_accuracy: 0.9618\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows Gray 1000 - Vect 1024 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/2ggsgp9k\" target=\"_blank\">hopeful-paper-160</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 1024,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n1770/1770 [==============================] - 409s 221ms/step - loss: 0.2089 - accuracy: 0.6588 - val_loss: 0.1921 - val_accuracy: 0.6997\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 2/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1854 - accuracy: 0.7159 - val_loss: 0.1692 - val_accuracy: 0.7565\nEpoch 3/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1622 - accuracy: 0.7629 - val_loss: 0.1552 - val_accuracy: 0.7739\nEpoch 4/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.1477 - accuracy: 0.7865 - val_loss: 0.1468 - val_accuracy: 0.7867\nEpoch 5/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1371 - accuracy: 0.8044 - val_loss: 0.1338 - val_accuracy: 0.8105\nEpoch 6/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.1266 - accuracy: 0.8222 - val_loss: 0.1268 - val_accuracy: 0.8224\nEpoch 7/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1158 - accuracy: 0.8398 - val_loss: 0.1146 - val_accuracy: 0.8404\nEpoch 8/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.1054 - accuracy: 0.8554 - val_loss: 0.1069 - val_accuracy: 0.8533\nEpoch 9/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0952 - accuracy: 0.8722 - val_loss: 0.0952 - val_accuracy: 0.8713\nEpoch 10/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.0857 - accuracy: 0.8864 - val_loss: 0.0893 - val_accuracy: 0.8787\nEpoch 11/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0773 - accuracy: 0.8984 - val_loss: 0.0867 - val_accuracy: 0.8830\nEpoch 12/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0696 - accuracy: 0.9092 - val_loss: 0.0759 - val_accuracy: 0.8991\nEpoch 13/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0625 - accuracy: 0.9196 - val_loss: 0.0745 - val_accuracy: 0.9016\nEpoch 14/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0551 - accuracy: 0.9310 - val_loss: 0.0675 - val_accuracy: 0.9113\nEpoch 15/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0489 - accuracy: 0.9397 - val_loss: 0.0638 - val_accuracy: 0.9174\nEpoch 16/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0430 - accuracy: 0.9479 - val_loss: 0.0667 - val_accuracy: 0.9124\nEpoch 17/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0380 - accuracy: 0.9548 - val_loss: 0.0612 - val_accuracy: 0.9199\nEpoch 18/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0331 - accuracy: 0.9617 - val_loss: 0.0695 - val_accuracy: 0.9082\nEpoch 19/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0291 - accuracy: 0.9667 - val_loss: 0.0658 - val_accuracy: 0.9154\nEpoch 20/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0259 - accuracy: 0.9707 - val_loss: 0.0600 - val_accuracy: 0.9227\nEpoch 21/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0225 - accuracy: 0.9753 - val_loss: 0.0655 - val_accuracy: 0.9140\nEpoch 22/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0208 - accuracy: 0.9772 - val_loss: 0.0604 - val_accuracy: 0.9226\nEpoch 23/25\n1770/1770 [==============================] - 393s 220ms/step - loss: 0.0191 - accuracy: 0.9791 - val_loss: 0.0635 - val_accuracy: 0.9189\nEpoch 24/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0177 - accuracy: 0.9808 - val_loss: 0.0627 - val_accuracy: 0.9207\nEpoch 25/25\n1770/1770 [==============================] - 392s 220ms/step - loss: 0.0168 - accuracy: 0.9817 - val_loss: 0.0644 - val_accuracy: 0.9177\n"
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Vect 2048 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/170lfkni\" target=\"_blank\">fearless-butterfly-170</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 456s 236ms/step - loss: 0.1751 - accuracy: 0.7466 - val_loss: 0.1419 - val_accuracy: 0.8179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 420s 234ms/step - loss: 0.1248 - accuracy: 0.8380 - val_loss: 0.1084 - val_accuracy: 0.8626\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 418s 233ms/step - loss: 0.1002 - accuracy: 0.8713 - val_loss: 0.0908 - val_accuracy: 0.8839\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 417s 233ms/step - loss: 0.0890 - accuracy: 0.8847 - val_loss: 0.0849 - val_accuracy: 0.8924\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 415s 232ms/step - loss: 0.0822 - accuracy: 0.8931 - val_loss: 0.0769 - val_accuracy: 0.9021\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 414s 231ms/step - loss: 0.0760 - accuracy: 0.9018 - val_loss: 0.0738 - val_accuracy: 0.9065\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 414s 231ms/step - loss: 0.0713 - accuracy: 0.9080 - val_loss: 0.0703 - val_accuracy: 0.9090\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 414s 231ms/step - loss: 0.0666 - accuracy: 0.9144 - val_loss: 0.0656 - val_accuracy: 0.9173\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 414s 231ms/step - loss: 0.0621 - accuracy: 0.9208 - val_loss: 0.0651 - val_accuracy: 0.9163\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 414s 231ms/step - loss: 0.0582 - accuracy: 0.9264 - val_loss: 0.0572 - val_accuracy: 0.9281\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0545 - accuracy: 0.9311 - val_loss: 0.0554 - val_accuracy: 0.9298\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0512 - accuracy: 0.9361 - val_loss: 0.0524 - val_accuracy: 0.9344\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0485 - accuracy: 0.9398 - val_loss: 0.0528 - val_accuracy: 0.9340\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0450 - accuracy: 0.9448 - val_loss: 0.0521 - val_accuracy: 0.9343\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0427 - accuracy: 0.9474 - val_loss: 0.0497 - val_accuracy: 0.9372\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0399 - accuracy: 0.9514 - val_loss: 0.0460 - val_accuracy: 0.9426\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0378 - accuracy: 0.9543 - val_loss: 0.0457 - val_accuracy: 0.9431\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0356 - accuracy: 0.9570 - val_loss: 0.0449 - val_accuracy: 0.9438\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0338 - accuracy: 0.9592 - val_loss: 0.0473 - val_accuracy: 0.9399\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0318 - accuracy: 0.9622 - val_loss: 0.0417 - val_accuracy: 0.9487\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0299 - accuracy: 0.9645 - val_loss: 0.0423 - val_accuracy: 0.9480\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0281 - accuracy: 0.9669 - val_loss: 0.0418 - val_accuracy: 0.9477\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0272 - accuracy: 0.9681 - val_loss: 0.0409 - val_accuracy: 0.9504\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0256 - accuracy: 0.9702 - val_loss: 0.0419 - val_accuracy: 0.9486\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0244 - accuracy: 0.9718 - val_loss: 0.0401 - val_accuracy: 0.9509\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Vect 512 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/70e8ta6h\" target=\"_blank\">rare-leaf-171</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 512,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 428s 232ms/step - loss: 0.1779 - accuracy: 0.7417 - val_loss: 0.1400 - val_accuracy: 0.8237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 412s 230ms/step - loss: 0.1296 - accuracy: 0.8275 - val_loss: 0.1158 - val_accuracy: 0.8467\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.1087 - accuracy: 0.8578 - val_loss: 0.0960 - val_accuracy: 0.8779\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0956 - accuracy: 0.8752 - val_loss: 0.0913 - val_accuracy: 0.8802\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0880 - accuracy: 0.8849 - val_loss: 0.0831 - val_accuracy: 0.8922\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0823 - accuracy: 0.8929 - val_loss: 0.0793 - val_accuracy: 0.8969\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0779 - accuracy: 0.8989 - val_loss: 0.0783 - val_accuracy: 0.8969\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0731 - accuracy: 0.9053 - val_loss: 0.0719 - val_accuracy: 0.9074\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0697 - accuracy: 0.9098 - val_loss: 0.0657 - val_accuracy: 0.9158\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0660 - accuracy: 0.9155 - val_loss: 0.0639 - val_accuracy: 0.9178\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0622 - accuracy: 0.9210 - val_loss: 0.0659 - val_accuracy: 0.9150\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0594 - accuracy: 0.9247 - val_loss: 0.0598 - val_accuracy: 0.9241\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0559 - accuracy: 0.9297 - val_loss: 0.0588 - val_accuracy: 0.9254\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0539 - accuracy: 0.9322 - val_loss: 0.0568 - val_accuracy: 0.9274\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0511 - accuracy: 0.9367 - val_loss: 0.0536 - val_accuracy: 0.9324\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0487 - accuracy: 0.9393 - val_loss: 0.0519 - val_accuracy: 0.9341\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0467 - accuracy: 0.9420 - val_loss: 0.0522 - val_accuracy: 0.9338\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0444 - accuracy: 0.9451 - val_loss: 0.0502 - val_accuracy: 0.9366\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0418 - accuracy: 0.9490 - val_loss: 0.0535 - val_accuracy: 0.9319\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0400 - accuracy: 0.9510 - val_loss: 0.0474 - val_accuracy: 0.9400\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0379 - accuracy: 0.9539 - val_loss: 0.0482 - val_accuracy: 0.9389\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0361 - accuracy: 0.9563 - val_loss: 0.0447 - val_accuracy: 0.9447\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0343 - accuracy: 0.9589 - val_loss: 0.0456 - val_accuracy: 0.9429\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.0328 - accuracy: 0.9608 - val_loss: 0.0450 - val_accuracy: 0.9438\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.0311 - accuracy: 0.9632 - val_loss: 0.0443 - val_accuracy: 0.9447\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Vect 256 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 256,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 428s 232ms/step - loss: 0.2501 - accuracy: 0.4968 - val_loss: 0.2500 - val_accuracy: 0.5027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 412s 230ms/step - loss: 0.2500 - accuracy: 0.5012 - val_loss: 0.2500 - val_accuracy: 0.4973\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.4999 - val_loss: 0.2500 - val_accuracy: 0.5027\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5002 - val_loss: 0.2500 - val_accuracy: 0.5027\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.4993 - val_loss: 0.2500 - val_accuracy: 0.5027\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5008 - val_loss: 0.2500 - val_accuracy: 0.4973\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5007 - val_loss: 0.2500 - val_accuracy: 0.4973\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5016 - val_loss: 0.2500 - val_accuracy: 0.4988\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5020 - val_loss: 0.2500 - val_accuracy: 0.5008\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2500 - accuracy: 0.5035 - val_loss: 0.2500 - val_accuracy: 0.5010\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2500 - accuracy: 0.5030 - val_loss: 0.2500 - val_accuracy: 0.4978\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5069 - val_loss: 0.2500 - val_accuracy: 0.4973\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5078 - val_loss: 0.2500 - val_accuracy: 0.4971\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 412s 230ms/step - loss: 0.2499 - accuracy: 0.5083 - val_loss: 0.2500 - val_accuracy: 0.4977\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5086 - val_loss: 0.2501 - val_accuracy: 0.4969\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5083 - val_loss: 0.2501 - val_accuracy: 0.4974\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 412s 230ms/step - loss: 0.2499 - accuracy: 0.5093 - val_loss: 0.2501 - val_accuracy: 0.4982\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5096 - val_loss: 0.2501 - val_accuracy: 0.4987\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5094 - val_loss: 0.2501 - val_accuracy: 0.4981\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 414s 231ms/step - loss: 0.2499 - accuracy: 0.5098 - val_loss: 0.2501 - val_accuracy: 0.4980\n",
      "Epoch 21/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5095 - val_loss: 0.2501 - val_accuracy: 0.4987\n",
      "Epoch 22/25\n",
      "1770/1770 [==============================] - 413s 231ms/step - loss: 0.2499 - accuracy: 0.5102 - val_loss: 0.2501 - val_accuracy: 0.4984\n",
      "Epoch 23/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5099 - val_loss: 0.2501 - val_accuracy: 0.4986\n",
      "Epoch 24/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5096 - val_loss: 0.2501 - val_accuracy: 0.4989\n",
      "Epoch 25/25\n",
      "1770/1770 [==============================] - 412s 231ms/step - loss: 0.2499 - accuracy: 0.5097 - val_loss: 0.2501 - val_accuracy: 0.4990\n"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m wandb uses only the first 10000 datapoints to create the plots.\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows RGB 1000 - Vect 2048 - Big - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 3\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=True)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/9tnnkmui\" target=\"_blank\">restful-valley-173</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [96, 192, 384, 768],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x3\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - RGB\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 90K Rows Gray 1000 - Vect 2048 - Contrastive Eucledian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "anchor_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/anchor\"\n",
    "positive_images_path = \"npz_datasets/pairs_250k_224_224_rows_1000/positive\"\n",
    "width, height, channels = 113, 113, 1\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = create_tf_data_datasets_contrastive(anchor_images_path, positive_images_path, batch_size, height, width, rgb=False)\n",
    "\n",
    "anchor_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/anchor\"\n",
    "positive_images_test_path = \"npz_datasets/test_pairs_90k_224_224_rows_1000/positive\"\n",
    "test_dataset = create_tf_data_testset_contrastive(anchor_images_test_path, positive_images_test_path, height, width, rgb=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mschauppi\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/schauppi/Architecture_1/runs/gef7deu3\" target=\"_blank\">grateful-tree-176</a></strong> to <a href=\"https://wandb.ai/schauppi/Architecture_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Architecture_1\",\n",
    "                 config={\"learning_rate\": 0.01,\n",
    "                         \"weight_decay\": \"None\",\n",
    "                         \"otimizer\": \"Adam\",\n",
    "                         \"conv_depth\": [3,3,9,3],\n",
    "                         \"conv_dims\": [48, 96, 192, 384],\n",
    "                         \"loss_function\": \"contrastive loss\",\n",
    "                         \"distance_function\": \"eucledian\",\n",
    "                         \"epochs\": 25,\n",
    "                         \"batch_size\": 128,\n",
    "                         \"embedding_dimension\": 2048,\n",
    "                         \"image_size\": \"113x113x1\",\n",
    "                         \"architecture\": \"ConvNeXt - 250k - Gray\"})\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1770/1770 [==============================] - 409s 221ms/step - loss: 0.2076 - accuracy: 0.6630 - val_loss: 0.1876 - val_accuracy: 0.7139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[32m\u001B[41mERROR\u001B[0m Can't save model, h5py returned error: Layer ConvNeXt_Block has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1792 - accuracy: 0.7309 - val_loss: 0.1700 - val_accuracy: 0.7464\n",
      "Epoch 3/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1643 - accuracy: 0.7586 - val_loss: 0.1582 - val_accuracy: 0.7694\n",
      "Epoch 4/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1540 - accuracy: 0.7760 - val_loss: 0.1480 - val_accuracy: 0.7853\n",
      "Epoch 5/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1440 - accuracy: 0.7927 - val_loss: 0.1441 - val_accuracy: 0.7923\n",
      "Epoch 6/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1364 - accuracy: 0.8056 - val_loss: 0.1343 - val_accuracy: 0.8098\n",
      "Epoch 7/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1283 - accuracy: 0.8191 - val_loss: 0.1300 - val_accuracy: 0.8200\n",
      "Epoch 8/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1184 - accuracy: 0.8355 - val_loss: 0.1167 - val_accuracy: 0.8371\n",
      "Epoch 9/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1089 - accuracy: 0.8503 - val_loss: 0.1061 - val_accuracy: 0.8557\n",
      "Epoch 10/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.1002 - accuracy: 0.8641 - val_loss: 0.1041 - val_accuracy: 0.8589\n",
      "Epoch 11/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0920 - accuracy: 0.8772 - val_loss: 0.0981 - val_accuracy: 0.8669\n",
      "Epoch 12/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0844 - accuracy: 0.8880 - val_loss: 0.0976 - val_accuracy: 0.8680\n",
      "Epoch 13/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0758 - accuracy: 0.9004 - val_loss: 0.0916 - val_accuracy: 0.8756\n",
      "Epoch 14/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0680 - accuracy: 0.9126 - val_loss: 0.0873 - val_accuracy: 0.8834\n",
      "Epoch 15/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0602 - accuracy: 0.9239 - val_loss: 0.0817 - val_accuracy: 0.8902\n",
      "Epoch 16/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0522 - accuracy: 0.9348 - val_loss: 0.0811 - val_accuracy: 0.8921\n",
      "Epoch 17/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0450 - accuracy: 0.9455 - val_loss: 0.0786 - val_accuracy: 0.8958\n",
      "Epoch 18/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0387 - accuracy: 0.9541 - val_loss: 0.0833 - val_accuracy: 0.8891\n",
      "Epoch 19/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0333 - accuracy: 0.9613 - val_loss: 0.0806 - val_accuracy: 0.8937\n",
      "Epoch 20/25\n",
      "1770/1770 [==============================] - 392s 220ms/step - loss: 0.0293 - accuracy: 0.9665 - val_loss: 0.0811 - val_accuracy: 0.8942\n",
      "Epoch 21/25\n",
      "1661/1770 [===========================>..] - ETA: 21s - loss: 0.0260 - accuracy: 0.9706"
     ]
    }
   ],
   "source": [
    "model = create_convnext_model(input_shape=(height, width, channels), depths=config.conv_depth, dims=config.conv_dims, emb_dim=config.embedding_dimension, drop_path=0., layer_scale_init_value=1e-6)\n",
    "\n",
    "left_input = layers.Input(shape=(height, width, channels))\n",
    "right_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "distance = layers.Lambda(euclidean_distance)([encoded_l, encoded_r])\n",
    "prediction = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "siamese_model = keras.models.Model([left_input, right_input], outputs=prediction)\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_siamese_model = siamese_model.fit(train_dataset, epochs=config.epochs, validation_data=val_dataset, callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5),WandbCallback()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision, recall, f1_score, preds_wandb, labels = get_classification_report(test_dataset, siamese_model)\n",
    "wandb.log({\"roc\": wandb.plot.roc_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({\"pr\": wandb.plot.pr_curve(labels, preds_wandb, labels=None, classes_to_plot=None)})\n",
    "wandb.log({'Precision': precision})\n",
    "wandb.log({'Recall': recall})\n",
    "wandb.log({'F1 - Score': f1_score})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
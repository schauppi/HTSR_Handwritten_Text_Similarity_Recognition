{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebaf7371-e729-4df9-869c-c86a4b0da37d",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "%load_ext autoreload\n%autoreload 2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b21788c-284d-48be-8b39-63a8e58c142b",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "from paths import dataset_paths, dataset_test_paths, dataset_test_paths_tests, combined_list, combined_list_excluded, dataset_test_paths_excluded\nimport random\nimport os\nfrom PIL import Image, ImageOps, ImageDraw\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport numpy as np"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c9ddb5-170a-46f4-a765-682186001533",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "def create_image_pairs(height, width, path_anchors, path_positives, dataset_size, rgb=True):\n    \"\"\"\n    Function to create image pairs and save it to folders on disk\n    Takes pairs from the same image in same folder\n    \n    Arguments:\n        height: desired height of the cropped images - integer\n        width: desired width of the cropped images - interger\n        rgb: rgb oder gray images, standard rgb=True\n        path_anchor: path of the folder for anchor images to save in on disk - string_format\n        path_positives: path of the folder for positive images to save in on disk - string_format\n        dataset_size: how many image pairs should be generated and saved on disk - integer\n    \"\"\"\n    \n    i = 0\n    #padding for image saving\n    padding = len(str(dataset_size))\n\n    while i < dataset_size:\n\n        #select random path and image\n        path = (random.choice(dataset_paths))\n        image = random.choice(os.listdir(path))\n\n        #Convert image to grayscale iw wanted\n        if rgb == True:\n            image = Image.open(path + \"/\" + image)\n        else:\n            image = Image.open(path + \"/\" + image).convert(\"L\")\n\n        #check if image is wider than 500 px\n        if image.size[0] > 500:\n            #resize the image to specific height and keeping the same aspect ratio\n            height_precent = (height / float(image.size[1]))\n            resized_width = int((float(image.size[0]) * float(height_precent)))\n            image = image.resize((resized_width, height), Image.NEAREST)\n\n            #crop randomly 2 images out of source image\n            img_array = np.array(image)\n            x_max = img_array.shape[1] - width\n            x_1 = np.random.randint(0, x_max)\n            x_2 = np.random.randint(0, x_max)\n            anchor = img_array[0:width, x_1: x_1 + width]\n            positive = img_array[0:width, x_2: x_2 + width]\n\n            #save the crops on disk\n            anchor = Image.fromarray(anchor)\n            positive = Image.fromarray(positive)\n\n            anchor.save(os.path.join(path_anchors + \"/\" + f\"{i:0{padding}}.jpg\"))\n            positive.save(os.path.join(path_positives + \"/\" + f\"{i:0{padding}}.jpg\"))\n\n            i += 1\n        else:\n            pass"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9187e59-53f1-47d2-a57a-100f67edbbcc",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 2min 30s, sys: 43.8 s, total: 3min 14s\nWall time: 3min 15s\n"
    }
   ],
   "source": "%%time\ncreate_image_pairs(height=224, width=224, path_anchors=\"npz_datasets/pairs_20k/anchor\", path_positives=\"npz_datasets/pairs_20k/positive\", dataset_size=20000, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73bf2e64-9908-4cee-a290-7fb6a717227e",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "def create_image_pairs_new(height, width, path_anchors, path_positives, dataset_size, rgb=True):\n    \"\"\"\n    Function to create image pairs and save it to folders on disk\n    Takes pairs from different images in the same folder\n    \n    Arguments:\n        height: desired height of the cropped images - integer\n        width: desired width of the cropped images - interger\n        rgb: rgb oder gray images, standard rgb=True\n        path_anchor: path of the folder for anchor images to save in on disk - string_format\n        path_positives: path of the folder for positive images to save in on disk - string_format\n        dataset_size: how many image pairs should be generated and saved on disk - integer\n    \"\"\"\n\n    i = 0\n    padding = len(str(dataset_size))\n\n    while i < dataset_size:\n        path = (random.choice(dataset_paths))\n        image_1, image_2 = random.sample(os.listdir(path), 2)\n\n        if rgb == True:\n            try:\n                image_1 = Image.open(path + \"/\" + image_1)\n                image_2 = Image.open(path + \"/\" + image_2)\n            except:\n                continue\n        else:\n            try:\n                image_1 = Image.open(path + \"/\" + image_1).convert(\"L\")\n                image_2 = Image.open(path + \"/\" + image_2).convert(\"L\")\n            except:\n                continue\n\n        try:\n            if image_1.size[0] > 500 and image_2.size[0] > 500:\n                #resize the images to specific height and keeping the same aspect ratio\n                height_precent_image_1 = (height / float(image_1.size[1]))\n                height_precent_image_2 = (height / float(image_2.size[1]))\n\n                resized_width_image_1 = int((float(image_1.size[0]) * float(height_precent_image_1)))\n                resized_width_image_2 = int((float(image_2.size[0]) * float(height_precent_image_2)))\n\n                image_1 = image_1.resize((resized_width_image_1, height), Image.NEAREST)\n                image_2 = image_2.resize((resized_width_image_2, height), Image.NEAREST)\n\n                #crop randomly  image out of source image\n                img_array_1 = np.array(image_1)\n                img_array_2 = np.array(image_2)\n\n                x_max_1 = img_array_1.shape[1] - width\n                x_max_2 = img_array_2.shape[1] - width\n\n                x_1 = np.random.randint(0, x_max_1)\n                x_2 = np.random.randint(0, x_max_2)\n\n                anchor = img_array_1[0:width, x_1: x_1 + width]\n                positive = img_array_2[0:width, x_2: x_2 + width]\n\n                anchor = Image.fromarray(anchor)\n                positive = Image.fromarray(positive)\n\n                anchor.save(os.path.join(path_anchors + \"/\" + f\"{i:0{padding}}.jpg\"))\n                positive.save(os.path.join(path_positives + \"/\" + f\"{i:0{padding}}.jpg\"))\n                i += 1\n            else:\n                pass\n\n        except:\n            continue"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38be72d7-63e9-425f-a4b5-7439f28d8fe6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 25min 26s, sys: 5min 30s, total: 30min 56s\nWall time: 31min 13s\n"
    }
   ],
   "source": "%%time\ncreate_image_pairs_new(height=224, width=224, path_anchors=\"npz_datasets/pairs_150k_224_224/anchor\", path_positives=\"npz_datasets/pairs_150k_224_224/positive\", dataset_size=150000, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cb71044f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "def create_image_pairs_rows(height, width, path_anchors, path_positives, dataset_size, rgb=True):\n    \"\"\"\n    Function to create image pairs on row level and save it to folders on disk\n    Takes pairs from different images in the same folder\n\n    Arguments:\n        height: desired height of the cropped images - integer\n        width: desired width of the cropped images - interger\n        rgb: rgb oder gray images, standard rgb=True\n        path_anchor: path of the folder for anchor images to save in on disk - string_format\n        path_positives: path of the folder for positive images to save in on disk - string_format\n        dataset_size: how many image pairs should be generated and saved on disk - integer\n    \"\"\"\n\n    i = 0\n    padding = len(str(dataset_size))\n\n    while i < dataset_size:\n        path = (random.choice(dataset_paths))\n        image_1, image_2 = random.sample(os.listdir(path), 2)\n\n        if rgb == True:\n            try:\n                image_1 = Image.open(path + \"/\" + image_1)\n                image_2 = Image.open(path + \"/\" + image_2)\n            except:\n                continue\n        else:\n            try:\n                image_1 = Image.open(path + \"/\" + image_1).convert(\"L\")\n                image_2 = Image.open(path + \"/\" + image_2).convert(\"L\")\n            except:\n                continue\n\n        try:\n            if image_1.size[0] > width and image_2.size[0] > width:\n                #resize the images to specific height and keeping the same aspect ratio\n                height_precent_image_1 = (height / float(image_1.size[1]))\n                height_precent_image_2 = (height / float(image_2.size[1]))\n\n                resized_width_image_1 = int((float(image_1.size[0]) * float(height_precent_image_1)))\n                resized_width_image_2 = int((float(image_2.size[0]) * float(height_precent_image_2)))\n\n                image_1 = image_1.resize((resized_width_image_1, height), Image.NEAREST)\n                image_2 = image_2.resize((resized_width_image_2, height), Image.NEAREST)\n\n                #crop randomly  image out of source image\n                img_array_1 = np.array(image_1)\n                img_array_2 = np.array(image_2)\n\n                anchor = img_array_1[0:height, 0:width]\n                positive = img_array_2[0:height, 0:width]\n\n                anchor = Image.fromarray(anchor)\n                positive = Image.fromarray(positive)\n\n                anchor.save(os.path.join(path_anchors + \"/\" + f\"{i:0{padding}}.jpg\"))\n                positive.save(os.path.join(path_positives + \"/\" + f\"{i:0{padding}}.jpg\"))\n\n                i += 1\n            else:\n                pass\n        except:\n            continue"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "17125fd3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 1min 6s, sys: 11.9 s, total: 1min 18s\nWall time: 1min 18s\n"
    }
   ],
   "source": "%%time\ncreate_image_pairs_rows(height=113, width=1000, path_anchors=\"npz_datasets/test_pairs_113_1000/anchor\", path_positives=\"npz_datasets/test_pairs_113_1000/positive\", dataset_size=5000, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8e52ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "def create_image_pairs_rows_fixed_size(height, width, crop_width, path_anchors, path_positives, dataset_size, rgb=True):\n    \"\"\"\n    Function to create image pairs on row level and save it to folders on disk\n    Takes pairs from different images in the same folder\n\n    Arguments:\n        height: desired height of the cropped images - integer\n        width: desired width of the cropped images - interger\n        crop_width: desired crop width of the row - before resizing to width\n        rgb: rgb oder gray images, standard rgb=True\n        path_anchor: path of the folder for anchor images to save in on disk - string_format\n        path_positives: path of the folder for positive images to save in on disk - string_format\n        dataset_size: how many image pairs should be generated and saved on disk - integer\n    \"\"\"\n\n    i = 0\n    padding = len(str(dataset_size))\n\n    while i < dataset_size:\n        path = (random.choice(dataset_paths))\n        image_1, image_2 = random.sample(os.listdir(path), 2)\n\n        if rgb == True:\n            try:\n                image_1 = Image.open(path + \"/\" + image_1)\n                image_2 = Image.open(path + \"/\" + image_2)\n            except:\n                continue\n        else:\n            try:\n                image_1 = Image.open(path + \"/\" + image_1).convert(\"L\")\n                image_2 = Image.open(path + \"/\" + image_2).convert(\"L\")\n            except:\n                continue\n\n        try:\n            if image_1.size[0] > crop_width and image_2.size[0] > crop_width:\n                #resize the images to specific height and keeping the same aspect ratio\n                height_precent_image_1 = (height / float(image_1.size[1]))\n                height_precent_image_2 = (height / float(image_2.size[1]))\n\n                resized_width_image_1 = int((float(image_1.size[0]) * float(height_precent_image_1)))\n                resized_width_image_2 = int((float(image_2.size[0]) * float(height_precent_image_2)))\n\n                image_1 = image_1.resize((resized_width_image_1, height), Image.NEAREST)\n                image_2 = image_2.resize((resized_width_image_2, height), Image.NEAREST)\n\n                center_image_1 = int(image_1.size[0] / 2)\n                center_image_2 = int(image_2.size[0] / 2)\n\n                x_1 = int(center_image_1 - (crop_width/2))\n                x_2 = int(center_image_2 - (crop_width/2))\n\n                #crop randomly  image out of source image\n                img_array_1 = np.array(image_1)\n                img_array_2 = np.array(image_2)\n\n                anchor = img_array_1[0:height, x_1:x_1+crop_width]\n                positive = img_array_2[0:height, x_2:x_2+crop_width]\n\n                anchor = Image.fromarray(anchor)\n                positive = Image.fromarray(positive)\n\n                anchor = anchor.resize((width, height))\n                positive = positive.resize((width, height))\n\n                anchor.save(os.path.join(path_anchors + \"/\" + f\"{i:0{padding}}.jpg\"))\n                positive.save(os.path.join(path_positives + \"/\" + f\"{i:0{padding}}.jpg\"))\n\n                i += 1\n            else:\n                pass\n        except:\n            continue"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598aefb6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "%%time\ncreate_image_pairs_rows_fixed_size(height=224, width=224, crop_width=1000, path_anchors=\"npz_datasets/pairs_1m_224_224_rows_1000/anchor\", path_positives=\"npz_datasets/pairs_1m_224_224_rows_1000/positive\", dataset_size=1000000, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95b5c21a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "def create_cvl_image_pairs_rows_fixed_size(height, width, crop_width, path_anchors, path_positives, dataset_size, rgb=True):\n    \"\"\"\n    Function to create image pairs on row level and save it to folders on disk\n    Takes pairs from different images in the same folder\n\n    Arguments:\n        height: desired height of the cropped images - integer\n        width: desired width of the cropped images - interger\n        crop_width: desired crop width of the row - before resizing to width\n        rgb: rgb oder gray images, standard rgb=True\n        path_anchor: path of the folder for anchor images to save in on disk - string_format\n        path_positives: path of the folder for positive images to save in on disk - string_format\n        dataset_size: how many image pairs should be generated and saved on disk - integer\n    \"\"\"\n    i = 0\n    padding = len(str(dataset_size))\n\n    while i < dataset_size:\n        cvl_paths = \"CVL_Database/lines\"\n        dir_path = (random.choice(os.listdir(cvl_paths)))\n        full_path = (cvl_paths + \"/\" + dir_path)\n        image_1, image_2 = random.sample(os.listdir(full_path), 2)\n\n        if rgb == True:\n            try:\n                image_1 = Image.open(full_path + \"/\" + image_1)\n                image_2 = Image.open(full_path + \"/\" + image_2)\n            except:\n                continue\n        else:\n            try:\n                image_1 = Image.open(full_path + \"/\" + image_1).convert(\"L\")\n                image_2 = Image.open(full_path + \"/\" + image_2).convert(\"L\")\n            except:\n                continue\n\n        try:\n            if image_1.size[0] > crop_width and image_2.size[0] > crop_width:\n                #resize the images to specific height and keeping the same aspect ratio\n                height_precent_image_1 = (height / float(image_1.size[1]))\n                height_precent_image_2 = (height / float(image_2.size[1]))\n\n                resized_width_image_1 = int((float(image_1.size[0]) * float(height_precent_image_1)))\n                resized_width_image_2 = int((float(image_2.size[0]) * float(height_precent_image_2)))\n\n                image_1 = image_1.resize((resized_width_image_1, height), Image.NEAREST)\n                image_2 = image_2.resize((resized_width_image_2, height), Image.NEAREST)\n\n                center_image_1 = int(image_1.size[0] / 2)\n                center_image_2 = int(image_2.size[0] / 2)\n\n                x_1 = int(center_image_1 - (crop_width/2))\n                x_2 = int(center_image_2 - (crop_width/2))\n\n                #crop randomly  image out of source image\n                img_array_1 = np.array(image_1)\n                img_array_2 = np.array(image_2)\n\n                anchor = img_array_1[0:height, x_1:x_1+crop_width]\n                positive = img_array_2[0:height, x_2:x_2+crop_width]\n\n                anchor = Image.fromarray(anchor)\n                positive = Image.fromarray(positive)\n\n                anchor = anchor.resize((width, height))\n                positive = positive.resize((width, height))\n\n                anchor.save(os.path.join(path_anchors + \"/\" + f\"{i:0{padding}}.jpg\"))\n                positive.save(os.path.join(path_positives + \"/\" + f\"{i:0{padding}}.jpg\"))\n\n                i += 1\n            else:\n                pass\n        except:\n            continue"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f88814ab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 3min 12s, sys: 1.97 s, total: 3min 14s\nWall time: 3min 14s\n"
    }
   ],
   "source": "%%time\ncreate_cvl_image_pairs_rows_fixed_size(height=224, width=224, crop_width=1000, path_anchors=\"npz_datasets/test_pairs_cvl_224_224_rows_1000/anchor\", path_positives=\"npz_datasets/test_pairs_cvl_224_224_rows_1000/positive\", dataset_size=10000, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5027b54",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 9min 41s, sys: 7.14 s, total: 9min 49s\nWall time: 10min 4s\n"
    }
   ],
   "source": "%%time\nwidths_list = []\nfor i in range(len(dataset_paths)):\n    image_list = os.listdir(dataset_paths[i])\n    for j in range(len(image_list)):\n        try:\n            image = Image.open(dataset_paths[i] + \"/\" + image_list[j])\n        except:\n            continue\n        try:\n            if image.size[0] < 500:\n                pass\n            else:\n                height_precent = (height / float(image.size[1]))\n                resized_width = int((float(image.size[0]) * float(height_precent)))\n                image = image.resize((resized_width, height), Image.NEAREST)\n                widths_list.append(image.size[0])\n        except:\n            continue"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11b727ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "average sentence length in px: 1915\nmax sentence length in px: 7808\nmin sentence length in px: 414\nhow many greater then 1000px: 293764\nhow many smaller then 1000px: 3166\nhow many greater then 1500px: 193724\nhow many smaller then 1500px: 102951\n"
    }
   ],
   "source": "print(f\"average sentence length in px: {(int(sum(widths_list) / len(widths_list)))}\")\nprint(f\"max sentence length in px: {(int(max(widths_list)))}\")\nprint(f\"min sentence length in px: {(int(min(widths_list)))}\")\nprint(f\"how many greater then 1000px: {(int(sum(i > 1000 for i in widths_list)))}\")\nprint(f\"how many smaller then 1000px: {(int(sum(i < 1000 for i in widths_list)))}\")\nprint(f\"how many greater then 1500px: {(int(sum(i > 1500 for i in widths_list)))}\")\nprint(f\"how many smaller then 1500px: {(int(sum(i < 1500 for i in widths_list)))}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a097e68",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "def create_image_pairs_rows_fixed_size_ccl213_ccl_214(height, width, crop_width, data_path, save_path):\n\n    data_path_list = os.listdir(data_path)\n\n    for i in range(len(data_path_list)):\n    #for i in range(10):\n\n        image = Image.open(data_path + \"/\" + data_path_list[i])\n\n        if image.size[0] > crop_width and image.size[0] > crop_width:\n\n            height_precent_image = (height / float(image.size[1]))\n            resized_width_image = int((float(image.size[0]) * float(height_precent_image)))\n\n            image = image.resize((resized_width_image, height), Image.NEAREST)\n\n            center_image = int(image.size[0] / 2)\n\n            x = int(center_image - (crop_width/2))\n\n            img_array = np.array(image)\n\n            image = img_array[0:height, x:x+crop_width]\n\n            image = Image.fromarray(image)\n\n            image = image.resize((width, height))\n\n            image.save(os.path.join(save_path + \"/\" + f\"{i}.jpg\"))\n\n        else:\n            pass"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1824ac93",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 1min 14s, sys: 945 ms, total: 1min 15s\nWall time: 1min 17s\n"
    }
   ],
   "source": "%%time\ndata_path = \"Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand B 259\"\nsave_path = \"npz_datasets/handB_259\"\ncreate_image_pairs_rows_fixed_size_ccl213_ccl_214(height=224, width=224, crop_width=1000, data_path=data_path, save_path=save_path)"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d1075558",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "def create_test_image_pairs(height, width, crop_width, path_anchors, path_positives, dataset_size, dataset_path, rgb=True):\n\n    i = 0\n    padding = len(str(dataset_size))\n\n    while i < dataset_size:\n        hand_choice = (random.choice(dataset_path))\n        hand_path_1, hand_path_2 = random.sample(hand_choice, 2)\n        image_1 = random.choice(os.listdir(hand_path_1))\n        image_2 = random.choice(os.listdir(hand_path_2))\n\n        if rgb == True:\n            try:\n                image_1 = Image.open(str(hand_path_1) + \"/\" + str(image_1))\n                image_2 = Image.open(str(hand_path_2) + \"/\" + str(image_2))\n            except:\n                continue\n        else:\n            try:\n                image_1 = Image.open(str(hand_path_1) + \"/\" + str(image_1)).convert(\"L\")\n                image_2 = Image.open(str(hand_path_1) + \"/\" + str(image_1)).convert(\"L\")\n            except:\n                continue\n\n        try:\n            if image_1.size[0] > crop_width and image_2.size[0] > crop_width:\n                #resize the images to specific height and keeping the same aspect ratio\n                height_precent_image_1 = (height / float(image_1.size[1]))\n                height_precent_image_2 = (height / float(image_2.size[1]))\n\n                resized_width_image_1 = int((float(image_1.size[0]) * float(height_precent_image_1)))\n                resized_width_image_2 = int((float(image_2.size[0]) * float(height_precent_image_2)))\n\n                image_1 = image_1.resize((resized_width_image_1, height), Image.NEAREST)\n                image_2 = image_2.resize((resized_width_image_2, height), Image.NEAREST)\n\n                center_image_1 = int(image_1.size[0] / 2)\n                center_image_2 = int(image_2.size[0] / 2)\n\n                x_1 = int(center_image_1 - (crop_width/2))\n                x_2 = int(center_image_2 - (crop_width/2))\n\n                #crop randomly  image out of source image\n                img_array_1 = np.array(image_1)\n                img_array_2 = np.array(image_2)\n\n                anchor = img_array_1[0:height, x_1:x_1+crop_width]\n                positive = img_array_2[0:height, x_2:x_2+crop_width]\n\n                anchor = Image.fromarray(anchor)\n                positive = Image.fromarray(positive)\n\n                anchor = anchor.resize((width, height))\n                positive = positive.resize((width, height))\n\n                anchor.save(os.path.join(path_anchors + \"/\" + f\"{i:0{padding}}.jpg\"))\n                positive.save(os.path.join(path_positives + \"/\" + f\"{i:0{padding}}.jpg\"))\n\n                i += 1\n            else:\n                pass\n        except:\n            continue"
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4434d194",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "def create_test_image_pairs_2(height, width, crop_width, path_anchors, path_positives, dataset_size, dataset_path, rgb=True):\n\n    i = 0\n    padding = len(str(dataset_size))\n\n    while i < dataset_size:\n        hand_choice = (random.choice(dataset_test_paths_tests))\n        if (len(hand_choice)) <= 1:\n            image_1, image_2 = random.sample(os.listdir(hand_choice[0]), 2)\n            hand_path_1, hand_path_2 = hand_choice[0], hand_choice[0]\n        else:\n            hand_path_1, hand_path_2 = random.sample(hand_choice, 2)\n            image_1 = random.choice(os.listdir(hand_path_1))\n            image_2 = random.choice(os.listdir(hand_path_2))\n\n        if rgb == True:\n            try:\n                image_1 = Image.open(str(hand_path_1) + \"/\" + str(image_1))\n                image_2 = Image.open(str(hand_path_2) + \"/\" + str(image_2))\n            except:\n                continue\n        else:\n            try:\n                image_1 = Image.open(str(hand_path_1) + \"/\" + str(image_1)).convert(\"L\")\n                image_2 = Image.open(str(hand_path_1) + \"/\" + str(image_1)).convert(\"L\")\n            except:\n                continue\n\n        try:\n            if image_1.size[0] > crop_width and image_2.size[0] > crop_width:\n                #resize the images to specific height and keeping the same aspect ratio\n                height_precent_image_1 = (height / float(image_1.size[1]))\n                height_precent_image_2 = (height / float(image_2.size[1]))\n\n                resized_width_image_1 = int((float(image_1.size[0]) * float(height_precent_image_1)))\n                resized_width_image_2 = int((float(image_2.size[0]) * float(height_precent_image_2)))\n\n                image_1 = image_1.resize((resized_width_image_1, height), Image.NEAREST)\n                image_2 = image_2.resize((resized_width_image_2, height), Image.NEAREST)\n\n                center_image_1 = int(image_1.size[0] / 2)\n                center_image_2 = int(image_2.size[0] / 2)\n\n                x_1 = int(center_image_1 - (crop_width/2))\n                x_2 = int(center_image_2 - (crop_width/2))\n\n                #crop randomly  image out of source image\n                img_array_1 = np.array(image_1)\n                img_array_2 = np.array(image_2)\n\n                anchor = img_array_1[0:height, x_1:x_1+crop_width]\n                positive = img_array_2[0:height, x_2:x_2+crop_width]\n\n                anchor = Image.fromarray(anchor)\n                positive = Image.fromarray(positive)\n\n                anchor = anchor.resize((width, height))\n                positive = positive.resize((width, height))\n\n                anchor.save(os.path.join(path_anchors + \"/\" + f\"{i:0{padding}}.jpg\"))\n                positive.save(os.path.join(path_positives + \"/\" + f\"{i:0{padding}}.jpg\"))\n\n                i += 1\n            else:\n                pass\n        except:\n            continue"
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5b92f23e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 1h 19min 9s, sys: 10min 45s, total: 1h 29min 54s\nWall time: 1h 30min 45s\n"
    }
   ],
   "source": "%%time\ncreate_test_image_pairs_2(height=224, width=224, crop_width=1000, path_anchors=\"npz_datasets/pairs_250k_224_224_rows_1000_test/anchor\", path_positives=\"npz_datasets/pairs_250k_224_224_rows_1000_test/positive\", dataset_size=250000, dataset_path=dataset_test_paths, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0431606",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": "def create_test_image_pairs_combined_list_excluded(height, width, crop_width, ground_path,path_anchors, path_positives, examples_per_pair, dataset_path_positives, dataset_path_negatives, rgb=True):\n\n    #calculate length for padding of saved image numbers\n    len_dataset = (len(dataset_path_positives) * examples_per_pair) * 2\n    len_dataset = len_dataset + 180\n    padding = len(str(len_dataset))\n    overall_count = 0\n    labels = []\n    for i in range(len(dataset_path_positives)):\n        j = 0\n        while j <= examples_per_pair:\n            image_1 = random.choice(os.listdir(dataset_path_positives[i][0]))\n            image_2 = random.choice(os.listdir(dataset_path_positives[i][1]))\n            image_1_path = dataset_path_positives[i][0] + \"/\" + image_1\n            image_2_path = dataset_path_positives[i][1] + \"/\" + image_2\n\n            if rgb == True:\n                try:\n                    image_1 = Image.open(str(image_1_path))\n                    image_2 = Image.open(str(image_2_path))\n                except:\n                    continue\n            else:\n                try:\n                    image_1 = Image.open(str(image_1_path)).convert(\"L\")\n                    image_2 = Image.open(str(image_2_path)).convert(\"L\")\n                except:\n                    continue\n            try:\n                if image_1.size[0] > crop_width and image_2.size[0] > crop_width:\n                    #resize the images to specific height and keeping the same aspect ratio\n                    height_precent_image_1 = (height / float(image_1.size[1]))\n                    height_precent_image_2 = (height / float(image_2.size[1]))\n\n                    resized_width_image_1 = int((float(image_1.size[0]) * float(height_precent_image_1)))\n                    resized_width_image_2 = int((float(image_2.size[0]) * float(height_precent_image_2)))\n\n                    image_1 = image_1.resize((resized_width_image_1, height), Image.NEAREST)\n                    image_2 = image_2.resize((resized_width_image_2, height), Image.NEAREST)\n\n                    center_image_1 = int(image_1.size[0] / 2)\n                    center_image_2 = int(image_2.size[0] / 2)\n\n                    x_1 = int(center_image_1 - (crop_width/2))\n                    x_2 = int(center_image_2 - (crop_width/2))\n\n                    #crop randomly  image out of source image\n                    img_array_1 = np.array(image_1)\n                    img_array_2 = np.array(image_2)\n\n                    anchor = img_array_1[0:height, x_1:x_1+crop_width]\n                    positive = img_array_2[0:height, x_2:x_2+crop_width]\n\n                    anchor = Image.fromarray(anchor)\n                    positive = Image.fromarray(positive)\n\n                    anchor = anchor.resize((width, height))\n                    positive = positive.resize((width, height))\n\n                    anchor.save(os.path.join(path_anchors + \"/\" + f\"{overall_count:0{padding}}.jpg\"))\n                    positive.save(os.path.join(path_positives + \"/\" + f\"{overall_count:0{padding}}.jpg\"))\n\n                    j += 1\n                    overall_count += 1\n                    labels.append(1)\n                else:\n                    pass\n            except:\n                continue\n    i = 0\n    while i < (labels.count(1)):\n        hand_1, hand_2 = random.sample(dataset_path_negatives, 2)\n        hand_1 = random.choice(hand_1)\n        hand_2 = random.choice(hand_2)\n        image_1 = random.choice(os.listdir(hand_1))\n        image_2 = random.choice(os.listdir(hand_2))\n        image_1_path = hand_1 + \"/\" + image_1\n        image_2_path = hand_2 + \"/\" + image_2\n\n        if rgb == True:\n            try:\n                image_1 = Image.open(str(image_1_path))\n                image_2 = Image.open(str(image_2_path))\n            except:\n                continue\n        else:\n            try:\n                image_1 = Image.open(str(image_1_path)).convert(\"L\")\n                image_2 = Image.open(str(image_2_path)).convert(\"L\")\n            except:\n                continue\n        try:\n            if image_1.size[0] > crop_width and image_2.size[0] > crop_width:\n                #resize the images to specific height and keeping the same aspect ratio\n                height_precent_image_1 = (height / float(image_1.size[1]))\n                height_precent_image_2 = (height / float(image_2.size[1]))\n\n                resized_width_image_1 = int((float(image_1.size[0]) * float(height_precent_image_1)))\n                resized_width_image_2 = int((float(image_2.size[0]) * float(height_precent_image_2)))\n\n                image_1 = image_1.resize((resized_width_image_1, height), Image.NEAREST)\n                image_2 = image_2.resize((resized_width_image_2, height), Image.NEAREST)\n\n                center_image_1 = int(image_1.size[0] / 2)\n                center_image_2 = int(image_2.size[0] / 2)\n\n                x_1 = int(center_image_1 - (crop_width/2))\n                x_2 = int(center_image_2 - (crop_width/2))\n\n                #crop randomly  image out of source image\n                img_array_1 = np.array(image_1)\n                img_array_2 = np.array(image_2)\n\n                anchor = img_array_1[0:height, x_1:x_1+crop_width]\n                positive = img_array_2[0:height, x_2:x_2+crop_width]\n\n                anchor = Image.fromarray(anchor)\n                positive = Image.fromarray(positive)\n\n                anchor = anchor.resize((width, height))\n                positive = positive.resize((width, height))\n\n                anchor.save(os.path.join(path_anchors + \"/\" + f\"{overall_count:0{padding}}.jpg\"))\n                positive.save(os.path.join(path_positives + \"/\" + f\"{overall_count:0{padding}}.jpg\"))\n\n                i += 1\n                overall_count += 1\n                labels.append(0)\n            else:\n                pass\n        except:\n            continue\n    with open(ground_path + \"/\" + 'labels.txt', 'w') as f:\n        for item in labels:\n            f.write(\"%s\\n\" % item)"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae1f5c6a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 3min 43s, sys: 1min 23s, total: 5min 6s\nWall time: 5min 11s\n"
    }
   ],
   "source": "%%time\ncreate_test_image_pairs_combined_list_excluded(height=224, width=224, crop_width=1000, ground_path=\"npz_datasets/test_dataset_A215_B20_not_excluded_test\",  path_anchors=\"npz_datasets/test_dataset_A215_B20_not_excluded_test/anchor\", path_positives=\"npz_datasets/test_dataset_A215_B20_not_excluded_test/positive\", dataset_path_positives=combined_list, dataset_path_negatives=dataset_test_paths,examples_per_pair=50, rgb=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_209/rows/hand A 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand A 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_209/rows/hand A 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_949/rows/hand A 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand A 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_209/rows/hand A 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand A 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_949/rows/hand A 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_949/rows/hand A 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_209/rows/hand A 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_949/rows/hand A 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand A 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_622/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_706/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_212/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_671/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_622/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_622/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_706/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_622/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_212/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_622/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_671/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_706/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_706/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_622/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_706/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_212/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_706/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_671/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_212/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_212/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_622/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_212/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_706/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_212/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_671/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_671/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_671/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_622/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_671/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_706/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_671/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_212/rows/hand B 259'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_21/rows/hand A 20',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_28/rows/hand A 20'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_21/rows/hand A 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_39/rows/hand A 20'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_28/rows/hand A 20',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_21/rows/hand A 20'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_28/rows/hand A 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_39/rows/hand A 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_39/rows/hand A 20',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_21/rows/hand A 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_39/rows/hand A 20',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_28/rows/hand A 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_22/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_195/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_22/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_216/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_22/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_764/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_195/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_22/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_195/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_216/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_195/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_764/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_216/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_22/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_216/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_195/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_216/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_764/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_764/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_22/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_764/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_195/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_764/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_216/rows/hand B 20'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_215/rows/hand A 215',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_219/rows/hand A 215'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_215/rows/hand A 215',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_703/rows/hand A 215'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_219/rows/hand A 215',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_215/rows/hand A 215'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_219/rows/hand A 215',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_703/rows/hand A 215'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_703/rows/hand A 215',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_215/rows/hand A 215'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_703/rows/hand A 215',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_219/rows/hand A 215'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_258/rows/hand A 258',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_707/rows/hand A 258'),\n ('Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_707/rows/hand A 258',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_258/rows/hand A 258')]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "combined_list",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "id": "81510819"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[['Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_30/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_31/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_197/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_206/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_226/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_246/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_256/rows/hand A 30',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_257/rows/hand A 30'],\n ['Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_209/rows/hand A 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand A 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_949/rows/hand A 259'],\n ['Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_259/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_622/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_706/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_212/rows/hand B 259',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_671/rows/hand B 259'],\n ['Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_21/rows/hand A 20',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_28/rows/hand A 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_39/rows/hand A 20'],\n ['Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_22/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_195/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_A/processed/rgb_rows/ccl_216/rows/hand B 20',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_764/rows/hand B 20'],\n ['Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_215/rows/hand A 215',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_219/rows/hand A 215',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_703/rows/hand A 215'],\n ['Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_258/rows/hand A 258',\n  'Dataset_22092021_lina/SCID/Data_B/processed/rgb_rows/ccl_707/rows/hand A 258']]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "dataset_test_paths",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "id": "9dcf2e91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": "",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "id": "1cecb6c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}